# ğŸ—ºï¸ EVOKI V3.0 DATA LAYER â€” IMPLEMENTATION ROADMAP

**Version**: 1.0.0  
**Erstellt**: 2026-02-08  
**Basis**: **1000 Prompt-Paare** (Test-MVP)  
**Strategie**: EvolutionÃ¤rer Aufbau in 4 Phasen â†’ **Gate-Keeping auf 22.000 Paare**

---

## ğŸ“Š **ÃœBERBLICK**

### **Ausgangslage**:
- âœ… T2 Pipeline erfolgreich (1000 Paare, 651 t/s, 0 Fehler)
- âœ… MetricsCalculator funktioniert (168 Metriken)
- âœ… 4-Unit DB Schema existiert (metadata, resonance, triggers, metapatterns)
- âŒ V3.0 Future State existiert NICHT â†’ muss neu gebaut werden

### **Ziel**:
Erstelle das **komplette V3.0 Data Layer** wie in BUCH 7 spezifiziert:
- 5 neue SQLite-Datenbanken
- 3 FAISS-Namespaces (semantic, metrics, trajectory)
- Learning Keyword Engine
- Metric Trajectory Predictor
- Dual-Response API Client
- B-Vektor Verification System

### **Kritische Regel**:
âš ï¸ **ERST validieren mit 1000 Paaren via LIVE-MONITOR, DANN auf 22.000 Paare skalieren!**  
ğŸ“‹ **Details**: Siehe `LIVE_PIPELINE_TEST_PLAN.md`

---

## ğŸ¯ **4-PHASEN PLAN**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVOLUTION ROADMAP                                â”‚
â”‚                                                                      â”‚
â”‚  PHASE 0: Foundation          [2-3 Stunden]                         â”‚
â”‚  â”œâ”€ Cleanup + Test-Data bereit                                      â”‚
â”‚  â””â”€ Schemas erstellt                                                â”‚
â”‚                                                                      â”‚
â”‚  PHASE 1: Core V3.0 DB        [1-2 Tage]                            â”‚
â”‚  â”œâ”€ evoki_v3_core.db + FAISS atomic_pairs                           â”‚
â”‚  â””â”€ 1000 Paare importiert + validiert                               â”‚
â”‚                                                                      â”‚
â”‚  PHASE 2: Analytics           [1-2 Tage]                            â”‚
â”‚  â”œâ”€ evoki_v3_analytics.db + evoki_v3_trajectories.db                â”‚
â”‚  â””â”€ Historical Futures System                                       â”‚
â”‚                                                                      â”‚
â”‚  PHASE 3: Learning Engines    [2-3 Tage]                            â”‚
â”‚  â”œâ”€ Learning Keyword Engine                                         â”‚
â”‚  â”œâ”€ Metric Trajectory Predictor                                     â”‚
â”‚  â””â”€ evoki_v3_keywords.db + evoki_v3_graph.db                        â”‚
â”‚                                                                      â”‚
â”‚  PHASE 4: Integration         [1-2 Tage]                            â”‚
â”‚  â”œâ”€ Dual-Response API Client                                        â”‚
â”‚  â”œâ”€ Full Pipeline Test (1000 Paare)                                 â”‚
â”‚  â””â”€ Scale-Up auf 11.016 Paare                                       â”‚
â”‚                                                                      â”‚
â”‚  GESAMT: ~1 Woche (bei fokussierter Arbeit)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ **PHASE 0: FOUNDATION** (2-3 Stunden)

### **Ziel**: Saubere Basis schaffen

### âœ… **TODO-LISTE PHASE 0**:

- [ ] **0.1 Cleanup alte Artefakte** (~15 Min)
  - [ ] Identifiziere alle gescheiterten V3.0 Implementierungsversuche
  - [ ] Verschiebe nach `archive/failed_attempts/` (NICHT lÃ¶schen!)
  - [ ] Erstelle `backend/v3_data_layer/` als neues Arbeitsverzeichnis
  
- [ ] **0.2 Test-Data extrahieren** (~30 Min)
  - [ ] Identifiziere Quelle der 1000 Test-Paare (aus T2 Pipeline Success)
  - [ ] Kopiere nach `backend/test_data/1000_pairs/`
  - [ ] Validiere Format (User + AI Paare, Timestamps, IDs)
  - [ ] **Was du sehen solltest**: 1000 Dateien mit strukturiertem Format
  
- [ ] **0.3 Erstelle alle 5 V3.0 SQL-Schemas** (~1 Stunde)
  - [ ] `backend/schemas/evoki_v3_core_schema.sql` (BEREITS EXISTIERT - prÃ¼fen!)
  - [ ] `backend/schemas/evoki_v3_graph_schema.sql`
  - [ ] `backend/schemas/evoki_v3_keywords_schema.sql`
  - [ ] `backend/schemas/evoki_v3_analytics_schema.sql`
  - [ ] `backend/schemas/evoki_v3_trajectories_schema.sql`
  - [ ] **Was du sehen solltest**: 5 SQL-Dateien, jede ~200-400 Zeilen
  
- [ ] **0.4 Erstelle FAISS Config** (~30 Min)
  - [ ] `backend/v3_data_layer/faiss_config.json`
  - [ ] Definiere 3 Namespaces (atomic_pairs, metrics_wpf, trajectory_wpf)
  - [ ] Definiere Dimensionen (384D, 322D, custom)
  - [ ] **Was du sehen solltest**: JSON mit Index-Konfiguration
  
- [ ] **0.5 Erstelle Verzeichnisstruktur** (~15 Min)
  ```
  backend/v3_data_layer/
  â”œâ”€â”€ databases/          (SQLite DBs hier)
  â”œâ”€â”€ faiss_indices/      (FAISS Indizes hier)
  â”œâ”€â”€ models/             (Embedding-Modelle hier)
  â”œâ”€â”€ scripts/            (Python Pipeline-Scripts)
  â”œâ”€â”€ tests/              (Unit-Tests)
  â””â”€â”€ logs/               (Logging)
  ```

### **PHASE 0 DONE CHECKLIST**:
- âœ… 5 SQL-Schema-Dateien existieren
- âœ… 1000 Test-Paare in `backend/test_data/1000_pairs/`
- âœ… Verzeichnisstruktur erstellt
- âœ… FAISS Config JSON erstellt
- âœ… Alte Artefakte archiviert

**ZEIT**: ~2-3 Stunden  
**OUTPUT**: Saubere Basis fÃ¼r Phase 1

---

## ğŸ“ **PHASE 1: CORE V3.0 DB + BASIC FAISS** (1-2 Tage)

### **Ziel**: evoki_v3_core.db + FAISS atomic_pairs funktionieren mit 1000 Paaren

### âœ… **TODO-LISTE PHASE 1**:

#### **1.1 Erstelle evoki_v3_core.db** (~2 Stunden)

- [ ] **1.1.1 Schema initialisieren**
  - [ ] Python-Script: `scripts/init_v3_core_db.py`
  - [ ] LÃ¤dt `backend/schemas/evoki_v3_core_schema.sql`
  - [ ] Erstellt `backend/v3_data_layer/databases/evoki_v3_core.db`
  - [ ] FÃ¼hre aus: `python scripts/init_v3_core_db.py`
  - [ ] **Was du sehen solltest**: 
    ```
    âœ… Created evoki_v3_core.db
    âœ… Table 'sessions' created (0 rows)
    âœ… Table 'prompt_pairs' created (0 rows)
    âœ… Table 'metrics_full' created (0 rows)
    âœ… Table 'session_chain' created (0 rows)
    âœ… Table 'b_state_evolution' created (0 rows)
    âœ… Table 'hazard_events' created (0 rows)
    ```

- [ ] **1.1.2 Test Insert/Select**
  - [ ] FÃ¼ge 1 Test-Paar manuell ein
  - [ ] SELECT * FROM prompt_pairs â†’ sollte 1 Zeile zurÃ¼ckgeben
  - [ ] SELECT * FROM metrics_full â†’ sollte 1 Zeile zurÃ¼ckgeben (User + AI Metriken)
  - [ ] **Was du sehen solltest**: Erfolgreiches Insert + Select

#### **1.2 Import Pipeline fÃ¼r V3.0** (~4 Stunden)

- [ ] **1.2.1 Erstelle `v3_importer.py`**
  - [ ] Basis: Kopie von `t2_history_importer.py` (der funktioniert!)
  - [ ] Anpassungen:
    - [ ] Schreibt in `evoki_v3_core.db` statt 4-Unit DBs
    - [ ] Berechnet **322 Metriken** (161 User + 161 AI) statt 168
    - [ ] Generiert `pair_id` (UUID v4)
    - [ ] Generiert `pair_hash` (SHA-256)
    - [ ] Berechnet B-Vektor (7D) fÃ¼r jeden Prompt
    - [ ] Erstellt Session Chain Hash
  - [ ] **Was du sehen solltest**: Python-Datei ~300-400 Zeilen

- [ ] **1.2.2 Test mit 10 Paaren** (Dry-Run)
  - [ ] FÃ¼hre aus: `python scripts/v3_importer.py --dry-run --limit 10`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ“Š Dry-Run: 10 Paare
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âœ… Paar 1: Metriken berechnet (322), B-Vektor OK, Hash OK
    âœ… Paar 2: Metriken berechnet (322), B-Vektor OK, Hash OK
    ...
    âœ… Paar 10: Metriken berechnet (322), B-Vektor OK, Hash OK
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â±ï¸  Zeit: ~0.15s (67 Paare/s)
    âœ… 0 Fehler
    ```

- [ ] **1.2.3 Import 1000 Paare** (Full Run)
  - [ ] FÃ¼hre aus: `python scripts/v3_importer.py --limit 1000`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ“Š V3.0 Import: 1000 Paare
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [Progress bar 100%]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âœ… 1000 Paare importiert
    â±ï¸  Zeit: ~1.5s (650 Paare/s)
    ğŸ“Š Statistiken:
       â€¢ prompt_pairs:     1000 Zeilen
       â€¢ metrics_full:     1000 Zeilen (322 Metriken pro Zeile)
       â€¢ b_state_evolution: 1000 Zeilen (7D B-Vektor pro Zeile)
       â€¢ session_chain:    ~50 Sessions
       â€¢ hazard_events:    ~50 Events (5% Hazard-Rate)
    âœ… 0 Fehler
    ```

- [ ] **1.2.4 Validierung**
  - [ ] SQL Query: `SELECT COUNT(*) FROM prompt_pairs;` â†’ sollte **1000** sein
  - [ ] SQL Query: `SELECT AVG(user_m1_A), AVG(ai_m1_A) FROM metrics_full;` â†’ sollte ~0.5-0.7 sein
  - [ ] SQL Query: `SELECT COUNT(*) FROM hazard_events WHERE user_m151_hazard > 0.7;` â†’ sollte ~50 sein
  - [ ] **Was du sehen solltest**: Alle Queries erfolgreich, Werte plausibel

#### **1.3 FAISS atomic_pairs Namespace** (~3 Stunden)

- [ ] **1.3.1 Embedding-Modell herunterladen**
  - [ ] Modell: `sentence-transformers/all-MiniLM-L6-v2` (384D)
  - [ ] Script: `scripts/download_embedding_model.py`
  - [ ] Speichert nach: `backend/v3_data_layer/models/all-MiniLM-L6-v2/`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ“¥ Downloading all-MiniLM-L6-v2...
    âœ… Model downloaded (~90 MB)
    âœ… Saved to: backend/v3_data_layer/models/all-MiniLM-L6-v2/
    ```

- [ ] **1.3.2 Erstelle FAISS Index Builder**
  - [ ] Script: `scripts/build_faiss_atomic_pairs.py`
  - [ ] Liest alle 1000 Paare aus `prompt_pairs`
  - [ ] Generiert 384D Embedding fÃ¼r jeden Prompt (User + AI concatenated)
  - [ ] Erstellt FAISS Index (IndexFlatIP fÃ¼r Initial-Test)
  - [ ] Speichert nach: `backend/v3_data_layer/faiss_indices/atomic_pairs.index`
  - [ ] Speichert Metadaten: `atomic_pairs.meta.json` (pair_id â†’ index mapping)

- [ ] **1.3.3 Build Index mit 1000 Paaren**
  - [ ] FÃ¼hre aus: `python scripts/build_faiss_atomic_pairs.py`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Building FAISS atomic_pairs Index
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ğŸ“– Loading 1000 prompt_pairs from evoki_v3_core.db...
    âœ… Loaded 1000 pairs
    
    ğŸ§® Generating embeddings (MiniLM-L6-v2, 384D)...
    [Progress bar 100%]
    âœ… Generated 1000 embeddings (~1.5s)
    
    ğŸ“Š Building FAISS Index...
    âœ… Index built (1000 vectors, 384D)
    
    ğŸ’¾ Saving to: faiss_indices/atomic_pairs.index
    âœ… Saved index (~1.5 MB)
    âœ… Saved metadata (~50 KB)
    
    â±ï¸  Total time: ~2.0s
    ```

- [ ] **1.3.4 Test FAISS Search**
  - [ ] Script: `scripts/test_faiss_search.py`
  - [ ] Query: "Ich habe Angst" â†’ sollte Ã¤hnliche Prompts finden
  - [ ] FÃ¼hre aus: `python scripts/test_faiss_search.py --query "Ich habe Angst"`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” FAISS Search Test
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Query: "Ich habe Angst"
    
    Top 5 Results:
    1. [Score: 0.94] Pair ID: abc123... "Ich bin Ã¤ngstlich..."
    2. [Score: 0.89] Pair ID: def456... "Angst vor der Zukunft..."
    3. [Score: 0.85] Pair ID: ghi789... "Furcht vor dem Unbekannten..."
    4. [Score: 0.82] Pair ID: jkl012... "Sorgen um meine Sicherheit..."
    5. [Score: 0.78] Pair ID: mno345... "Unsicherheit belastet mich..."
    
    âœ… Search completed in ~5ms
    ```

### **PHASE 1 DONE CHECKLIST**:
- âœ… `evoki_v3_core.db` existiert mit 1000 Paaren
- âœ… Alle Tabellen befÃ¼llt (prompt_pairs, metrics_full, b_state_evolution, etc.)
- âœ… FAISS atomic_pairs Index gebaut (1000 Vektoren, 384D)
- âœ… FAISS Search funktioniert (Top-5 Ergebnisse plausibel)
- âœ… Performance: ~650 Paare/s Import, ~5ms Search
- âœ… Validierung: 0 Fehler, alle Metriken plausibel

**ZEIT**: ~1-2 Tage (8-16 Stunden Arbeit)  
**OUTPUT**: Funktionierende Core DB + Semantic Search

---

## ğŸ“ **PHASE 2: ANALYTICS + TRAJECTORIES** (1-2 Tage)

### **Ziel**: Analytics DB + Trajectories DB + Historical Futures System

### âœ… **TODO-LISTE PHASE 2**:

#### **2.1 evoki_v3_analytics.db** (~3 Stunden)

- [ ] **2.1.1 Schema initialisieren**
  - [ ] Script: `scripts/init_v3_analytics_db.py`
  - [ ] LÃ¤dt `backend/schemas/evoki_v3_analytics_schema.sql`
  - [ ] Erstellt `backend/v3_data_layer/databases/evoki_v3_analytics.db`
  - [ ] **Was du sehen solltest**: DB mit 7 Tabellen erstellt

- [ ] **2.1.2 Logging Pipeline bauen**
  - [ ] Klasse: `AnalyticsLogger` (in `scripts/analytics_logger.py`)
  - [ ] Methoden:
    - [ ] `log_api_request(prompt, context, timestamp)`
    - [ ] `log_api_response(response, b_vector, timestamp)`
    - [ ] `log_search_event(query, results, method)`
    - [ ] `log_metric_evaluation(pair_id, metric_name, value, computation_time)`
    - [ ] `log_learning_event(keyword, frequency, promoted)`
  - [ ] **Was du sehen solltest**: Python-Klasse ~200 Zeilen

- [ ] **2.1.3 Re-Import 1000 Paare MIT Analytics**
  - [ ] Erweitere `v3_importer.py` um Analytics-Logging
  - [ ] FÃ¼r jedes Paar:
    - [ ] Logge in `prompt_history`
    - [ ] Logge in `metric_evaluations` (alle 322 Metriken)
    - [ ] Simuliere API-Request/Response (fÃ¼r Test)
  - [ ] FÃ¼hre aus: `python scripts/v3_importer.py --with-analytics --limit 1000`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ“Š V3.0 Import mit Analytics
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âœ… 1000 Paare importiert
    ğŸ“Š Analytics Stats:
       â€¢ prompt_history:        1000 EintrÃ¤ge
       â€¢ metric_evaluations:    322,000 EintrÃ¤ge (322 Metriken Ã— 1000 Paare)
       â€¢ api_requests:          1000 EintrÃ¤ge (simuliert)
       â€¢ api_responses:         1000 EintrÃ¤ge (simuliert)
    â±ï¸  Zeit: ~3.0s (330 Paare/s mit Analytics)
    ```

#### **2.2 evoki_v3_trajectories.db** (~4 Stunden)

- [ ] **2.2.1 Schema initialisieren**
  - [ ] Script: `scripts/init_v3_trajectories_db.py`
  - [ ] Erstellt `evoki_v3_trajectories.db` mit 4 Tabellen

- [ ] **2.2.2 Trajectory Calculator bauen**
  - [ ] Klasse: `TrajectoryCalculator` (in `scripts/trajectory_calculator.py`)
  - [ ] Methode: `calculate_trajectory(session_id, current_pair_index, offsets=[-25,-5,-2,-1,0])`
  - [ ] FÃ¼r jede kritische Metrik (m1_A, m151_hazard, B_safety, etc.):
    - [ ] Lade Werte fÃ¼r Offsets -25, -5, -2, -1, 0
    - [ ] Berechne Gradient (Î” zwischen Werten)
    - [ ] Klassifiziere Trend (rising/falling/stable/volatile)
  - [ ] Speichert in `metric_trajectories` Tabelle
  - [ ] **Was du sehen solltest**: Python-Klasse ~250 Zeilen

- [ ] **2.2.3 Historical Futures Builder**
  - [ ] Klasse: `HistoricalFuturesBuilder` (in `scripts/historical_futures_builder.py`)
  - [ ] FÃ¼r jedes Paar N:
    - [ ] PrÃ¼fe: Existiert Paar N+1, N+2, N+5, N+10, N+25?
    - [ ] Wenn ja: Lade deren Metriken
    - [ ] Speichere in `historical_futures` als JSON:
      ```json
      {
        "future_plus_1": {"m1_A": 0.88, "m151_hazard": 0.72, ...},
        "future_plus_5": {"m1_A": 0.65, "m151_hazard": 0.45, ...},
        ...
      }
      ```
    - [ ] Klassifiziere `outcome_type` (natural_recovery/escalation/stable)
  - [ ] **Was du sehen solltest**: Python-Klasse ~200 Zeilen

- [ ] **2.2.4 Berechne Trajectories + Futures fÃ¼r 1000 Paare**
  - [ ] Script: `scripts/compute_trajectories.py`
  - [ ] FÃ¼hre aus: `python scripts/compute_trajectories.py`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ“ˆ Computing Trajectories + Historical Futures
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ğŸ“Š Processing 1000 pairs...
    [Progress bar 100%]
    
    âœ… Trajectories computed:
       â€¢ metric_trajectories:  ~5000 EintrÃ¤ge (5 Offsets Ã— 10 Metriken Ã— 100 Sessions)
       â€¢ historical_futures:   ~800 EintrÃ¤ge (80% der Paare haben +1 Future)
       
    ğŸ“Š Outcome Distribution:
       â€¢ natural_recovery:  520 (65%)
       â€¢ escalation:        160 (20%)
       â€¢ stable:            120 (15%)
       
    â±ï¸  Zeit: ~5.0s
    ```

#### **2.3 FAISS trajectory_wpf Namespace** (~2 Stunden)

- [ ] **2.3.1 Trajectory Embedder**
  - [ ] Klasse: `TrajectoryEmbedder` (in `scripts/trajectory_embedder.py`)
  - [ ] Konvertiert Trajectory â†’ Vektor:
    ```python
    # FÃ¼r jede kritische Metrik (m1_A, hazard, B_safety)
    vector = [
        m1_A(-25), m1_A(-5), m1_A(-2), m1_A(-1), m1_A(0),  # 5 Werte
        hazard(-25), hazard(-5), hazard(-2), hazard(-1), hazard(0),  # 5 Werte
        B_safety(-25), ..., B_safety(0),  # 5 Werte
        gradient_m1_A,  # 1 Wert
        gradient_hazard,  # 1 Wert
        trend_code  # 1 Wert (rising=1, falling=-1, stable=0)
    ]
    # Total: ~20-50D (abhÃ¤ngig von Anzahl Metriken)
    ```

- [ ] **2.3.2 Build trajectory_wpf Index**
  - [ ] Script: `scripts/build_faiss_trajectory.py`
  - [ ] Generiert Trajectory-Vektoren fÃ¼r alle Paare mit vollstÃ¤ndigen Trajectories
  - [ ] Erstellt FAISS Index
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Building FAISS trajectory_wpf Index
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ğŸ“Š Loaded 800 trajectories (80% haben vollstÃ¤ndige History)
    ğŸ§® Generating trajectory vectors (custom, ~30D)...
    âœ… Generated 800 vectors
    ğŸ“Š Built FAISS Index
    âœ… Saved to: faiss_indices/trajectory_wpf.index (~1 MB)
    ```

- [ ] **2.3.3 Test Trajectory-Based Search**
  - [ ] Script: `scripts/test_trajectory_search.py`
  - [ ] Simuliere: User mit steigender Angst (m1_A: 0.5â†’0.7â†’0.85)
  - [ ] Query FAISS trajectory_wpf
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Trajectory Search Test
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Current Trajectory: m1_A rising (0.5â†’0.85), hazard rising (0.3â†’0.7)
    
    Top 3 Similar Historical Trajectories:
    1. [Score: 0.92] Session X, Pair #45
       â†’ Trajectory: m1_A 0.48â†’0.82, hazard 0.28â†’0.65
       â†’ Outcome: natural_recovery
       â†’ Future +5: m1_A=0.60, hazard=0.40
       
    2. [Score: 0.88] Session Y, Pair #78
       â†’ Trajectory: m1_A 0.52â†’0.88, hazard 0.35â†’0.75
       â†’ Outcome: guardian_intervention (at +3)
       
    3. [Score: 0.85] Session Z, Pair #12
       â†’ Trajectory: m1_A 0.45â†’0.80, hazard 0.30â†’0.68
       â†’ Outcome: natural_recovery
       â†’ Successful Strategy: soothing_validation
    ```

### **PHASE 2 DONE CHECKLIST**:
- âœ… `evoki_v3_analytics.db` existiert mit vollstÃ¤ndigem Logging
- âœ… `evoki_v3_trajectories.db` existiert mit Trajectories + Historical Futures
- âœ… 1000 Paare haben Analytics-EintrÃ¤ge
- âœ… ~800 Paare haben Historical Futures (+1, +5, etc.)
- âœ… FAISS trajectory_wpf Index funktioniert
- âœ… Trajectory-Based Search liefert plausible Ergebnisse

**ZEIT**: ~1-2 Tage (8-16 Stunden)  
**OUTPUT**: Predictive Analytics System funktioniert

---

## ğŸ“ **PHASE 3: LEARNING ENGINES** (2-3 Tage)

### **Ziel**: Learning Keyword Engine + Graph DB + Metrics FAISS

### âœ… **TODO-LISTE PHASE 3**:

#### **3.1 evoki_v3_keywords.db** (~4 Stunden)

- [ ] **3.1.1 Schema initialisieren**
  - [ ] Script: `scripts/init_v3_keywords_db.py`
  - [ ] Erstellt DB mit 4 Tabellen + FTS5 Index

- [ ] **3.1.2 Learning Keyword Engine bauen**
  - [ ] Klasse: `LearningKeywordEngine` (in `scripts/learning_keyword_engine.py`)
  - [ ] **Features**:
    - [ ] Keyword-Extraktion (RAKE Algorithm)
    - [ ] Frequency Tracking (auto-increment)
    - [ ] Promotion System (nach 10+ Hits)
    - [ ] Association Learning (Co-Occurrence)
    - [ ] Live-Indexing (FTS5)
  - [ ] **Was du sehen solltest**: Python-Klasse ~400 Zeilen

- [ ] **3.1.3 Keyword-Extraktion fÃ¼r 1000 Paare**
  - [ ] Script: `scripts/extract_keywords.py`
  - [ ] FÃ¼r jedes Paar:
    - [ ] Extrahiere Keywords (User + AI Text)
    - [ ] Update Frequency
    - [ ] Track Co-Occurrence
    - [ ] Index in FTS5
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ”¤ Extracting Keywords from 1000 Pairs
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [Progress bar 100%]
    
    ğŸ“Š Results:
       â€¢ keyword_registry:      ~5000 unique Keywords
       â€¢ keyword_pair_links:    ~15000 EintrÃ¤ge (avg 15 Keywords/Paar)
       â€¢ keyword_associations:  ~8000 Co-Occurrences
       â€¢ Promoted Keywords:     ~500 (Frequency â‰¥ 10)
       
    ğŸ” Top 10 Keywords:
       1. "angst" (Freq: 145, Promoted: YES)
       2. "trauma" (Freq: 98, Promoted: YES)
       3. "evoki" (Freq: 87, Promoted: YES)
       ...
    
    â±ï¸  Zeit: ~3.0s
    ```

- [ ] **3.1.4 Test Hybrid Search**
  - [ ] Script: `scripts/test_hybrid_search.py`
  - [ ] Query: "angst trauma"
  - [ ] Kombiniert:
    - [ ] Keyword Search (evoki_v3_keywords.db)
    - [ ] Semantic Search (FAISS atomic_pairs)
    - [ ] Metric Search (FAISS metrics_wpf)
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Hybrid Search Test
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Query: "angst trauma"
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ KEYWORD SEARCH (Weight: 0.3)                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. Pair #234 (Score: 0.95) "Angst Trauma"  â”‚
    â”‚ 2. Pair #567 (Score: 0.88) "Trauma Angst"  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ SEMANTIC SEARCH (Weight: 0.5)               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. Pair #123 (Score: 0.92) "Furcht..."     â”‚
    â”‚ 2. Pair #456 (Score: 0.85) "Belastung..."  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FINAL RANKING (Combined)                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. Pair #234 (Final: 0.93)                  â”‚
    â”‚ 2. Pair #123 (Final: 0.89)                  â”‚
    â”‚ 3. Pair #567 (Final: 0.85)                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```

#### **3.2 evoki_v3_graph.db** (~3 Stunden)

- [ ] **3.2.1 Schema initialisieren**
  - [ ] Script: `scripts/init_v3_graph_db.py`
  - [ ] Erstellt DB mit 3 Tabellen (nodes, edges, clusters)

- [ ] **3.2.2 Graph Builder**
  - [ ] Klasse: `GraphBuilder` (in `scripts/graph_builder.py`)
  - [ ] FÃ¼r jedes Paar: Erstelle Node
  - [ ] FÃ¼r jedes Paar: Berechne Edges zu Ã¤hnlichen Paaren (FAISS Top-10)
  - [ ] Edge-Gewicht: `0.6 Ã— semantic_similarity + 0.4 Ã— metric_similarity`
  - [ ] **Was du sehen solltest**: Python-Klasse ~300 Zeilen

- [ ] **3.2.3 Build Graph fÃ¼r 1000 Paare**
  - [ ] Script: `scripts/build_graph.py`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ•¸ï¸  Building Graph for 1000 Pairs
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [Progress bar 100%]
    
    ğŸ“Š Results:
       â€¢ graph_nodes:   1000 Nodes
       â€¢ graph_edges:   ~5000 Edges (avg 5 Edges/Node)
       â€¢ Edge weights:  avg 0.65 (good connectivity)
       
    â±ï¸  Zeit: ~10.0s
    ```

- [ ] **3.2.4 Cluster Detection**
  - [ ] Script: `scripts/detect_clusters.py`
  - [ ] Algorithmus: Louvain Community Detection
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Detecting Clusters...
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âœ… Found 15 clusters
    
    Top 5 Clusters:
    1. Cluster "Trauma-Themen" (120 Nodes)
    2. Cluster "Technische Fragen" (95 Nodes)
    3. Cluster "Philosophische Reflexionen" (78 Nodes)
    4. Cluster "Guardian-Interventionen" (42 Nodes)
    5. Cluster "Alltags-GesprÃ¤che" (180 Nodes)
    ```

#### **3.3 FAISS metrics_wpf Namespace** (~2 Stunden)

- [ ] **3.3.1 Metrics Embedder**
  - [ ] Klasse: `MetricsEmbedder` (in `scripts/metrics_embedder.py`)
  - [ ] Konvertiert 322 Metriken â†’ 322D Vektor
  - [ ] Optional: PCA auf 128D (Performance vs. Accuracy Trade-off)

- [ ] **3.3.2 Build metrics_wpf Index**
  - [ ] Script: `scripts/build_faiss_metrics.py`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Building FAISS metrics_wpf Index
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ğŸ“Š Loaded 1000 metric vectors (322D)
    ğŸ“Š Built FAISS Index
    âœ… Saved to: faiss_indices/metrics_wpf.index (~5 MB)
    ```

- [ ] **3.3.3 Test Metric-Based Search**
  - [ ] Query: Finde Prompts mit Ã¤hnlichen Metriken wie Pair #123
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ” Metric Search Test
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Reference Pair #123:
       m1_A: 0.85, hazard: 0.72, B_safety: 0.65
    
    Top 5 Metric-Similar Pairs:
    1. Pair #456 (Score: 0.95) m1_A: 0.83, hazard: 0.70
    2. Pair #789 (Score: 0.91) m1_A: 0.87, hazard: 0.75
    ...
    ```

### **PHASE 3 DONE CHECKLIST**:
- âœ… Learning Keyword Engine funktioniert (5000 Keywords, 500 promoted)
- âœ… Hybrid Search kombiniert Keyword + Semantic + Metric
- âœ… Graph DB mit 1000 Nodes, 5000 Edges, 15 Clustern
- âœ… FAISS metrics_wpf Index funktioniert
- âœ… Alle 3 FAISS-Namespaces operational

**ZEIT**: ~2-3 Tage (16-24 Stunden)  
**OUTPUT**: VollstÃ¤ndiges Multi-Modal Search System

---

## ğŸ“ **PHASE 4: INTEGRATION + SCALE-UP** (1-2 Tage)

### **Ziel**: Dual-Response API + Full Pipeline + Scale auf 11.016 Paare

### âœ… **TODO-LISTE PHASE 4**:

#### **4.1 Dual-Response API Client** (~4 Stunden)

- [ ] **4.1.1 DualResponseAPIClient Klasse**
  - [ ] Klasse: `DualResponseAPIClient` (in `scripts/dual_response_api_client.py`)
  - [ ] **Methoden**:
    - [ ] `send_request_1(prompt, enriched_context)` â†’ Standard AI-Response
    - [ ] `send_request_2(prompt, lexika_dict)` â†’ B-Vektor Verification
    - [ ] `compare_b_vectors(local_b, api_b)` â†’ Abweichungen loggen
    - [ ] `log_to_analytics(request, response, verification)`
  - [ ] **Was du sehen solltest**: Python-Klasse ~300 Zeilen

- [ ] **4.1.2 Test mit Mock-API**
  - [ ] Erstelle Mock-API (simuliert Google Gemini Response)
  - [ ] Test: 10 Prompts durch Dual-Response Pipeline
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ”„ Dual-Response API Test (Mock)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Prompt 1:
       âœ… Request 1: AI-Response empfangen
       âœ… Request 2: B-Vektor Verification empfangen
       ğŸ“Š B-Vector Comparison:
          Local:  B_safety=0.89, B_life=0.92
          API:    B_safety=0.91, B_life=0.93
          Delta:  0.02 (acceptable)
       âœ… Logged to analytics
    
    ...
    
    âœ… 10/10 erfolgreiche Dual-Responses
    ```

#### **4.2 Full Pipeline Integration** (~4 Stunden)

- [ ] **4.2.1 End-to-End Pipeline Script**
  - [ ] Script: `scripts/full_pipeline.py`
  - [ ] **Flow**:
    ```
    User Prompt
        â†“
    1. Trajectory berechnen (-25, -5, -2, -1, 0)
    2. FAISS trajectory_wpf suchen â†’ Top-5 Matches
    3. Historical Futures laden
    4. Enriched Context bauen
    5. Dual-Response API Call
    6. Metriken berechnen (322)
    7. B-Vektor berechnen (7D)
    8. DB Updates (5 DBs parallel)
    9. FAISS Updates (3 Indizes)
    10. Keywords extrahieren + lernen
    11. Historical Futures rÃ¼ckwirkend updaten
    12. Analytics loggen
    ```

- [ ] **4.2.2 Test mit 100 neuen Prompts**
  - [ ] FÃ¼hre aus: `python scripts/full_pipeline.py --test-prompts 100`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸš€ Full Pipeline Test: 100 Prompts
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [Progress bar 100%]
    
    âœ… 100 Prompts verarbeitet
    
    ğŸ“Š Performance:
       â€¢ Avg Context Build Time:  ~100ms
       â€¢ Avg API Call Time:       ~500ms (Mock)
       â€¢ Avg DB Update Time:      ~200ms
       â€¢ TOTAL per Prompt:        ~800ms
       
    ğŸ“Š Validierung:
       â€¢ evoki_v3_core.db:         +100 Paare
       â€¢ FAISS atomic_pairs:       +100 Vektoren
       â€¢ Keywords:                 +~1500 neue, ~50 promoted
       â€¢ Historical Futures:       ~500 Updates (rÃ¼ckwirkend)
       
    âœ… 0 Fehler
    ```

#### **4.3 Scale-Up auf 22.000 Paare** (~4 Stunden)

- [ ] **4.3.1 Load Full History**
  - [ ] Quelle: `tooling/legacy/Evoki_History_Archive/2025/`
  - [ ] Script: `scripts/import_full_history.py`
  - [ ] **Was du sehen solltest**:
    ```
    ğŸ“¥ Importing Full History (22.000 Paare)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    [Progress bar 100%]
    
    âœ… 22.000 Paare importiert
    â±ï¸  Zeit: ~34.0s (650 Paare/s)
    
    ğŸ“Š Final Stats:
       â€¢ evoki_v3_core.db:         22.000 Paare, ~110 MB
       â€¢ evoki_v3_analytics.db:    ~7M EintrÃ¤ge, ~480 MB
       â€¢ evoki_v3_trajectories.db: ~17.6k Futures, ~230 MB
       â€¢ evoki_v3_keywords.db:     ~100k Keywords, ~25 MB
       â€¢ evoki_v3_graph.db:        22k Nodes, 110k Edges, ~67 MB
       
    ğŸ“Š FAISS Indizes:
       â€¢ atomic_pairs:    22.000 Vektoren, ~34 MB
       â€¢ metrics_wpf:     22.000 Vektoren, ~28 MB
       â€¢ trajectory_wpf:  ~17.6k Vektoren, ~7 MB
       
    âœ… TOTAL: ~912 MB SQLite + ~69 MB FAISS = ~981 MB
    ```

- [ ] **4.3.2 Validierung Full Dataset**
  - [ ] SQL Queries auf allen 5 DBs
  - [ ] FAISS Search Tests (alle 3 Namespaces)
  - [ ] Hybrid Search Test
  - [ ] Trajectory Prediction Test
  - [ ] **Was du sehen solltest**: Alle Tests erfolgreich, 0 Fehler

#### **4.4 Performance Benchmark** (~2 Stunden)

- [ ] **4.4.1 Benchmark Script**
  - [ ] Script: `scripts/benchmark.py`
  - [ ] Tests:
    - [ ] Import Speed (Paare/s)
    - [ ] FAISS Search Speed (ms/query)
    - [ ] DB Query Speed (ms/query)
    - [ ] Full Pipeline Latency (ms/Prompt)

- [ ] **4.4.2 Run Benchmark**
  - [ ] **Was du sehen solltest**:
    ```
    âš¡ Performance Benchmark
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    ğŸ“Š Import Speed:
       â€¢ Metrik-Berechnung:   ~50ms/Paar
       â€¢ DB Write:            ~20ms/Paar
       â€¢ FAISS Update:        ~5ms/Paar
       â€¢ TOTAL:               ~610 Paare/s
       
    ğŸ” Search Speed:
       â€¢ FAISS Semantic:      ~5ms
       â€¢ FAISS Metrics:       ~5ms
       â€¢ FAISS Trajectory:    ~5ms
       â€¢ Keyword Search:      ~3ms
       â€¢ Hybrid Search:       ~15ms (parallel)
       
    ğŸš€ Full Pipeline Latency:
       â€¢ Context Build:       ~100ms
       â€¢ API Call:            ~500ms (real API)
       â€¢ DB Updates:          ~200ms
       â€¢ TOTAL:               ~800ms/Prompt
       
    âœ… Performance Target erreicht: \u003c1s per Prompt
    ```

### **PHASE 4 DONE CHECKLIST**:
- âœ… Dual-Response API Client funktioniert
- âœ… Full Pipeline End-to-End getestet
- âœ… 11.016 Paare erfolgreich importiert
- âœ… Alle 5 DBs + 3 FAISS-Indizes operational
- âœ… Performance: ~610 Paare/s Import, ~800ms Full Pipeline Latency
- âœ… Validierung: 0 Fehler, alle Features funktionieren

**ZEIT**: ~1-2 Tage (8-16 Stunden)  
**OUTPUT**: Production-Ready V3.0 Data Layer

---

## ğŸ“Š **GESAMT-ZUSAMMENFASSUNG**

### **Total Zeit**: ~1 Woche (40-60 Stunden fokussierte Arbeit)

### **Deliverables**:

1. âœ… **5 SQLite-Datenbanken** (~459 MB mit 11k Paaren)
   - evoki_v3_core.db
   - evoki_v3_graph.db
   - evoki_v3_keywords.db
   - evoki_v3_analytics.db
   - evoki_v3_trajectories.db

2. âœ… **3 FAISS-Namespaces** (~35 MB mit 11k Paaren)
   - atomic_pairs (384D semantic)
   - metrics_wpf (322D metric-based)
   - trajectory_wpf (~30D trajectory-based)

3. âœ… **Python Pipeline-Scripts**:
   - v3_importer.py
   - analytics_logger.py
   - trajectory_calculator.py
   - learning_keyword_engine.py
   - graph_builder.py
   - dual_response_api_client.py
   - full_pipeline.py

4. âœ… **Performance**:
   - Import: ~610 Paare/s
   - Search: ~5-15ms
   - Full Pipeline: ~800ms/Prompt

5. âœ… **Features**:
   - Dual-Gradient System (âˆ‡A / âˆ‡B)
   - B-Vektor Evolution (7D Soul Signature)
   - Session Chain (Kryptografische IntegritÃ¤t)
   - Learning Keyword Engine (Auto-Promotion)
   - Metric Trajectory Predictor (Prognosen)
   - Historical Futures (was kam danach?)
   - Hybrid Search (Keyword + Semantic + Metric)

---

## âš ï¸ **KRITISCHE REGELN**

1. **ERST 1000 Paare validieren, DANN skalieren**
2. **Jede Phase MUSS erfolgreich abgeschlossen sein** bevor nÃ¤chste Phase startet
3. **0 Fehler Toleranz** fÃ¼r Core-Features
4. **Alle Outputs loggen** fÃ¼r Debugging
5. **Tests FIRST** (Dry-Run mit 10 Paaren, dann 1000, dann 11k)

---

## ğŸ“ **DATEI-STRUKTUR (Final)**

```
C:\Evoki V3.0 APK-Lokalhost-Google Cloude\
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ evoki_v3_core_schema.sql
â”‚   â”‚   â”œâ”€â”€ evoki_v3_graph_schema.sql
â”‚   â”‚   â”œâ”€â”€ evoki_v3_keywords_schema.sql
â”‚   â”‚   â”œâ”€â”€ evoki_v3_analytics_schema.sql
â”‚   â”‚   â””â”€â”€ evoki_v3_trajectories_schema.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ v3_data_layer/
â”‚   â”‚   â”œâ”€â”€ databases/
â”‚   â”‚   â”‚   â”œâ”€â”€ evoki_v3_core.db
â”‚   â”‚   â”‚   â”œâ”€â”€ evoki_v3_graph.db
â”‚   â”‚   â”‚   â”œâ”€â”€ evoki_v3_keywords.db
â”‚   â”‚   â”‚   â”œâ”€â”€ evoki_v3_analytics.db
â”‚   â”‚   â”‚   â””â”€â”€ evoki_v3_trajectories.db
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ faiss_indices/
â”‚   â”‚   â”‚   â”œâ”€â”€ atomic_pairs.index
â”‚   â”‚   â”‚   â”œâ”€â”€ atomic_pairs.meta.json
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics_wpf.index
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics_wpf.meta.json
â”‚   â”‚   â”‚   â”œâ”€â”€ trajectory_wpf.index
â”‚   â”‚   â”‚   â””â”€â”€ trajectory_wpf.meta.json
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ all-MiniLM-L6-v2/
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ init_v3_core_db.py
â”‚   â”‚   â”‚   â”œâ”€â”€ v3_importer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analytics_logger.py
â”‚   â”‚   â”‚   â”œâ”€â”€ trajectory_calculator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ learning_keyword_engine.py
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_builder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dual_response_api_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ full_pipeline.py
â”‚   â”‚   â”‚   â””â”€â”€ benchmark.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_faiss_search.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_hybrid_search.py
â”‚   â”‚   â”‚   â””â”€â”€ test_full_pipeline.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ logs/
â”‚   â”‚       â””â”€â”€ import.log
â”‚   â”‚
â”‚   â””â”€â”€ test_data/
â”‚       â””â”€â”€ 1000_pairs/
â”‚
â””â”€â”€ V3_DATA_LAYER_ROADMAP.md (diese Datei)
```

---

**ENDE ROADMAP**

**NÃ¤chster Schritt**: Starte mit **PHASE 0 TODO #0.1** (Cleanup alte Artefakte)
