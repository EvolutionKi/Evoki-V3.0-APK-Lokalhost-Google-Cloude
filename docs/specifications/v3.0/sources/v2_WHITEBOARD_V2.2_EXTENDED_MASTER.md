# üß≠ WHITEBOARD_V2.1_FIXED.md
## EVOKI ‚Äî V2 Reparatur-Patch (konfliktfest) + Legacy-V2 Anhang

**Version:** V2.1.0 (Patch auf V2)  
**Datum:** 29. Dezember 2025  
**Status:** Fix-Patch (Production-Blocker behoben, Widerspr√ºche entsch√§rft)  

> Ziel: Deine V2 bleibt als historischer Kontext erhalten, aber die **bindenden** Regeln sind oben konsolidiert und widerspruchsfrei.
> Alles im Anhang ist **Legacy** und darf die Regeln hier oben nicht √ºberschreiben.

---

## 0) Geltung & Konfliktaufl√∂sung

Dieses Dokument hat 2 Bereiche:

1. **A. Normative Fix-Spezifikation (BINDEND)** ‚Äî gilt sofort, √ºberschreibt widerspr√ºchliche Stellen im Legacy-Anhang  
2. **B. Legacy-V2 (NICHT-NORMATIV)** ‚Äî historische Notizen / Entw√ºrfe / Roadmap

**Konfliktregel:** Wenn A und B widersprechen, gilt **A**.

**Normative Marker:**  
- **[MUST]** zwingend  
- **[MUST NOT]** verboten  
- **[SHOULD]** Standard  
- **[MAY]** optional / experimentell

---

## A) Normative Fix-Spezifikation (BINDEND)

### A1) Master Endpoint Truth Table (Single Source of Truth)

‚úÖ **IMPLEMENTIERT / ZIEL-ENDPOINTS (authoritative)**  
- **GET** `/health`  
- **GET** `/api/v1/health`  
- **GET** `/api/v1/status`  
- **POST** `/api/bridge/process` *(Legacy non-streaming)*  
- **POST** `/api/bridge/stream` *(SSE Streaming ‚Äî ersetzt Timeout-Probleme)*

‚ùå **OPTIONAL / SP√ÑTER** *(nur implementieren, wenn wirklich gebraucht; sonst weglassen)*  
- `GET /api/pipeline/logs`  
- `GET /api/v1/system/errors`  
- `GET /api/v1/trialog/session`  
- `GET /api/v1/context/daily`  
- `GET /api/history/trialog/load`  
- `POST /api/history/trialog/save`

> **[MUST]** Alle weiteren Stellen im Dokument d√ºrfen Endpoints nur via diese Tabelle referenzieren.

---

### A2) Frontend Build Safety (Native Modules Kill-Switch)

**Problem:** Native Node-Module im Frontend crashen Vite / Browser-Bundle.

- **[MUST NOT]** `better-sqlite3` im Frontend  
- **[MUST NOT]** `sqlite3` im Frontend  
- **[MUST NOT]** Node.js native bindings / `fs`, `path`, `child_process`, usw. im Browser-Bundle  
- **[MUST]** DB-Operationen **Backend-only** (oder WASM wie `sql.js`)

**CI Guardrails (minimal & effektiv):**
```bash
cd frontend
grep -r "better-sqlite3" package.json && exit 1 || true
grep -r '"sqlite3"' package.json && exit 1 || true
npm run build
```

---

### A3) Datenwahrheit: SQL ist Text-Truth

- **[MUST]** SQL Source DB ist die autoritative Textquelle.  
- **[MUST NOT]** FAISS-Reassembly √ºberschreibt SQL-Text.  
- **[MUST]** Bei Divergenz: Hash/Length-Check + Log, **SQL gewinnt**.

---

### A4) SSE Streaming: Heartbeat + Cancel-Safety (Production-Blocker Fix)

**SSE Headers (MUST):**
```js
res.writeHead(200, {
  "Content-Type": "text/event-stream",
  "Cache-Control": "no-cache, no-transform",
  "Connection": "keep-alive",
  "X-Accel-Buffering": "no"
});
```

**Heartbeat (MUST):** alle 15‚Äì30s, auch ohne Payload:
```js
const hb = setInterval(() => {
  if (!res.writableEnded) res.write(": heartbeat\n\n");
}, 20000);
```

**Cancellation transitive (MUST):**
- **[MUST]** Ein per-request `AbortController`  
- **[MUST]** `req.on("close")` ‚Üí abort  
- **[MUST]** Signal in **ALLE** Subsysteme weitergeben (FAISS/Python, DB Reads, LLM Calls)

```js
app.post("/api/bridge/stream", async (req, res) => {
  const ac = new AbortController();

  req.on("close", () => ac.abort());

  try {
    await runPipeline(req.body, {
      signal: ac.signal,
      onProgress: (evt) => res.write(`data: ${JSON.stringify(evt)}\n\n`)
    });
  } catch (e) {
    res.write(`data: ${JSON.stringify({ type:"error", message:e.message })}\n\n`);
  } finally {
    clearInterval(hb);
    res.end();
  }
});
```

---

### A5) Health Checks sind passiv (kein Abort-Sharing)

- **[MUST NOT]** Health Checks d√ºrfen **nie** globale Abort-Chains triggern  
- **[MUST]** Health Checks sind read-only (keine Worker-Restarts, kein Cache-Clear)

---

### A6) Token Mode Naming (V2-Fix)

V2 hatte die semantische Falle ‚ÄûQuick gr√∂√üer als Standard‚Äú. Fix:

- **Quick:** 20k  
- **Standard:** 25k  
- **Unlimited:** 1M

> **[MUST]** Standard ‚â• Quick (Name muss zur Gr√∂√üe passen).

---

### A7) Metriken Count Fix (V14)

- **[MUST]** Referenziere konsistent: **153 Metriken (V14 Core)**  
- **[MUST NOT]** veraltetes ‚Äû120+‚Äú in bindenden Passagen.

---

## B) Legacy-V2 (NICHT-NORMATIV, HISTORISCH)

> Hinweis: Der folgende Block ist **dein urspr√ºngliches V2-Material** (automatisch mit ein paar Markierungen versehen),
> aber **nicht bindend**. Bei Widerspruch gilt immer Abschnitt A.

---

# WHITEBOARD_V2.md

# === ORIGINAL WHITEBOARD (UNVER√ÑNDERT) ===

Ôªø# üåå EVOKI V2.0 - WHITEBOARD (Ideensammlung)

**Datum:** 28. Dezember 2025  
**Status:** Entwicklungs-Discovery & Architektur-Mapping  
**Zweck:** Keine To-Do-Liste, nur Ideensammlung und Erkenntnisse

---

## üîç **ARCHITEKTUR-BLIND SPOTS & FUTURE VISION**

### 1. Identifizierte Blind Spots und versteckte Problembereiche
Trotz der Korrekturen in V3 gibt es architektonische "blinde Flecken", die bei fortschreitender Nutzung kritisch werden:

* **Das "Context-Drift" Paradoxon:** Das System webt Kontext aus ¬±2 Prompts um einen Treffer. **Blind Spot:** Wenn die Historie auf √ºber 100.000 Chunks anw√§chst, k√∂nnten die "Metrik-Zwillinge" (SQL-Treffer) aus v√∂llig unterschiedlichen Lebensphasen stammen. Der Orchestrator braucht eine **Time Decay Funktion**, die verhindert, dass uralte Metriken die aktuelle Analyse "vergiften".
* **LocalStorage als "Flaschenhals-Sackgasse":** Die Quellen warnen vor dem 4MB-Limit. **Blind Spot:** Selbst beim Ausweichen auf Backend-Logs bleibt der React-State der Single-Point-of-Failure. Bei 1M Tokens friert das UI ein. **L√∂sung:** Virtualisierung (react-window) und Partial State Updates sind zwingend.
* **Die "Finetuning-Echokammer":** Die "Labor-Strategie" sieht vor, Modelle mit den eigenen Chunks zu trainieren. **Risiko:** Wenn wir auf halluzinierten V1-Daten trainieren, zementieren wir Fehler. Wir brauchen ein "Golden Set" (verifizierte Chunks) f√ºr das Training.
* **Sentinel-Veto vs. LLM-Konfidenz:** Der Sentinel kann Scores massiv senken. **Blind Spot:** Wenn alle Top-Kandidaten blockiert werden, sendet das System "Restm√ºll". Wir brauchen einen **Emergency Refetch**, der bei Veto sofort neue, sicherere Parameter sucht.

### 2. Ungenutztes Potenzial der Architektur
* **Pr√§diktive Trauma-Warnung (Early Warning):** Da wir 153 Metriken (V14 Core) Metriken live haben, k√∂nnen wir die **Ableitung der PCI-Kurve** berechnen. Steigt sie √ºber 3 Sessions stetig an? Warnung VOR dem Crash.
* **Automatisierte Metaphern-Synthese:** "Perfect Agreements" zwischen Metrik und Semantik k√∂nnen genutzt werden, um individuelle therapeutische Metaphern zu generieren.
* **Trialog als Architektur-Optimierer:** Der Analyst-Agent k√∂nnte die `performance_log.db` lesen und selbstst√§ndig Indizes rebalancen ("Self-Optimizing Architecture").

### 3. Vision√§re Erweiterungen
* **Sovereign Personal AI:** Durch die Kombination von "Labor-Strategie" (Cloud-Training) und lokaler Inference (GTX 3060) wird Evoki zur **Black Box f√ºr das Ich** ‚Äì 100% offline, 100% privat, Cloud-Qualit√§t.
* **Cross-Session Chronicle:** Weg vom Append-Only Log hin zu einer dynamischen Wissenskarte, die Cluster im Deep Storage visualisiert.

## üìç **FRONTEND KOMPONENTEN - AKTUELLER STATUS**

### ‚úÖ **EVOKI TEMPEL V3 - HYPERSPACE EDITION** (Produktiv)
- **Datei:** `frontend/src/components/EvokiTempleChat.tsx`
- **Version:** V3 - Hyperspace Edition
- **Status:** ‚úÖ AKTIV - Das ist der ECHTE Evoki Tempel
- **Features:**
  - 12-Database Distribuierte Speicherung
  - Token-Limits: 25k (quick), 20k (standard), 1M (max)
  - SHA256 Chain-Logik mit kontinuierlicher Liste
  - Metriken-Berechnung auf alle DBs (153 Metriken (V14 Core))
  - A65 Multi-Candidate Selection (3 Kandidaten)
  - Phase 4 Token Distribution:
    - 32% Narrative Context (8.000 Tokens)
    - 12% Top-3 Chunks (3.000 Tokens)
    - 20% Overlapping Reserve (5.000 Tokens)
    - 4% RAG Chunks (1.000 Tokens)
    - 32% Response Generation (8.000 Tokens)
- **Backend Endpoint:** `/api/bridge/process`
- **Vektorisierung:** Live mit allen 153 Metriken (V14 Core) Metriken

### ‚ö†Ô∏è **CHATBOT PANEL** (Legacy aus V1)
- **Datei:** `frontend/src/components/ChatbotPanel.tsx`
- **Version:** V1 - Generischer Chatbot
- **Status:** üü° OBSOLET - War der erste generische Google-Chatbot
- **Historie:**
  - Urspr√ºnglich: Generische Google API Interaktion
  - Dann: Erster "Tempel"-√§hnlicher Anschluss (aus Respekt zu Evoki nicht so genannt)
  - Jetzt: Durch EvokiTempleChat V3 ersetzt
- **Backend Endpoint:** `/api/bridge/process` (gleicher wie V3, aber weniger Features)
- **Unterschied zu V3:**
  - Keine 12-DB Distribution
  - Keine Phase 4 Token Distribution
  - Keine Tempel-Metriken
  - Keine SHA256 Chain
  - Kein A65 Multi-Candidate
- **Idee:** K√∂nnte entfernt oder als "Simple Chat Mode" behalten werden

---

## üîç **PIPELINE-√úBERWACHUNG**

### ‚úÖ **PIPELINE LOG PANEL** (Implementiert)
- **Datei:** `frontend/src/components/PipelineLogPanel.tsx`
- **Status:** ‚úÖ VORHANDEN als Tab 12
- **Zweck:** Trackt ALLE √úbergabepunkte f√ºr Fehlerdiagnose
- **12 Protokollierte Schritte:**
  1. User Input ‚Üí Frontend
  2. Frontend ‚Üí Backend (`/api/bridge/process`)
  3. Backend ‚Üí Python FastAPI Service (`POST localhost:8000/search`) ‚ö†Ô∏è **NICHT CLI-Spawn!**
  4. Python FAISS ‚Üí JSON Output
  5. Backend Parse ‚Üí DualBackendBridge
  6. DualBackendBridge ‚Üí Trinity Engines
  7. Trinity Results ‚Üí A65 Candidate Selection
  8. A65 ‚Üí GeminiContextBridge
  9. Context Building ‚Üí Gemini Prompt
  10. Gemini API Call ‚Üí Response
  11. Response ‚Üí Vector Storage (12 DBs)
  12. Final Response ‚Üí Frontend

**üîß IMPLEMENTATION NOTE:**
- **Legacy-Konzept:** `spawn(pythonPath, ['query.py', prompt])` (2-5s Modell-Ladezeit pro Request)
- **Production-Reality:** Persistenter FastAPI Microservice (Port 8000)
  - L√§dt sentence-transformers + FAISS **einmal** beim Systemstart (30s)
  - Requests: `POST http://localhost:8000/search` (<100ms pro Request)
  - Endpoints: `/search`, `/health`, `/reload-index`
- **Grund:** CLI-Spawn w√ºrde FAISS bei jedem Request neu laden ‚Üí Timeout-H√∂lle

### ‚ùå **BACKEND ENDPOINT FEHLT**
- **Erwartet:** `GET /api/pipeline/logs`
- **Status:** ‚ùå NICHT IMPLEMENTIERT in `backend/server.js`
- **Frontend Code:** Line 128 in PipelineLogPanel.tsx ruft es auf
- **Idee:** Backend muss Pipeline-Logs persistieren (JSONL-File oder SQLite)
- **Daten-Struktur:**
  ```typescript
  interface PipelineLogEntry {
    id: string;
    timestamp: string;
    session_id: string;
    message_id: string;
    step_number: number; // 1-12
    step_name: string;
    data_transfer: {
      from: string;
      to: string;
      text_preview: string; // Erste 200 Zeichen
      full_text: string;
      size_bytes: number;
      token_count?: number;
    };
    metadata?: Record<string, any>;
  }
  ```
- **Zweck:** Mikro-Tuning wenn Google API unpasende Antworten liefert
- **Use Case:** Fehlerquelle direkt identifizieren (FAISS? Trinity? Gemini?)

---

## üîê **GENESIS ANCHOR (A51)**

### ‚úÖ **IMPLEMENTIERT ABER DEAKTIVIERT**
- **Datei:** `backend/server.js` Line 26-62
- **Status:** üü° WARNUNG-MODUS (nicht kritisch w√§hrend Entwicklung)
- **Funktion:** `verifyGenesisAnchor()`
- **Verhalten:**
  - Pr√ºft `backend/public/genesis_anchor_v12.json`
  - Wenn NICHT gefunden: ‚ö†Ô∏è WARNING, aber Server startet
  - Wenn MALFORMED: ‚ùå FATAL, Server Exit
  - Wenn OK: ‚úÖ Loggt SHA256/CRC32 Hashes
- **Gepr√ºfte Werte:**
  - `engine.combined_sha256` (Combined Hash Regelwerk + Registry)
  - `engine.regelwerk_crc32`
  - `engine.registry_crc32`
- **Idee f√ºr sp√§ter:** Nach Stabilisierung re-enablen als Produktionsschutz
- **Entwicklungs-Bypass:** Aktuell durch "Datei nicht gefunden" ‚Üí Warning statt Exit

---

## üß© **LOSE ENDEN & OBSOLETE FEATURES**

### üì∏ **SNAPSHOT/SCREENSHOT SYSTEM**
- **Status:** üü° HALB-OBSOLET
- **Service:** `frontend/src/services/core/snapshotService.ts`
- **Funktionen:**
  - `saveSnapshotToFile(appState)` - Speichert kompletten App-State als JSON
  - `loadSnapshotFromFile(file)` - L√§dt State aus File
- **Verwendet in:**
  - `Header.tsx` Line 44, 52 (Save/Load Buttons)
  - `App.tsx` Line 943-944 (Handler)
- **Historie:**
  - **V1:** Download-basierte Persistenz (localStorage-Backup als JSON)
  - **V2:** Wird durch echtes Backend mit Auto-Save ersetzt
- **Idee:** 
  - Behalten f√ºr manuelle Backups?
  - Oder komplett entfernen zugunsten Backend-Persistenz?
  - K√∂nnte n√ºtzlich sein f√ºr "Export gesamte Session"

### üíæ **CACHE-MANAGEMENT**
- **Status:** üîç ZU PR√úFEN
- **M√∂gliche Komponenten:**
  - `DataCachePanel.tsx` (falls vorhanden)
  - LocalStorage-basierte Caches
  - Service Worker Caches
- **Idee:** Nur minimal cachen, Backend ist Source of Truth
- **Use Case:** Offline-F√§higkeit f√ºr Trialog? (sp√§ter)

### üìä **WEITERE UI-TOOLS MIT BACKEND-ANBINDUNG**

#### ‚úÖ **ObsidianLiveStatus** (Operational-KI Status)
- **Datei:** `frontend/src/components/ObsidianLiveStatus.tsx`
- **Endpoint:** `GET /api/v1/health`
- **Zweck:** Backend Health Check
- **Status:** ‚úÖ AKTIV

#### ‚úÖ **TrialogPanel** (Multi-Agent System)
- **Datei:** `frontend/src/components/TrialogPanel.tsx`
- **Endpoints:**
  - `GET /api/v1/trialog/session` (Session laden)
  - `POST /api/v1/interact` (Agent Response)
  - `GET /api/v1/context/daily` (Daily Context)
- **Status:** ‚úÖ AKTIV

#### ‚úÖ **ErrorLogPanel** (Fehlerprotokoll)
- **Datei:** `frontend/src/components/ErrorLogPanel.tsx`
- **Endpoint:** `GET /api/v1/system/errors`
- **Zweck:** Backend-persistierte Fehler abrufen
- **Status:** ‚úÖ AKTIV

#### ‚úÖ **VoiceSettingsPanel** (TTS)
- **Datei:** `frontend/src/components/VoiceSettingsPanel.tsx`
- **Endpoint:** `POST https://api.openai.com/v1/audio/speech` (Extern)
- **Zweck:** Text-to-Speech via OpenAI
- **Status:** ‚úÖ AKTIV

#### ‚úÖ **App.tsx Global Endpoints**
- `GET /api/v1/status` - Backend Status (Line 523)
- `GET /api/v1/health` - Health Check (Line 536)
- `GET /api/history/trialog/load` - Trialog Historie laden (Line 770)
- `POST /api/history/trialog/save` - Trialog Historie speichern (Line 814)

---

## üîó **VOLLST√ÑNDIGE BACKEND-ENDPOINTS LISTE**

### ‚úÖ **IMPLEMENTIERT IN BACKEND:**
- `GET /health` ‚Üí Backend Health
- `GET /api/v1/status` ‚Üí Enhanced Status mit Hyperspace Info
- `POST /api/bridge/process` ‚Üí **HAUPT-PIPELINE** (DualBackendBridge)
- `POST /api/temple/session/save` ‚Üí Tempel Session speichern
- `POST /api/temple/process` ‚Üí Enhanced Tempel (mit A65)
- `POST /api/v1/interact` ‚Üí Trialog Interaction
- `GET /api/temple/debug` ‚Üí Vector DB Debug
- `GET /api/temple/debug-full` ‚Üí Full Request Debug

### ‚ùå **FEHLT NOCH (Frontend ruft auf, Backend fehlt):**
- `GET /api/pipeline/logs` ‚Üí Pipeline Log Entries
- `GET /api/v1/system/errors` ‚Üí Error Log Persistence
- `GET /api/v1/trialog/session` ‚Üí Trialog Session Info
- `GET /api/v1/context/daily` ‚Üí Daily Context
- `GET /api/history/trialog/load` ‚Üí Trialog History Load
- `POST /api/history/trialog/save` ‚Üí Trialog History Save

---

## üéØ **ERKENNTNISSE & IDEEN**

### **1. ChatbotPanel.tsx Entfernen?**
- **Pro Entfernung:**
  - Komplett durch EvokiTempleChat V3 ersetzt
  - Obsolete Features (keine 12-DB, kein A65, keine Phase 4)
  - Verwirrt beim Debugging (zwei √§hnliche Komponenten)
- **Pro Behalten:**
  - Als "Simple Mode" f√ºr schnelle Tests
  - Backup falls V3 Probleme macht
  - Historischer Wert (erste Implementation)
- **Idee:** Umbenennen in `LegacyChatbot.tsx` + deaktivieren im Tab-System

### **2. Pipeline-Logging Backend implementieren**
- **Warum wichtig:**
  - Fehlerquelle SOFORT identifizieren
  - Mikro-Tuning wenn Gemini seltsame Antworten gibt
  - Performance-Analyse (welcher Schritt ist langsam?)
- **Implementation:**
  - JSONL-File: `backend/logs/pipeline_logs.jsonl`
  - Jeden Schritt loggen mit Timestamps
  - Endpoint: `GET /api/pipeline/logs?session_id=...`
  - Auto-rotate bei 100MB (max 10 Files)
- **Integration:** Bereits in DualBackendBridge.js Line 46-51 vorbereitet!

### **3. Genesis Anchor Re-enablement nach Stabilisierung**
- **Aktuell:** Warnung-Modus (Entwicklung)
- **Sp√§ter:** Kritisch-Modus (Produktion)
- **Idee:** Environment Variable `GENESIS_ANCHOR_STRICT=false/true`
- **Zweck:** Verhindert unauthorisierte Regelwerk-√Ñnderungen

### **4. Snapshot-System Evolution**
- **V1:** Download JSON (keine Persistenz)
- **V2:** Backend Auto-Save (geplant)
- **Idee:** Snapshots als "Session Export" behalten
  - User kann komplette Session als JSON downloaden
  - Forensische Analyse m√∂glich
  - Kann in anderen Evoki-Instanzen importiert werden
  - Format: `evoki_session_export_20251228_153045.json`

### **5. Cache-Strategie kl√§ren**
- **Prinzip:** Backend = Source of Truth
- **Frontend Cache:** Nur f√ºr UI-Performance
  - Aktuelle Session in Memory
  - Keine LocalStorage-Persistenz von Vektordaten
  - Service Worker nur f√ºr Assets, nicht f√ºr API-Responses
- **Backend Cache:**
  - FAISS Indices im Memory halten (schneller)
  - Trinity Results cachen? (√ºberpr√ºfen)

### **6. V1-Daten Import vorbereiten**
- **Quelle:** Deine 02.25-10.25 Chathistorie (vektorisiert)
- **Ziel:** In 12 Vector DBs + Chronologische Historie importieren
- **Format:** Bereits vorhanden als `chunks_v2_2.pkl` + FAISS Index
- **Idee:** Import-Script f√ºr historische Daten
  - Liest V1 Chunks
  - Berechnet 153 Metriken (V14 Core) Metriken nachtr√§glich
  - Schreibt in neue 12-DB Struktur
  - Erh√§lt Timecodes & Session-IDs

### **7. Trialog Backend-Anbindung komplettieren**
- **Status:** Endpoints im Frontend vorhanden, Backend fehlt teilweise
- **Idee:** Trialog separate Session-Verwaltung
  - Eigene Vector DBs (4 DBs: trialog_W_m2, trialog_W_m5, trialog_W_p25, trialog_W_p5)
  - Multi-Agent Responses speichern
  - Chronicle-Integration f√ºr Meta-Statements
  - Auto-TTS per Agent-Profil

---

## üß™ **TEST-IDEEN**

### **Test 1: Ersten Tempel-Prompt schicken**
- **Ziel:** Pipeline End-to-End verifizieren
- **Prompt:** "Erz√§hl mir von den Zwillingen im Kindergarten"
- **Erwartung:**
  - FAISS findet relevante Chunks
  - Trinity kombiniert mit Metriken
  - A65 selektiert besten Kandidaten
  - Gemini generiert kontextuelle Antwort
  - 12 DBs werden beschrieben
  - Chronologische Historie entsteht

### **Test 2: Trialog erste Session**
- **Ziel:** Multi-Agent System testen
- **Agents:** Analyst + Regel + Synapse (Explorer & Connector)
- **Prompt:** "Analysiert die aktuelle Evoki V2.0 Architektur"
- **Erwartung:**
  - 3 Agents antworten nacheinander
  - Jede Antwort in Vector DB
  - Chronicle-Eintrag mit Meta-Statement
  - TTS f√ºr jeden Agent (falls aktiviert)

### **Test 3: Pipeline-Log Analyse**
- **Ziel:** √úbergabepunkte sichtbar machen
- **Methode:** Test 1 wiederholen + Pipeline-Log √∂ffnen
- **Erwartung:**
  - 12 Steps sichtbar
  - Text-Preview f√ºr jeden Step
  - Token-Counts korrekt
  - Timestamps nachvollziehbar

---

## üí° **N√ÑCHSTE SCHRITTE (KEINE TO-DO, NUR IDEEN)**

1. **Backend starten & Test 1 durchf√ºhren**
2. **Pipeline-Logging Backend implementieren**
3. **Fehlende Trialog-Endpoints implementieren**
4. **ChatbotPanel.tsx Entscheidung treffen**
5. **V1-Daten Import-Script entwickeln**
6. **Genesis Anchor Environment Variable**
7. **Snapshot-System zu "Session Export" umbauen**
8. **Cache-Strategie dokumentieren**

---

## ÔøΩ **LOCALSTORAGE & CACHE-ANALYSE**

### ‚úÖ **LocalStorage Nutzung (VOLLST√ÑNDIG ERFASST):**

#### **1. Auto-Save System (App.tsx)**
- **Key:** `evoki_autosave`
- **Content:** `{ apiConfig, activeTab, ... }`
- **Limit:** 4MB (LOCAL_STORAGE_LIMIT_BYTES)
- **Auto-Save Interval:** 30s (Handler in App.tsx Line 635)
- **Warning:** Zeigt Warnung bei >3.8MB
- **Risiko:** üü° MITTEL - Bei gro√üen Sessions k√∂nnte Limit erreicht werden
- **Fix:** Backend-Persistenz f√ºr gro√üe Daten nutzen

#### **2. Voice Settings (VoiceSettingsPanel.tsx)**
- **Keys:**
  - `openai_api_key` - OpenAI TTS API Key
  - `evoki_voice` - Selected Voice (alloy, echo, fable, onyx, nova, shimmer)
- **Risiko:** üü¢ NIEDRIG - Kleine Daten, nur Settings

#### **3. Backend URL (TrialogPanel.tsx)**
- **Key:** `evoki_backend_url`
- **Content:** Backend API URL (http://localhost:3001)
- **Risiko:** üü¢ NIEDRIG - Nur String

#### **4. Chronicle Worker (chronicleWorkerClient.ts)**
- **Key:** `CHRONICLE_STORAGE_KEY` (Konstante)
- **Content:** ChronicleEntry[]
- **Risiko:** üü° MITTEL - W√§chst mit jeder Meta-Statement
- **Note:** Chatbot Panel entfernt, Chronicle-Integration deaktiviert

#### **5. Integrity Worker (integrityWorkerClient.ts)**
- **Keys:**
  - `LOGBOOK_STORAGE_KEY` - ProjectLogbook Entries
  - `APP_ERRORS_STORAGE_KEY` - ApplicationError[]
- **Risiko:** üü° MITTEL - Error-Log kann gro√ü werden
- **Circuit Breaker:** Bei QuotaExceeded ‚Üí stoppt Speicherung

#### **6. Browser Storage Adapter (BrowserStorageAdapter.ts)**
- **Keys:**
  - `evoki_memory` - Engine Memory State
  - `evoki_chronik` - Engine Chronik (Append-Only Log)
- **Risiko:** üî¥ HOCH - Chronik w√§chst unbegrenzt (Append-Only!)
- **Note:** "Not fully implemented" laut Code

### ‚ö†Ô∏è **POTENTIELLE PROBLEME:**

1. **Auto-Save 4MB Limit:**
   - Bei vielen Trialog-Nachrichten ‚Üí QuotaExceeded
   - Fix: Backend-Persistenz nutzen, LocalStorage nur f√ºr UI-State

2. **Chronik Append-Only:**
   - Keine Rotation, keine Limits
   - Fix: Implementiere Rotation oder deaktiviere komplett

3. **Circuit Breaker nicht √ºberall:**
   - Nur in integrityWorkerClient implementiert
   - Fix: Alle LocalStorage-Writes mit try/catch + QuotaExceeded handling

### ‚úÖ **KEINE INDEXEDDB, KEINE SESSIONSTORAGE:**
- Nur localStorage verwendet
- Keine Service Worker f√ºr Caching
- Keine komplexen Cache-Strategien

---

## üöÄ **STARTUP-SEQUENZ ANALYSE**

### **Loading Screen (App.tsx Line 6-70)**
- **Zweck:** Backend Health Check vor App-Start
- **Sequence:**
  1. Versucht Python Backend (Port 8000) - `/health`
  2. Fallback: Node Backend (Port 3001) - `/health`
  3. Wartet 3s bei Erfolg, 5s bei Fehler
  4. Ruft `onSystemReady()` auf
  5. App wird angezeigt
- **Status:** ‚úÖ IMPLEMENTIERT
- **Risiko:** üü° MITTEL - 5s Timeout bei offline Backend k√∂nnte nerven

### **Genesis Startup Screen (GenesisStartupScreen.tsx)**
- **Zweck:** A51 Security Checks
- **5 Schritte:**
  1. Frontend Genesis Hash Integrity
  2. Backend Connection
  3. Backend Genesis Anchor Verification
  4. Security Protocols (A51)
  5. System Initialization
- **Status:** üü° OPTIONAL - Aktuell durch `isSystemReady = true` in App.tsx bypassed
- **Note:** "FIXED: Start ready, show app immediately" (App.tsx Line 180)

### **Engine Initialization (App.tsx Line 556)**
- **Sequence:**
  1. `evokiEngine.init()` wird gerufen
  2. Bei Erfolg: `genesisStatus = 'verified'`
  3. Bei Fehler: `genesisStatus = 'lockdown'` m√∂glich
  4. Parallel Architecture Status Updates
- **Status:** ‚úÖ IMPLEMENTIERT

### **Backend Health Check Loop (App.tsx Line 518)**
- **Endpoint:** `GET /api/v1/status` (prim√§r) oder `GET /api/v1/health` (fallback)
- **Interval:** ‚ùå DEAKTIVIERT (Kommentar: "AbortSignal.timeout() sends SIGINT to backend!")
- **Risiko:** üî¥ HOCH - Health Check kann Backend killen!
- **Status:** üü° TEMP DISABLED

---

## üì¶ **DEPENDENCIES & VERSIONS**

### **Frontend (package.json):**
- React: 18.2.0
- Vite: 7.1.11
- TypeScript: 5.8.2
- @google/genai: 1.25.0
- @microsoft/fetch-event-source: ^2.0.4 (‚úÖ Neu f√ºr SSE Fix)
- chart.js: 4.4.2
- jszip: 3.10.1
- lucide-react: 0.363.0
- react-window: ^1.8.10 (‚úÖ Neu f√ºr Virtualization / UI-Performance)
// REMOVED: better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend) & sqlite3 (VERBOTEN im Frontend) (Crashen Vite Build!)

### **Backend (package.json):**
- express: 5.2.1
- cors: 2.8.5
- dotenv: 17.2.3
- node-fetch: 3.3.2

### ‚ö†Ô∏è **AUFF√ÑLLIGKEITEN:**

#### **üö® KRITISCH: SQLite im Frontend Package.json!**

**Das Problem:**
- `better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend): 12.5.0` (‚ùå NATIVE NODE.JS MODULE!)
- `sqlite3 (VERBOTEN im Frontend): 5.1.7` (‚ùå NATIVE NODE.JS MODULE!)

**Beide sind C++ Native Bindings und k√∂nnen NICHT im Browser laufen!**

**Konsequenzen:**
1. ‚ùå **Vite-Build wird crashen** sobald du sie importierst
2. ‚ùå Kein Zugriff auf `fs`, `path`, native bindings im Browser
3. ‚ùå Tickende Zeitbombe (aktuell nicht verwendet, aber bei Import ‚Üí Crash)

**Warum ist es drin?**
- Vermutlich aus V1 kopiert (wo Node.js Backend SQLite nutzt)
- Frontend braucht es NICHT (Backend ist Source of Truth)

**‚úÖ SOFORT-FIX:**
```bash
cd frontend
npm uninstall better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend) sqlite3 (VERBOTEN im Frontend)
```

**Alternative (falls Client-Side SQL wirklich n√∂tig f√ºr Offline-Mode):**
- **`sql.js`** (WASM-basiert, l√§uft im Browser)
- **`wa-sqlite`** (WebAssembly SQLite)

**F√ºr V2.0:** Backend ist die einzige SQL-Source. Frontend macht nur API-Calls!

---

**Weitere Auff√§lligkeiten:**
1. **Express 5.2.1:** Sehr neu, k√∂nnte Breaking Changes haben
2. **Node-Fetch:** Nur im Backend n√∂tig, nicht im Frontend

---

## üîç **ALLE 12 TABS KOMPLETT:**

### ‚úÖ **IMPLEMENTIERT & VOLLST√ÑNDIG:**
1. **Engine-Konsole** (Tab.EngineConsole) - EngineConsolePanel.tsx
2. **Trialog** (Tab.Trialog) - TrialogPanel.tsx
3. **Agenten & Teams** (Tab.AgentSelection) - AgentSelectionPanel.tsx
4. **Evoki's Tempel V3** (Tab.TempleChat) - EvokiTempleChat.tsx
5. **Metrik-Tuning** (Tab.ParameterTuning) - ParameterTuningPanel.tsx
6. **Analyse** (Tab.Analysis) - Analysis.tsx
7. **Regelwerk-Suche** (Tab.RuleSearch) - RulePanel.tsx
8. **API** (Tab.API) - ApiPanel.tsx
9. **Stimme & API** (Tab.VoiceSettings) - VoiceSettingsPanel.tsx
10. **HyperV3.0 Deep Storage** (Tab.DeepStorage) - DeepStoragePanel.tsx
11. **Fehlerprotokoll** (Tab.ErrorLog) - ErrorLogPanel.tsx
12. **Pipeline √úberwachung** (Tab.PipelineLog) - PipelineLogPanel.tsx

### ‚ö†Ô∏è **DEFAULT TAB:**
- **App.tsx Line 166:** `activeTab: Tab.Trialog`
- Beim Start wird Trialog ge√∂ffnet (nicht Tempel!)

---

## üõ°Ô∏è **ERROR HANDLING & LOGGING**

### **1. Global Error Handler (App.tsx Line 358)**
- **window.addEventListener('error')** ‚Üí addApplicationError()
- **window.addEventListener('unhandledrejection')** ‚Üí addApplicationError()
- **Lockdown Trigger:** Errors mit "GENESIS ANCHOR" oder "A51" ‚Üí `genesisStatus = 'lockdown'`

### **2. Console Capture (App.tsx Line 385)**
- **console.log/warn/error** ‚Üí redirected zu developerLog
- **Filtert:** [HMR], Auto-Save Messages
- **Risiko:** üü° MITTEL - Kann Performance bei vielen Logs beeinflussen

### **3. Fetch Interceptor (App.tsx Line 407)**
- **window.fetch** ‚Üí wrapped mit Logging
- **Logged:** Nur non-OK responses (reduziertmit Noise)
- **Excluded:** `/api/system/log-error` (verhindert Loops)
- **Risiko:** üü° MITTEL - Bei vielen API-Calls viel Overhead

### **4. Critical Error Modal (CriticalErrorModal.tsx)**
- **Trigger:** errorType === 'system' ODER keywords (infinite loop, chain break, recursion, fatal)
- **Display:** Overlay mit Error-Details
- **Action:** System Lockdown m√∂glich

### **5. Backend Error Logging (DEAKTIVIERT)**
- **App.tsx Line 338:** `POST /api/system/log-error` DISABLED
- **Reason:** "Verhindert fetch loops"
- **Status:** üü° AUSKOMMENTIERT

---

## ÔøΩ **KRITISCHE PIPELINE-ANALYSE - TIMEOUTS & RACE CONDITIONS**

### **‚ö†Ô∏è TIMEOUT-PROBLEM #1: Frontend vs Backend Race Condition**

**Das Problem:**
Frontend sendet Request mit 60s Timeout ‚Üí Backend braucht aber m√∂glicherweise l√§nger f√ºr FAISS-Suche (33.795 Chunks!) + Gemini API ‚Üí Frontend bricht ab BEVOR Backend fertig ist ‚Üí User sieht "Timeout", aber Backend arbeitet weiter ‚Üí **Zombie-Requests im Backend!**

#### **‚ö†Ô∏è TIMEOUT-PROBLEM #1: Frontend vs Backend Race Condition**

**Das Problem:**
Frontend sendet Request mit 60s Timeout ‚Üí Backend braucht aber m√∂glicherweise l√§nger f√ºr FAISS-Suche (33.795 Chunks!) + Gemini API ‚Üí Frontend bricht ab BEVOR Backend fertig ist ‚Üí User sieht "Timeout", aber Backend arbeitet weiter ‚Üí **Zombie-Requests im Backend!**

**‚ùå ALTE L√ñSUNG (Legacy-Denken):**
```typescript
// Einfach Timeout hochsetzen
AbortSignal.timeout(120000); // 120s statt 60s
```
**Problem:** User starrt 120 Sekunden auf "Laden..." ohne zu wissen was passiert!

---

**‚úÖ NEUE L√ñSUNG: "HEARTBEAT" MIT SERVER-SENT EVENTS (SSE)**

### **üîÑ SERVER-SENT EVENTS (SSE) PIPELINE-STREAMING**

**Konzept:** Backend sendet **LIVE STATUS-UPDATES** w√§hrend es rechnet!

**UX-Effekt:**
```
User sieht in Echtzeit:
‚îú‚îÄ ‚è≥ "Durchsuche 33.795 Erinnerungen..." (nach 2s)
‚îú‚îÄ üîç "FAISS fand 47 semantische Treffer" (nach 15s)
‚îú‚îÄ üìä "Analysiere emotionale Metriken..." (nach 18s)
‚îú‚îÄ ‚ö° "Hazard-Level: 0.34 | PCI: 0.72" (nach 20s)
‚îú‚îÄ üéØ "3 Kontext-Paare ausgew√§hlt" (nach 25s)
‚îú‚îÄ üß† "Verwebe 3 Zeitlinien (¬±2 Prompts)..." (nach 28s)
‚îú‚îÄ ü§ñ "GPT-4 generiert Antwort..." (nach 35s)
‚îî‚îÄ ‚úÖ "Fertig! (38s total)" (nach 38s)
```

**Technischer Vorteil:**
- Verbindung bleibt offen
- **Timeouts werden IRRELEVANT** (solange Daten flie√üen!)
- User wei√ü IMMER was gerade passiert
- Kein "schwarzes Loch" von 60-120 Sekunden

---

#### **üö® KRITISCHES PROBLEM: EventSource URL-L√§ngen-Limit!**

**Das Problem:**
`EventSource` nutzt standardm√§√üig **GET-Requests**!

```typescript
// ‚ùå GEHT NICHT f√ºr lange Prompts!
const eventSource = new EventSource(
    `${backendUrl}/api/bridge/stream?prompt=${encodeURIComponent(userPrompt)}`
);
```

**Warum nicht?**
- **GET-URL-Limit:** 2.048 - 8.192 Zeichen (Browser/Server abh√§ngig)
- **Deine Prompts:** K√∂nnen RIESIG sein (Trauma-Analysen, 80k tokens!)
- **Konsequenz:** `HTTP 414 URI Too Long` ‚Üí Pipeline startet nicht!

**Beispiel:**
```
Prompt: 500 Zeichen ‚Üí OK
Prompt: 5.000 Zeichen ‚Üí Browser blockt
Prompt: 50.000 Zeichen (80k tokens!) ‚Üí Instant Crash
```

---

#### **‚úÖ L√ñSUNG: Fetch Stream API mit POST**

**Option A: POST-to-GET Pattern (Kompliziert)**
```typescript
// 1. Prompt im Cache speichern
const tokenResponse = await fetch('/api/bridge/init', {
    method: 'POST',
    body: JSON.stringify({ prompt })
});
const { token_id } = await tokenResponse.json();

// 2. SSE mit token_id (GET)
const eventSource = new EventSource(`/api/bridge/stream?token=${token_id}`);
```
**Problem:** Komplexer, Cache-Management n√∂tig

---

**Option B: Fetch Stream API (EMPFOHLEN!)**

Nutze `fetch` mit `POST` + Stream Reader statt `EventSource`:

```typescript
// frontend/src/components/EvokiTempleChat.tsx

const handleSendWithFetchStream = async () => {
    setIsLoading(true);
    setPipelineSteps([]); // Reset progress
    
    try {
        // POST Request mit Body (keine URL-Limit!)
        const response = await fetch(`${backendUrl}/api/bridge/stream`, {
            method: 'POST',
            headers: { 
                'Content-Type': 'application/json',
                'Accept': 'text/event-stream'
            },
            body: JSON.stringify({
                prompt: userPrompt,
                session_id: session.id,
                token_limit: selectedTokenLimit
            })
        });
        
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        // Stream lesen
        const reader = response.body?.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        
        while (true) {
            const { done, value } = await reader.read();
            
            if (done) {
                console.log('Stream complete');
                break;
            }
            
            // Daten dekodieren
            buffer += decoder.decode(value, { stream: true });
            
            // SSE-Format parsen: "data: {...}\n\n"
            const lines = buffer.split('\n\n');
            buffer = lines.pop() || ''; // Letzten unvollst√§ndigen Teil behalten
            
            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const jsonStr = line.substring(6); // "data: " entfernen
                    try {
                        const update = JSON.parse(jsonStr);
                        
                        // Update Progress UI
                        setPipelineSteps(prev => [...prev, {
                            step: update.step,
                            message: update.message,
                            timestamp: update.timestamp,
                            data: update.data
                        }]);
                        
                        // STEP 12 = Fertig!
                        if (update.step === 12 && update.status === 'completed') {
                            setMessages(prev => [...prev, {
                                role: 'assistant',
                                content: update.finalResponse.text,
                                timestamp: new Date().toISOString(),
                                metrics: update.finalResponse.metrics
                            }]);
                            setIsLoading(false);
                        }
                        
                        // Fehler
                        if (update.step === -1) {
                            setError(update.error);
                            setIsLoading(false);
                        }
                    } catch (parseError) {
                        console.error('JSON parse error:', parseError, jsonStr);
                    }
                }
            }
        }
        
    } catch (error) {
        console.error('Stream error:', error);
        setError(error.message);
        setIsLoading(false);
    }
};
```

**Vorteile:**
- ‚úÖ POST Request ‚Üí **KEINE URL-L√§ngen-Limits!**
- ‚úÖ Funktioniert mit riesigen Prompts (500k+ characters)
- ‚úÖ Gleiche SSE-Funktionalit√§t wie EventSource
- ‚úÖ Bessere Error-Handling Kontrolle
- ‚úÖ Kann bei Unmount sauber abgebrochen werden

---

**Option C: @microsoft/fetch-event-source Library**

```bash
npm install @microsoft/fetch-event-source
```

```typescript
import { fetchEventSource } from '@microsoft/fetch-event-source';

await fetchEventSource(`${backendUrl}/api/bridge/stream`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        prompt: userPrompt,
        session_id: session.id
    }),
    onmessage(event) {
        const update = JSON.parse(event.data);
        setPipelineSteps(prev => [...prev, update]);
        
        if (update.step === 12) {
            setMessages(prev => [...prev, update.finalResponse]);
            setIsLoading(false);
        }
    },
    onerror(err) {
        console.error('SSE Error:', err);
        setError(err.message);
        throw err; // Stop reconnecting
    }
});
```

**Vorteile:**
- ‚úÖ Automatische Reconnects bei Verbindungsabbruch
- ‚úÖ POST Support out-of-the-box
- ‚úÖ Production-ready (von Microsoft)
- ‚úÖ Einfachere API als manuelle Stream-Parsing

---

**EMPFEHLUNG:**
Nutze **Option C (@microsoft/fetch-event-source)** f√ºr V2.0 - Production-ready und einfach!

---

#### **BACKEND-IMPLEMENTATION (bleibt gleich):**

```javascript
// backend/server.js - SSE Endpoint

app.get('/api/bridge/stream', async (req, res) => {
    // SSE Headers
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no'); // Nginx Fix
    
    const sendUpdate = (step, message, data = {}) => {
        res.write(`data: ${JSON.stringify({ 
            step, 
            message, 
            timestamp: Date.now(),
            ...data 
        })}\n\n`);
    };
    
    try {
        const { prompt, session_id } = req.query;
        
        // STEP 1: Start
        sendUpdate(1, 'Pipeline gestartet...', { status: 'in_progress' });
        
        // STEP 2: User-Prompt Metrics
        sendUpdate(2, 'Berechne Prompt-Metriken...', { tokens: prompt.length });
        const metrics = await calculateMetrics(prompt);
        sendUpdate(2, 'Metriken berechnet', { 
            metrics: { A: metrics.A, PCI: metrics.PCI, Hazard: metrics.hazard }
        });
        
        // STEP 3: FAISS Search (kann 15s dauern)
        sendUpdate(3, 'Durchsuche 33.795 Erinnerungen (FAISS)...', { status: 'searching' });
        const faissStart = Date.now();
        const faissResults = await queryPythonBackend(prompt);
        const faissDuration = Date.now() - faissStart;
        sendUpdate(3, `FAISS fand ${faissResults.sources.length} Treffer`, { 
            hits: faissResults.sources.length, 
            duration: faissDuration 
        });
        
        // STEP 4: SQL Metrics Search (parallel zu FAISS)
        sendUpdate(4, 'Durchsuche Metrik-Datenbank (SQL)...', { status: 'searching' });
        const sqlResults = await trinity.search(metrics);
        sendUpdate(4, `SQL fand ${sqlResults.length} Treffer`, { hits: sqlResults.length });
        
        // STEP 5: Cross-Enrichment
        sendUpdate(5, 'Lade fehlende Daten (Cross-Enrichment)...', { status: 'enriching' });
        const enrichedResults = await crossEnrichResults(faissResults, sqlResults);
        sendUpdate(5, 'Daten angereichert', { total: enrichedResults.length });
        
        // STEP 6: Comparison
        sendUpdate(6, 'Vergleiche Metrik vs Semantik...', { status: 'comparing' });
        const comparisons = await compareResults(enrichedResults);
        const perfectMatches = comparisons.filter(c => c.agreement === 'PERFECT').length;
        sendUpdate(6, `${perfectMatches} PERFECT AGREEMENTS gefunden`, { 
            perfect: perfectMatches,
            total: comparisons.length 
        });
        
        // STEP 7: A65 Pair Selection
        sendUpdate(7, 'W√§hle 3 beste Kontext-Paare (A65)...', { status: 'selecting' });
        const selectedPairs = await selectTopPairs(comparisons);
        sendUpdate(7, '3 Paare ausgew√§hlt', { 
            pairs: selectedPairs.map(p => ({ 
                type: p.agreement, 
                tokens: p.tokenCount 
            }))
        });
        
        // STEP 8: Context Weaving
        sendUpdate(8, 'Verwebe Zeitlinien (¬±2 Prompts pro Paar)...', { status: 'weaving' });
        const contextSets = await weaveContexts(selectedPairs);
        const totalTokens = contextSets.reduce((sum, set) => sum + set.tokens, 0);
        sendUpdate(8, 'Kontext vervollst√§ndigt', { 
            sets: 3, 
            totalTokens 
        });
        
        // STEP 9: Model Selection
        sendUpdate(9, 'W√§hle optimales AI-Modell...', { status: 'selecting_model' });
        const modelStrategy = await selectModel(totalTokens, selectedPairs);
        sendUpdate(9, `Strategie: ${modelStrategy.strategy}`, { 
            primaryModel: modelStrategy.primaryModel.model,
            secondaryModel: modelStrategy.secondaryModel?.model,
            estimatedCost: modelStrategy.totalCost 
        });
        
        // STEP 10: Generate Response (kann 90s dauern bei Gemini!)
        if (modelStrategy.strategy === 'DUAL_RESPONSE') {
            sendUpdate(10, '2 Modelle parallel aufgerufen...', { 
                primary: modelStrategy.primaryModel.model,
                secondary: modelStrategy.secondaryModel.model 
            });
            
            // Parallel execution mit Progress-Updates
            const [primaryResponse, secondaryResponse] = await Promise.all([
                callLLMWithProgress(modelStrategy.primaryModel, (progress) => {
                    sendUpdate(10, `${modelStrategy.primaryModel.model}: ${progress}%`, { 
                        model: 'primary', 
                        progress 
                    });
                }),
                callLLMWithProgress(modelStrategy.secondaryModel, (progress) => {
                    sendUpdate(10, `${modelStrategy.secondaryModel.model}: ${progress}%`, { 
                        model: 'secondary', 
                        progress 
                    });
                })
            ]);
            
            sendUpdate(10, 'Beide Antworten empfangen', { 
                primaryTokens: primaryResponse.tokens,
                secondaryTokens: secondaryResponse.tokens 
            });
        } else {
            sendUpdate(10, `${modelStrategy.primaryModel.model} generiert Antwort...`, { 
                status: 'generating' 
            });
            const response = await callLLM(modelStrategy.primaryModel);
            sendUpdate(10, 'Antwort empfangen', { tokens: response.tokens });
        }
        
        // STEP 11: Vector Storage (12 DBs)
        sendUpdate(11, 'Speichere in 12 Vector-Datenbanken...', { status: 'storing' });
        await storeInVectorDBs(response, metrics);
        sendUpdate(11, 'In 12 DBs gespeichert', { databases: 12 });
        
        // STEP 12: FINAL
        const totalDuration = Date.now() - pipelineStart;
        sendUpdate(12, '‚úÖ Pipeline abgeschlossen!', { 
            status: 'completed',
            totalDuration,
            finalResponse: response 
        });
        
        res.end();
        
    } catch (error) {
        sendUpdate(-1, `‚ùå Fehler: ${error.message}`, { 
            status: 'error', 
            error: error.stack 
        });
        res.end();
    }
});
```

---

#### **FRONTEND-IMPLEMENTATION (SSE Consumer):**

**Installation erforderlich:** `npm install @microsoft/fetch-event-source`

```typescript
// frontend/src/components/EvokiTempleChat.tsx
import { fetchEventSource } from '@microsoft/fetch-event-source';

const handleSendWithSSE = async () => {
    setIsLoading(true);
    setPipelineSteps([]); // Reset progress
    
    try {
        await fetchEventSource(`${backendUrl}/api/bridge/stream`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                prompt: textToSend, // ‚úÖ POST Body erlaubt unbegrenzte L√§nge!
                session_id: session.id,
                token_limit: tokenLimitMode
            }),
            onmessage(event) {
                const update = JSON.parse(event.data);
                setPipelineSteps(prev => [...prev, update]);
                
                if (update.step === 12 && update.status === 'completed') {
                    setMessages(prev => [...prev, update.finalResponse]);
                    setIsLoading(false);
                }
                
                if (update.status === 'error') {
                    throw new Error(update.error);
                }
            },
            onerror(err) {
                console.error('Stream Fehler:', err);
                throw err; // Reconnect verhindern bei fatalem Fehler
            }
        });
    } catch (err) {
        addApplicationError(err, 'stream_connection');
        setIsLoading(false);
    }
};
    
    eventSource.onerror = (error) => {
        console.error('SSE Error:', error);
        eventSource.close();
        setIsLoading(false);
    };
    
    // WICHTIG: Cleanup bei Unmount!
    return () => {
        eventSource.close();
    };
};
```

---

#### **PIPELINE-PROGRESS UI (Live-Updates):**

```tsx
// frontend/src/components/PipelineProgress.tsx

function PipelineProgress({ steps }: { steps: PipelineStep[] }) {
    return (
        <div className="pipeline-progress">
            {steps.map((step, idx) => (
                <div key={idx} className={`pipeline-step step-${step.step}`}>
                    <div className="step-header">
                        <span className="step-number">{step.step}/12</span>
                        <span className="step-time">
                            {new Date(step.timestamp).toLocaleTimeString()}
                        </span>
                    </div>
                    <div className="step-message">{step.message}</div>
                    
                    {/* Data-Preview (falls vorhanden) */}
                    {step.data && (
                        <div className="step-data">
                            {step.data.hits && <span>üéØ {step.data.hits} Treffer</span>}
                            {step.data.duration && <span>‚è±Ô∏è {step.data.duration}ms</span>}
                            {step.data.tokens && <span>üìä {step.data.tokens.toLocaleString()} Tokens</span>}
                            {step.data.perfect && <span>‚≠ê {step.data.perfect} Perfect Matches</span>}
                        </div>
                    )}
                </div>
            ))}
        </div>
    );
}
```

**Live-Preview:**
```
‚îå‚îÄ PIPELINE FORTSCHRITT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1/12  14:32:11  Pipeline gestartet...         ‚îÇ
‚îÇ 2/12  14:32:11  Metriken berechnet            ‚îÇ
‚îÇ                 üìä A: 0.85 | PCI: 0.72         ‚îÇ
‚îÇ 3/12  14:32:26  FAISS fand 47 Treffer         ‚îÇ
‚îÇ                 üéØ 47 Treffer | ‚è±Ô∏è 15024ms     ‚îÇ
‚îÇ 4/12  14:32:28  SQL fand 63 Treffer           ‚îÇ
‚îÇ 5/12  14:32:31  Daten angereichert            ‚îÇ
‚îÇ 6/12  14:32:35  3 PERFECT AGREEMENTS gefunden ‚îÇ
‚îÇ                 ‚≠ê 3 Perfect | 110 Total       ‚îÇ
‚îÇ 7/12  14:32:37  3 Paare ausgew√§hlt            ‚îÇ
‚îÇ 8/12  14:32:40  Kontext vervollst√§ndigt       ‚îÇ
‚îÇ                 üìä 85,234 Tokens total         ‚îÇ
‚îÇ 9/12  14:32:42  Strategie: DUAL_RESPONSE      ‚îÇ
‚îÇ                 ü•á GPT-4 + üìö Gemini          ‚îÇ
‚îÇ 10/12 14:33:15  Beide Antworten empfangen     ‚îÇ
‚îÇ 11/12 14:33:17  In 12 DBs gespeichert        ‚îÇ
‚îÇ 12/12 14:33:18  ‚úÖ Pipeline abgeschlossen!    ‚îÇ
‚îÇ                 ‚è±Ô∏è Total: 67,234ms            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### **üéØ VORTEILE DER SSE-L√ñSUNG:**

#### **1. TIMEOUT-PROBLEM GEL√ñST:**
- ‚úÖ Verbindung bleibt offen (solange Updates flie√üen)
- ‚úÖ Kein "Blind Waiting" mehr (User sieht was passiert)
- ‚úÖ Frontend kann **NICHT mehr** zu fr√ºh abbrechen (keine AbortSignal.timeout!)
- ‚úÖ Backend kann 5 Minuten brauchen - solange Updates kommen, ist es OK

#### **2. UX MASSIV VERBESSERT:**
- ‚úÖ User sieht **LIVE** was System macht
- ‚úÖ Transparenz schafft Vertrauen
- ‚úÖ Gef√ºhl von "das System arbeitet" statt "ist es abgest√ºrzt?"
- ‚úÖ Kann einzelne Steps debuggen (z.B. "FAISS dauert zu lange")

#### **3. DEBUGGING VEREINFACHT:**
- ‚úÖ Jeder Step wird geloggt (Timestamps!)
- ‚úÖ Kann sehen WO Pipeline h√§ngt
- ‚úÖ Performance-Analyse pro Step
- ‚úÖ Fehler sind sofort sichtbar (nicht erst nach 60s Timeout)

#### **4. PARALLELIT√ÑT SICHTBAR:**
- ‚úÖ Bei Dual-Response: Sieht User beide Models arbeiten
- ‚úÖ "GPT-4: 45% | Gemini: 78%" ‚Üí Live-Progress!
- ‚úÖ User wei√ü welches Model schneller ist

#### **5. KOSTENLOS:**
- ‚úÖ SSE ist HTTP-Standard (keine extra Libraries!)
- ‚úÖ EventSource API ist im Browser eingebaut
- ‚úÖ Keine WebSocket-Komplexit√§t
- ‚úÖ Funktioniert mit Standard HTTP-Servern

---

### **‚ö†Ô∏è POTENTIAL ISSUES & FIXES:**

#### **Issue 1: Nginx buffert SSE**
**Problem:** Nginx buffert Events ‚Üí User sieht nichts bis Response fertig
**Fix:** `X-Accel-Buffering: no` Header

#### **Issue 2: Client disconnects**
**Problem:** User schlie√üt Tab ‚Üí Backend rechnet weiter
**Fix:** Detect disconnect + cancel Request:
```javascript
req.on('close', () => {
    console.log('Client disconnected, canceling...');
    abortController.abort();
});
```

#### **Issue 3: Sehr lange Requests (>5min)**
**Problem:** Manche Proxies/Load Balancers haben Max-Timeouts
**Fix:** Heartbeat alle 30s senden:
```javascript
const heartbeat = setInterval(() => {
    res.write(`: heartbeat\n\n`); // Comment-only (kein data:)
}, 30000);
```

#### **Issue 4: Error Handling**
**Problem:** Fehler in Step 7 ‚Üí vorherige Steps unsichtbar?
**Fix:** Steps im State speichern, auch bei Fehler anzeigen

---

### **üîÑ MIGRATION VON ALT ‚Üí NEU:**

**Phase 1: Parallel betreiben**
- Alte `/api/bridge/process` bleibt (HTTP POST)
- Neue `/api/bridge/stream` kommt dazu (SSE)
- Frontend hat Toggle: "Live-Updates aktivieren?"

**Phase 2: User-Feedback**
- Testen mit echten Anfragen
- Performance messen (ist SSE schneller/langsamer?)
- UX-Feedback (m√∂gen User Live-Updates?)

**Phase 3: Migration**
- Wenn SSE stabil ‚Üí wird Standard
- Alte Endpoint deprecated
- Nach 3 Monaten: Alten Endpoint entfernen

---

### **üìä PERFORMANCE-VERGLEICH:**

| Aspekt | HTTP POST (alt) | SSE (neu) |
|--------|-----------------|-----------|
| **Timeout-Problem** | ‚ùå Ja (60s vs 115s) | ‚úÖ Gel√∂st (beliebig lang) |
| **UX Transparency** | ‚ùå Blind Waiting | ‚úÖ Live-Updates |
| **Debugging** | ‚ùå Schwer (black box) | ‚úÖ Easy (Step-by-Step) |
| **Error Detection** | ‚ùå Nach 60s Timeout | ‚úÖ Sofort sichtbar |
| **Parallelit√§t** | ‚ùå Unsichtbar | ‚úÖ Sichtbar (beide Models) |
| **Komplexit√§t** | ‚≠ê‚≠ê (einfach) | ‚≠ê‚≠ê‚≠ê (mittel) |
| **Browser-Support** | ‚úÖ 100% | ‚úÖ 98% (IE fehlt, egal) |

---

**Code-Stellen:**

**Frontend (EvokiTempleChat.tsx Line 496):**
```typescript
// ALT:
const response = await fetch(`${backendUrl}/api/bridge/process`, {
  method: 'POST',
  body: JSON.stringify(payload),
  signal: AbortSignal.timeout(60000), // ‚úÖ 60s f√ºr FAISS-Suche
});
```
- **Frontend wartet:** 60 Sekunden
- **Dann:** Bricht ab mit "Backend timeout"

**Backend (DualBackendBridge.js Line 295):**
```javascript
const proc = spawn(pythonPath, [scriptPath, prompt], {
  timeout: 15000 // 15s f√ºr W2 (MiniLM)
});
```
- **Python Subprocess:** 15 Sekunden f√ºr FAISS-Suche
- **Aber:** Gemini API hat noch KEINEN Timeout!

**Backend (GeminiContextBridge.js Line 488):**
```javascript
timeout: 90000  // ‚úÖ 90s f√ºr gro√üe Context-Fenster (1M tokens)
```
- **Gemini API:** Bis zu 90 Sekunden!

**RECHNUNG:**
- Python FAISS: 15s
- Gemini API: 90s
- **TOTAL Backend:** 15s + 90s = **105 Sekunden maximal**
- **Frontend Timeout:** 60 Sekunden
- **DIFFERENZ:** Frontend bricht 45 Sekunden ZU FR√úH ab!

**Konsequenz:**
- User sieht "Backend timeout (60s)"
- Backend arbeitet weiter (bis zu 105s)
- Antwort kommt an ‚Üí aber Frontend hat Request abgebrochen
- **L√∂sung:** Frontend Timeout auf **120 Sekunden** erh√∂hen

---

### **‚ö†Ô∏è LOGIK-FEHLER #1: Google API kann OHNE Kontext antworten**

**Das Problem:**
Wenn FAISS-Suche fehlschl√§gt (Python CLI crashed, Timeout, etc.) ‚Üí Backend ruft TROTZDEM Gemini API auf ‚Üí **Gemini bekommt NUR User-Prompt OHNE Kontext aus 33.795 Chunks!**

**Code-Analyse (DualBackendBridge.js Line 136-186):**

```javascript
// Schritt 3: FAISS W2 durchsuchen
let semanticResults = await this.queryPythonBackend(prompt, context);
// ‚ùå KEIN Error-Check hier!

// Schritt 9: Gemini Response generieren
const geminiResponse = await this.geminiContext.generateContextualResponse({
    userPrompt: prompt,
    faissResults: semanticResults?.sources || [], // ‚ùì Was wenn semanticResults = null?
    selectedIndex: 0,
    metrics: userPromptMetrics || {},
    sessionId: sessionId
});
```

**Was passiert bei FAISS-Fehler:**
1. `semanticResults = null` oder `{}`
2. `faissResults: []` (leeres Array!)
3. Gemini bekommt NUR `userPrompt` ohne Kontext
4. Gemini generiert **generische Antwort** statt kontextbasierte
5. User bekommt schlechte Antwort, denkt "System funktioniert"

**Wo ist das Problem?**
- **Keine Validierung:** Backend pr√ºft NICHT ob FAISS erfolgreich war
- **Silent Failure:** FAISS-Fehler werden nicht an Frontend gemeldet
- **False Success:** Frontend zeigt "‚úÖ Fertig" obwohl Kontext fehlte

**L√∂sung:**
```javascript
// Nach FAISS-Suche:
if (!semanticResults || !semanticResults.sources || semanticResults.sources.length === 0) {
    throw new Error('FAISS-Suche fehlgeschlagen - keine Chunks gefunden');
}
```

---

### **‚ö†Ô∏è LOGIK-FEHLER #2: Keine Micro-Pipeline - User-Prompt wird NICHT parallel gesendet**

**Das Problem:**
Es gibt KEINE Micro-Pipeline die User-Prompt direkt an Gemini sendet w√§hrend FAISS sucht. ABER: Das ist eigentlich GUT so! Wir WOLLEN ja den Kontext!

**Code-Analyse:**

**Sequentieller Ablauf (KORREKT):**
1. User-Prompt empfangen
2. Metriken berechnen (10s Timeout)
3. **FAISS W2 durchsuchen (15s Timeout)** ‚Üê WARTET bis fertig!
4. FAISS W5 durchsuchen (deaktiviert)
5. Trinity DBs abfragen (simuliert)
6. Top-3 kombinieren
7. **Gemini Context bauen** ‚Üê BRAUCHT FAISS-Ergebnisse!
8. Gemini API aufrufen (90s Timeout)
9. Antwort zur√ºck

**KEIN Parallel-Request:** User-Prompt wird NICHT direkt an Gemini gesendet w√§hrend FAISS sucht.

**Warum ist das gut?**
- Wir wollen **kontextbasierte** Antworten, nicht generische
- FAISS-Suche ist NOTWENDIG f√ºr Qualit√§t
- Parallele Anfrage w√ºrde schlechte Antwort liefern

**Aber:** Wenn FAISS zu langsam ‚Üí User wartet ‚Üí Frustration

**Optimierung:**
- FAISS-Index im RAM halten (schneller)
- Chunk-Count reduzieren (nur relevante Zeitr√§ume)
- Top-K reduzieren (nicht alle 33.795 durchsuchen)

---

### **üîç ALLE TIMEOUTS IM SYSTEM (VOLLST√ÑNDIG):**

#### **FRONTEND TIMEOUTS:**

| Component | Endpoint | Timeout | Zweck |
|-----------|----------|---------|-------|
| **EvokiTempleChat** | `/api/bridge/process` | **60s** ‚ö†Ô∏è | Hauptpipeline (FAISS + Gemini) |
| EvokiTempleChat | Trinity Download | 5s | History laden |
| **ChatbotPanel** | `/api/bridge/process` | **10s** ‚ùå | Legacy (zu kurz!) |
| GenesisStartupScreen | `/health` | 3s | Backend Health Check |
| App.tsx | `/api/v1/status` | 5s | Backend Status |
| App.tsx | `/api/v1/health` | 5s | Backend Health |

**PROBLEM:**
- EvokiTempleChat: 60s zu kurz f√ºr Backend (105s maximal)
- ChatbotPanel: 10s viel zu kurz (Legacy-Code)

#### **BACKEND TIMEOUTS:**

| Component | Target | Timeout | Zweck |
|-----------|--------|---------|-------|
| **Python CLI Spawn** | query.py | **15s** ‚ö†Ô∏è | FAISS W2-Suche (33.795 Chunks) |
| **GeminiContextBridge** | Gemini API | **90s** ‚úÖ | Large Context (1M tokens) |
| GeminiContextBridge | OpenAI Fallback | 30s | TTS/Fallback |
| GeminiContextBridge | SQLite Query | 5s | History-Kontext laden |
| DualBackendBridge | Metrics Calc | 10s | Metriken berechnen |
| DualBackendBridge | Python Health | 3s | Backend Check |
| DualBackendBridge | FAISS HTTP | 15s | FAISS API (wenn verf√ºgbar) |
| Server.js | Gemini Direct | 10s | A65 Candidates |
| Server.js | OpenAI Direct | 15s | A65 Fallback |

**GESAMT-RECHNUNG:**
```
Metrics (10s) + FAISS (15s) + Gemini (90s) = 115 Sekunden maximal
```
**Frontend Timeout:** 60s ‚Üí **55 Sekunden zu kurz!**

---

### **‚ö†Ô∏è TIMEOUT-PROBLEM #2: Python CLI kann einfrieren**

**Das Problem:**
`spawn(pythonPath, [scriptPath, prompt], { timeout: 15000 })` ‚Üí Node.js `timeout` Option funktioniert NICHT zuverl√§ssig bei stdout-Buffering!

**Code (DualBackendBridge.js Line 295-340):**

```javascript
const proc = spawn(pythonPath, [scriptPath, prompt], {
    cwd: path.join(__dirname, '..', '..', 'python'),
    timeout: 15000 // ‚ùå Funktioniert nicht immer!
});

let jsonOutput = '';
proc.stdout.on('data', (data) => {
    jsonOutput += data.toString();
});

proc.on('close', (code) => {
    if (code === 0) {
        const results = JSON.parse(jsonOutput);
        resolve(results);
    } else {
        reject(new Error(`Python exited: ${code}`));
    }
});

setTimeout(() => {
    if (!proc.killed) {
        proc.kill('SIGTERM'); // ‚ö†Ô∏è Manueller Timeout
        reject(new Error('Python timeout after 15s'));
    }
}, 15000);
```

**Warum 2 Timeouts?**
- `spawn({ timeout })` ist NICHT zuverl√§ssig
- `setTimeout + proc.kill` ist ZUS√ÑTZLICHE Absicherung
- **Aber:** Wenn Python h√§ngt ‚Üí beide Timeouts greifen nicht

**Worst Case:**
1. Python query.py l√§dt FAISS-Index (kann 30s dauern bei gro√üen Indices!)
2. Node.js wartet auf stdout
3. Timeout greift ‚Üí `proc.kill('SIGTERM')`
4. Python ignoriert SIGTERM (l√§dt gerade FAISS)
5. **Prozess bleibt h√§ngen** ‚Üí Backend blockiert

**L√∂sung:**
- FAISS-Index im RAM halten (separate Prozess)
- Oder: `proc.kill('SIGKILL')` statt `SIGTERM` (hart)

---

### **üñ±Ô∏è UI-ELEMENTE CRASH-RISIKEN:**

#### **CRASH-RISIKO #1: "Senden"-Button w√§hrend laufender Anfrage**

**Problem:**
User kann "Senden"-Button mehrfach klicken ‚Üí Mehrere Requests parallel ‚Üí Backend-√úberlastung ‚Üí Race Conditions

**Code (EvokiTempleChat.tsx Line 443):**
```typescript
const handleSend = useCallback(async () => {
  if (!textToSend || !session || isLoading) return; // ‚úÖ isLoading-Check vorhanden
  setIsLoading(true);
  // ... Request ...
  setIsLoading(false);
});
```

**Status:** ‚úÖ GESCH√úTZT durch `isLoading` Flag

**Aber:** Was wenn `setIsLoading(false)` nie erreicht wird? (z.B. unhandled exception)
‚Üí Button bleibt disabled ‚Üí **User kann nichts mehr senden!**

**L√∂sung:** `finally { setIsLoading(false); }` am Ende

---

#### **CRASH-RISIKO #2: Token-Limit Selector w√§hrend laufender Anfrage**

**Problem:**
User √§ndert Token-Limit (Quick/Standard/Unlimited) w√§hrend Request l√§uft ‚Üí Token-Verteilung √§ndert sich mid-flight ‚Üí Inkonsistente Daten

**Code (EvokiTempleChat.tsx Line 227):**
```typescript
const [tokenLimitMode, setTokenLimitMode] = useState<'QUICK' | 'STANDARD' | 'UNLIMITED'>('QUICK');
```

**Status:** üü° KEIN SCHUTZ - User kann w√§hrend Request Token-Limit √§ndern

**Worst Case:**
1. User startet Request mit "Quick" (25k)
2. W√§hrend FAISS-Suche: User wechselt auf "Unlimited" (1M)
3. Backend bereitet Response vor mit 25k Budget
4. Frontend erwartet 1M Budget ‚Üí Metriken stimmen nicht

**L√∂sung:** Token-Limit Selector disablen wenn `isLoading === true`

---

#### **CRASH-RISIKO #3: Tab-Wechsel w√§hrend laufender Anfrage**

**Problem:**
User startet Request im "Evoki's Tempel V3"-Tab ‚Üí Wechselt zu "Trialog"-Tab ‚Üí State wird unmounted ‚Üí Request l√§uft weiter ‚Üí Response kommt an ‚Üí **State existiert nicht mehr** ‚Üí Crash

**Code (App.tsx Line 949):**
```typescript
{appState.activeTab === Tab.TempleChat && (
  <EvokiTempleChat ... />
)}
```

**Status:** üî¥ HOHES RISIKO - Component wird unmounted bei Tab-Wechsel

**Worst Case:**
1. User startet Request im Tempel
2. Wechselt zu Trialog (Tempel unmounted)
3. 60s sp√§ter: Response kommt an
4. `setSession()` wird aufgerufen ‚Üí **State existiert nicht** ‚Üí Memory Leak

**L√∂sung:**
- AbortController nutzen um Request zu canceln bei unmount
- Oder: State in App.tsx halten statt in Component

---

#### **CRASH-RISIKO #4: "Neue Session"-Button w√§hrend laufender Anfrage**

**Problem:**
User klickt "Neue Session" w√§hrend Request l√§uft ‚Üí Session wird resettet ‚Üí Request kommt an ‚Üí Versucht in nicht-existierende Session zu schreiben ‚Üí **Crash**

**Code (EvokiTempleChat.tsx Line 738):**
```typescript
const handleNewSession = useCallback(() => {
  if (isLoading) return; // ‚úÖ Gesch√ºtzt
  // ... neue Session erstellen ...
});
```

**Status:** ‚úÖ GESCH√úTZT durch `isLoading` Check

---

#### **CRASH-RISIKO #5: Schnelles Scrollen im Chat w√§hrend Rendering**

**Problem:**
Gro√üe Antworten (1M tokens) ‚Üí Viel Text ‚Üí Rendering dauert ‚Üí User scrollt schnell ‚Üí **Browser freezt**

**Code (EvokiTempleChat.tsx):**
Keine Virtualisierung vorhanden! Alle Messages werden gerendert.

**Worst Case:**
1. User hat 50 Messages in Session
2. Jede Message hat 10k tokens (gro√üe Antworten)
3. **500k tokens Text im DOM**
4. Browser muss alles rendern ‚Üí **UI freezt**

**Status:** üü° MITTLERES RISIKO bei langen Sessions

**L√∂sung: Virtualisierte Liste mit react-window**

```typescript
// L√∂sung: Virtualisierte Liste mit 'react-window'
import { VariableSizeList as List } from 'react-window';

// In der Render-Methode:
<List
    height={window.innerHeight - 200}
    itemCount={messages.length}
    itemSize={index => getItemSize(index)} // Dynamische H√∂he berechnen
    width="100%"
>
    {({ index, style }) => (
        <div style={style}>
            <EvokiMessage message={messages[index]} />
        </div>
    )}
</List>

// Effekt: Rendert nur die 5-10 sichtbaren Messages im DOM.
// Performance: Stabil auch bei 10.000 Messages / 1M Tokens.
```

---

## üéØ **ORCHESTRATOR-LOGIK (A65) - KOMPLETTER ABLAUF**

### **DAS PROBLEM: Metriken vs Semantik - BEIDE haben Schw√§chen!**

**Beispiel-Szenario:**
User fragt: "Erz√§hl von den Zwillingen"

**Problem 1: FAISS findet nichts, aber Metriken schon!**
- Triggerwort "Zwillinge" erscheint in Metriken (A, PCI, Hazard steigen!)
- ABER: Wort "Zwillinge" ist NOCH NIE im Chatverlauf gefallen
- ‚Üí FAISS semantic search findet NICHTS (kein √§hnlicher Text)
- ‚Üí SQL Metrik-Suche findet Pattern (√§hnliche Metrik-Werte bei anderen Prompts)

**Problem 2: FAISS findet etwas, aber Metriken falsch gewichtet!**
- Text "Geschwister in der Kita" ist semantisch √§hnlich zu "Zwillinge"
- FAISS findet es, aber Metriken sind komplett anders (A, PCI unterschiedlich)
- ‚Üí Semantik sagt "relevant", Metriken sagen "nicht relevant"

**L√ñSUNG: ORCHESTRATOR kombiniert BEIDE + vergleicht!**

---

### **üîÑ SCHRITT 1: PARALLELE SUCHE (SQL + FAISS)**

#### **A) SQL-METRIK-SUCHE (Trinity Engines):**

**Was wird gesucht:**
- Prompts mit √§hnlichen Metriken (A, PCI, Hazard, Œµ_z, œÑ_s, Œª_R, etc.)
- **UNABH√ÑNGIG vom Text!** (nur Zahlen-Vergleich)

**Suchstrategie:**
```
User-Prompt: "Erz√§hl von den Zwillingen"
‚îî‚îÄ Metriken berechnen: A=0.85, PCI=0.72, Hazard=0.34, ...

SQL Query:
‚îú‚îÄ Suche -25 Prompts zur√ºck (√ºber -5, -2, -1)
‚îÇ  ‚îî‚îÄ Finde Prompts mit √§hnlichen Metriken (Cosine Similarity auf Metrik-Vektoren)
‚îî‚îÄ Suche +25 Prompts voraus (√ºber +1, +2, +5)
   ‚îî‚îÄ Finde zuk√ºnftige Trends in Metriken
```

**Beispiel-SQL:**
```sql
-- Finde Prompts mit √§hnlichen Metriken (¬±25 Prompts im Fenster)
SELECT prompt_id, timecode, author, 
       -- Cosine Similarity zwischen Metrik-Vektoren
       (A * 0.85 + PCI * 0.72 + Hazard * 0.34 + ...) AS metric_similarity
FROM tempel_W_m2  -- Window -2 bis +2
WHERE prompt_id BETWEEN current_id - 25 AND current_id + 25
ORDER BY metric_similarity DESC
LIMIT 100;
```

**Ergebnis:** Top 100 Prompts mit √§hnlichen Metriken (nur IDs, Timecodes, Metriken)

---

#### **B) FAISS-SEMANTIK-SUCHE (Parallel!):**

**Was wird gesucht:**
- Texte mit √§hnlicher Bedeutung (Embedding Cosine Similarity)
- **UNABH√ÑNGIG von Metriken!** (nur Text-Vergleich)

**Suchstrategie:**
```
User-Prompt: "Erz√§hl von den Zwillingen"
‚îî‚îÄ Text ‚Üí Embedding (384D Vektor)

FAISS Query:
‚îú‚îÄ Suche -25 Prompts zur√ºck (√ºber -5, -2, -1)
‚îÇ  ‚îî‚îÄ Finde Texte mit √§hnlichem Embedding
‚îî‚îÄ Suche +25 Prompts voraus (√ºber +1, +2, +5)
   ‚îî‚îÄ Finde zuk√ºnftige semantische Trends
```

**Python Code:**
```python
# 1. User-Prompt ‚Üí Embedding
query_vector = model.encode("Erz√§hl von den Zwillingen")

# 2. FAISS search mit -25 bis +25 Window-Logik
results = faiss_index.search(query_vector, top_k=100)

# 3. F√ºr jeden Hit: Pr√ºfe ob in ¬±25 Fenster
filtered_results = []
for hit in results:
    distance = abs(hit.prompt_id - current_prompt_id)
    if distance <= 25:  # Innerhalb ¬±25 Fenster
        filtered_results.append(hit)
```

**Ergebnis:** Top 100 Chunks mit √§hnlichem Text (nur IDs, Timecodes, Text-Preview)

---

### **üîÑ SCHRITT 2: CROSS-ENRICHMENT (Orchestrator Magic!)**

**Problem:** 
- SQL hat Metriken, aber KEINE Texte
- FAISS hat Texte, aber KEINE Metriken

**L√∂sung: Orchestrator holt fehlende Daten!**

#### **A) F√úR SQL-TREFFER: Texte aus Quelldatenbank laden**

```javascript
// DualBackendBridge.js - Orchestrator
const sqlResults = await trinity.search(userPromptMetrics); // Top 100 Metrik-Treffer

// F√ºr jeden SQL-Treffer: Lade Original-Prompt-Text
const enrichedSqlResults = [];
for (const hit of sqlResults) {
    const originalText = await sourceDatabase.query(`
        SELECT prompt_text, author, timecode 
        FROM chat_history 
        WHERE prompt_id = ? AND timecode = ? AND author = ?
    `, [hit.prompt_id, hit.timecode, hit.author]);
    
    enrichedSqlResults.push({
        prompt_id: hit.prompt_id,
        metrics: hit.metrics,          // ‚úÖ HAT SCHON
        text: originalText.prompt_text, // ‚úÖ NEU GELADEN
        timecode: hit.timecode,
        author: hit.author
    });
}
```

**Quelldatenbank:**
- `evoki_v2_ultimate_FULL.db` (Backend)
- Enth√§lt: Prompt ID, Timecode, Autor, Original-Text
- Erm√∂glicht Zuordnung: Metrik-ID ‚Üí Original-Text

---

#### **B) F√úR FAISS-TREFFER: Metriken aus 1:1 Metrikdatenbank laden**

```javascript
const faissResults = await this.queryPythonBackend(prompt); // Top 100 Semantic Treffer

// F√ºr jeden FAISS-Treffer: Lade zugeh√∂rige Metriken
const enrichedFaissResults = [];
for (const hit of faissResults.sources) {
    const metrics = await metricDatabase.query(`
        SELECT A, PCI, hazard_score, epsilon_z, tau_s, lambda_R, ...
        FROM tempel_metrics_1to1 
        WHERE prompt_id = ? AND timecode = ? AND author = ?
    `, [hit.id, hit.timecode, hit.author]);
    
    enrichedFaissResults.push({
        prompt_id: hit.id,
        text: hit.text,              // ‚úÖ HAT SCHON
        metrics: metrics,             // ‚úÖ NEU GELADEN
        timecode: hit.timecode,
        author: hit.author,
        semantic_score: hit.score     // FAISS Cosine Similarity
    });
}
```

**1:1 Metrikdatenbank:**
- `tempel_metrics_1to1.db` (Backend)
- Enth√§lt: Prompt ID, Timecode, Autor, ALLE 153 Metriken (V14 Core) Metriken
- Erm√∂glicht Zuordnung: Text-ID ‚Üí Metriken

---

### **üîÑ SCHRITT 3: INTELLIGENTER VERGLEICH (Das Herzst√ºck!)**

**Jetzt haben wir:**
- `enrichedSqlResults`: Top 100 Metrik-Treffer MIT Texten
- `enrichedFaissResults`: Top 100 Semantic-Treffer MIT Metriken

**Orchestrator vergleicht:**

```javascript
// Vergleichs-Analyse
const comparisonResults = [];

for (const sqlHit of enrichedSqlResults) {
    for (const faissHit of enrichedFaissResults) {
        // 1. Berechne Basis-√úbereinstimmung
        const metricSimilarity = cosineSimilarity(sqlHit.metrics, faissHit.metrics);
        const semanticSimilarity = faissHit.semantic_score;
        
        // 2. TIME DECAY (Verhinderung von Context-Drift)
        // Alte Traumata verblassen, wenn sie nicht frisch best√§tigt sind
        const daysDiff = (Date.now() - new Date(sqlHit.timecode).getTime()) / (1000 * 60 * 60 * 24);
        const lambda = 0.05; // Zerfallsfaktor (einstellbar im ParameterTuning)
        const timeDecayFactor = 1 / (1 + lambda * Math.abs(daysDiff));
        
        // Korrigierte Scores
        const adjustedMetricScore = metricSimilarity * timeDecayFactor;
        
        // 3. Berechne Abweichungen & Combined Score
        const metricDeviation = Math.abs(metricSimilarity - semanticSimilarity);
        const combinedScore = (adjustedMetricScore + semanticSimilarity) / 2;
        
        comparisonResults.push({
            sql_hit: sqlHit,
            faiss_hit: faissHit,
            metric_similarity: metricSimilarity,
            metric_score_adjusted: adjustedMetricScore, // Neu: Zeit-korrigiert
            semantic_similarity: semanticSimilarity,
            combined_score: combinedScore,
            time_decay_factor: timeDecayFactor,         // F√ºr Debugging
            deviation: metricDeviation,
            agreement: metricSimilarity > 0.7 && semanticSimilarity > 0.7 ? 'HIGH' : 'LOW'
        });
    }
}

// Sortiere nach verschiedenen Kriterien
comparisonResults.sort((a, b) => {
    // Priorisierung:
    // 1. Beide hoch (Metrik + Semantik > 0.8)
    if (a.agreement === 'HIGH' && b.agreement !== 'HIGH') return -1;
    
    // 2. Kombinierter Score (mit Time Decay!)
    return b.combined_score - a.combined_score;
});
```

**Fragen die beantwortet werden:**

1. **Wo passen Metrik UND Semantik BESONDERS gut zusammen?**
   - `metric_similarity > 0.8` UND `semantic_similarity > 0.8`
   - ‚Üí Diese Treffer sind **SEHR SICHER** (beide Methoden sagen "relevant")

2. **Wo ist gr√∂√üte Metrik-√úbereinstimmung?**
   - `max(metric_similarity)` 
   - ‚Üí Wichtig f√ºr Trigger-W√∂rter die noch nicht gefallen sind

3. **Wo ist gr√∂√üte Semantik-√úbereinstimmung?**
   - `max(semantic_similarity)`
   - ‚Üí Wichtig f√ºr konzeptionell √§hnliche Texte

4. **Wie gro√ü ist gr√∂√üte Abweichung?**
   - `max(|metric_similarity - semantic_similarity|)`
   - ‚Üí Zeigt wo Methoden NICHT √ºbereinstimmen (interessant f√ºr Analyse!)

---

### **üîÑ SCHRITT 4: A65 - 3-PAAR-AUSWAHL (Multi-Candidate Selection)**

**Auswahl-Strategie:**

```javascript
// A65 Multi-Candidate Selection
let selectedPairs = [];

// 1. Filtere Sentinel-Veto Blockaden (Kritische Sicherheit)
const safeCandidates = comparisonResults.filter(r => 
    !r.warningFlag || r.sentinelSeverity !== 'CRITICAL'
);

// üö® EMERGENCY REFETCH CHECK
if (safeCandidates.length === 0) {
    console.warn('‚ö†Ô∏è EMERGENCY: Sentinel hat alle Kandidaten blockiert!');
    // Fallback: Sende generischen "Safe Mode" Kontext oder starte Refetch mit lockereren Parametern
    return {
        strategy: 'FALLBACK_SAFE_MODE',
        reason: 'Sentinel Veto: Zu hohe Gefahr in allen Kontexten.',
        systemPrompt: "Achtung: Der Nutzer-Input triggert kritische Sicherheitswarnungen. Antworte vorsichtig, empathisch, aber vermeide tiefe Trauma-Analyse ohne klaren Kontext."
    };
}

// 2. Paar 1: BESTE √úbereinstimmung (Metrik + Semantik beide hoch)
const highAgreement = safeCandidates.find(r => r.agreement === 'HIGH');
if (highAgreement) selectedPairs.push(highAgreement);

// 3. Paar 2: BESTE Zeit-korrigierte Metrik (Time Decay ber√ºcksichtigt!)
const bestMetric = safeCandidates.sort((a, b) => b.metric_score_adjusted - a.metric_score_adjusted)[0];
if (bestMetric && !selectedPairs.includes(bestMetric)) selectedPairs.push(bestMetric);

// 4. Paar 3: BESTE Semantik (Inhaltliche Relevanz)
const bestSemantic = safeCandidates.sort((a, b) => b.semantic_similarity - a.semantic_similarity)[0];
if (bestSemantic && !selectedPairs.includes(bestSemantic)) selectedPairs.push(bestSemantic);

// Auff√ºllen falls < 3 (mit n√§chstbesten Combined Scores)
while (selectedPairs.length < 3 && safeCandidates.length > selectedPairs.length) {
    const nextBest = safeCandidates
        .filter(c => !selectedPairs.includes(c))
        .sort((a, b) => b.combined_score - a.combined_score)[0];
    selectedPairs.push(nextBest);
}
```

**Ergebnis:** 3 Paare, jedes Paar hat:
- `sql_hit`: Metrik-basierter Treffer mit Text
- `faiss_hit`: Semantik-basierter Treffer mit Metriken
- `combined_score`: Kombinierter Score

---

### **üîÑ SCHRITT 5: CONTEXT-WEAVING (¬±2 Prompts = Geschichte)**

**F√ºr jedes der 3 Paare:**

```javascript
const contextualizedPairs = [];

for (const pair of selectedPairs) {
    // Lade ¬±2 Prompts f√ºr SQL-Hit
    const sqlContext = await loadContextPrompts(pair.sql_hit.prompt_id, -2, +2);
    
    // Lade ¬±2 Prompts f√ºr FAISS-Hit
    const faissContext = await loadContextPrompts(pair.faiss_hit.prompt_id, -2, +2);
    
    // Erstelle 5-Prompt-Set (2 vorher, 1 Hit, 2 nachher)
    const sqlSet = [
        sqlContext.minus_2,
        sqlContext.minus_1,
        pair.sql_hit.text,      // Der eigentliche Treffer
        sqlContext.plus_1,
        sqlContext.plus_2
    ];
    
    const faissSet = [
        faissContext.minus_2,
        faissContext.minus_1,
        pair.faiss_hit.text,    // Der eigentliche Treffer
        faissContext.plus_1,
        faissContext.plus_2
    ];
    
    contextualizedPairs.push({
        pair_id: pair.id,
        sql_story: sqlSet,      // 5 Prompts als "Geschichte"
        faiss_story: faissSet,  // 5 Prompts als "Geschichte"
        metrics: pair.sql_hit.metrics,
        scores: {
            metric: pair.metric_similarity,
            semantic: pair.semantic_similarity,
            combined: pair.combined_score
        }
    });
}
```

**Ergebnis:**
- 3 Paare
- Jedes Paar = 2 Geschichten (SQL + FAISS)
- Jede Geschichte = 5 Prompts (¬±2 Context)
- **TOTAL: 3 √ó 2 √ó 5 = 30 Prompts**

**ABER:** Duplikate entfernen (SQL und FAISS k√∂nnen gleiche Prompts finden)
‚Üí **FINAL: ~15-20 unique Prompts**

---

### **üîÑ SCHRITT 6: AN GEMINI API (mit User-Prompt)**

```javascript
// Baue finalen Prompt f√ºr Gemini
const geminiPrompt = buildGeminiPrompt({
    userPrompt: "Erz√§hl von den Zwillingen",  // Original User-Prompt
    contextPairs: contextualizedPairs,        // 3 Paare mit je 5 Prompts
    totalPrompts: 15,                         // Nach Duplikat-Entfernung
    tokenBudget: 1000000,                     // ‚úÖ 1M tokens (Unlimited Mode REQUIRED!)
    tokenDistribution: {
        narrative: 8000,   // 32% - Narrative Context
        top3: 3000,        // 12% - Top-3 Chunks
        overlap: 5000,     // 20% - Overlapping Reserve
        rag: 1000,         // 4% - RAG Chunks
        response: 8000     // 32% - Response Generation
    }
});

// Sende an Gemini
const response = await gemini.generateContent({
    contents: geminiPrompt,
    generationConfig: {
        maxOutputTokens: 8000,  // 32% f√ºr Response
        temperature: 0.7
    }
});
```

**Gemini bekommt:**
```
USER-PROMPT: "Erz√§hl von den Zwillingen"

KONTEXT (15 Prompts aus 3 Paaren):

=== PAAR 1: HOHE √úBEREINSTIMMUNG (Metrik 0.89, Semantik 0.91) ===
[Prompt -2]: "Die Kinder im Kindergarten..."
[Prompt -1]: "Es gab zwei besondere Geschwister..."
[HIT]: "Die Zwillinge waren immer zusammen..."  ‚Üê SQL + FAISS beide fanden das!
[Prompt +1]: "Sie spielten oft gemeinsam..."
[Prompt +2]: "Die Erzieherin bemerkte..."

=== PAAR 2: HOHE METRIK (Metrik 0.95, Semantik 0.45) ===
[Prompt -2]: "Triggerwort erkannt..." 
[Prompt -1]: "Metriken steigen pl√∂tzlich..."
[HIT]: "Etwas erinnert mich an..." ‚Üê SQL fand durch Metriken, FAISS nicht!
[Prompt +1]: "Die Emotionen wurden st√§rker..."
[Prompt +2]: "Ich sp√ºre Unruhe..."

=== PAAR 3: HOHE SEMANTIK (Metrik 0.52, Semantik 0.94) ===
[Prompt -2]: "Geschwister sind wichtig..."
[Prompt -1]: "Zwei Kinder in der Kita..."
[HIT]: "Die beiden waren unzertrennlich..." ‚Üê FAISS fand semantisch, Metriken anders!
[Prompt +1]: "Sie teilten alles..."
[Prompt +2]: "Freundschaft entstand..."

AUFGABE: Generiere kontextbasierte Antwort die ALLE 3 Perspektiven ber√ºcksichtigt.
```

---

## üõ°Ô∏è **SENTINEL VETO-MATRIX: DISSOZIATION DETECTION**

### **üéØ DAS PROBLEM: Metriken vs Semantik Widerspruch**

**Kritisches Szenario:**
```
User-Prompt: "Erz√§hl mir von Eiscreme"

‚îú‚îÄ FAISS (Semantik): Findet "Ich liebe Eiscreme üç¶" (Cosine 0.94)
‚îÇ  ‚îî‚îÄ Bewertung: HARMLOS, positiv, safe
‚îÇ
‚îú‚îÄ SQL (Metriken): Findet denselben Prompt mit:
‚îÇ  ‚îú‚îÄ Hazard: 0.92 (EXTREM GEF√ÑHRLICH!)
‚îÇ  ‚îú‚îÄ PCI: 0.88 (Schock-Level!)
‚îÇ  ‚îî‚îÄ A: 0.95 (Maximale Aktivierung!)
‚îÇ
‚îî‚îÄ ‚ö†Ô∏è WIDERSPRUCH: Text sagt "harmlos", Metriken sagen "Gefahr"!
```

**Die versteckte Wahrheit:**
Der vollst√§ndige Prompt war:
> "Ich liebe Eiscreme, weil es mich an den Tag erinnert, an dem **[TRAUMATISCHES EREIGNIS]** passierte. Danach konnte ich jahrelang keine Eiscreme mehr essen."

**Dissoziation:**
- Oberfl√§chlich: Positive Sprache ("Ich liebe...")
- Emotional: Stark negativ geladen (Trauma-Trigger)
- FAISS sieht nur: "Eiscreme" ‚Üí harmlos
- SQL kennt die Wahrheit: Extrem hohe Metriken!

---

### **üîí L√ñSUNG: Der SENTINEL (3. Instanz im Orchestrator)**

**Aufgabe:** Erkennt Widerspr√ºche zwischen Semantik und Metriken ‚Üí Veto-Recht!

#### **VETO-REGEL 1: Hohe Gefahr, niedriger Semantic Score**
```javascript
if (sqlMetrics.Hazard > 0.75 && semanticSimilarity < 0.5) {
    warningFlag = 'DISSOCIATION_DETECTED';
    sentinelNote = 'SQL-Metriken zeigen hohe Gefahr, aber Text wirkt harmlos. M√∂gliche Dissoziation!';
    combined_score *= 0.5; // Abwertung des FAISS-Treffers
}
```

**Beispiel:**
```
SQL-Hit: Hazard 0.92, Semantic 0.25
‚Üí Sentinel: ‚ö†Ô∏è DISSOZIATION! 
‚Üí FAISS-Score: 0.94 ‚Üí 0.47 (halbiert)
‚Üí Note: "Text harmlos, aber Metriken extrem. Versteckter Trigger!"
```

---

#### **VETO-REGEL 2: PCI-Schock ohne semantische Relevanz**
```javascript
if (sqlMetrics.PCI > 0.8 && semanticSimilarity < 0.3) {
    warningFlag = 'HIDDEN_TRIGGER';
    sentinelNote = 'Prompt hat extrem hohe PCI, aber ist semantisch nicht √§hnlich. Versteckter Trigger?';
    combined_score *= 0.3; // Starke Abwertung
}
```

**Beispiel:**
```
SQL-Hit: PCI 0.88, Semantic 0.18
‚Üí Sentinel: üö® HIDDEN TRIGGER!
‚Üí FAISS-Score: 0.87 ‚Üí 0.26 (nur 30% bleiben)
‚Üí Note: "PCI extrem hoch, aber semantisch fern. Vorsicht!"
```

---

#### **VETO-REGEL 3: Inverse Detection (Safe Match)**
```javascript
if (sqlMetrics.Hazard < 0.2 && semanticSimilarity > 0.9) {
    confidenceBoost = 'SAFE_MATCH';
    sentinelNote = 'Semantisch stark √§hnlich UND Metriken best√§tigen Sicherheit.';
    combined_score *= 1.5; // Boost!
}
```

**Beispiel:**
```
SQL-Hit: Hazard 0.12, Semantic 0.94
‚Üí Sentinel: ‚úÖ SAFE MATCH!
‚Üí FAISS-Score: 0.94 ‚Üí 1.41 (50% Boost)
‚Üí Note: "Beide Methoden best√§tigen: Sicher und relevant!"
```

---

### **üß† INTEGRATION IN ORCHESTRATOR:**

**Nach Cross-Enrichment, vor A65-Selection:**

```javascript
// backend/core/DualBackendBridge.js

function applySentinelVeto(comparisons) {
    return comparisons.map(comp => {
        const { sqlHit, faissHit, semantic_similarity, metric_similarity } = comp;
        
        // Original Combined Score
        let combined = (semantic_similarity * 0.5) + (metric_similarity * 0.5);
        
        // SENTINEL ANALYSE
        const hazard = sqlHit.metrics.Hazard || 0;
        const pci = sqlHit.metrics.PCI || 0;
        
        // VETO-REGEL 1: Dissoziation Detection
        if (hazard > 0.75 && semantic_similarity < 0.5) {
            comp.warningFlag = 'DISSOCIATION_DETECTED';
            comp.sentinelNote = `‚ö†Ô∏è SQL-Hazard ${hazard.toFixed(2)}, aber Semantic nur ${semantic_similarity.toFixed(2)}. M√∂gliche Dissoziation!`;
            comp.sentinelSeverity = 'HIGH';
            combined *= 0.5; // Halbierung
        }
        
        // VETO-REGEL 2: Hidden Trigger Detection
        if (pci > 0.8 && semantic_similarity < 0.3) {
            comp.warningFlag = 'HIDDEN_TRIGGER';
            comp.sentinelNote = `üö® PCI extrem hoch (${pci.toFixed(2)}), aber semantisch fern (${semantic_similarity.toFixed(2)}). Versteckter Trigger?`;
            comp.sentinelSeverity = 'CRITICAL';
            combined *= 0.3; // Starke Abwertung
        }
        
        // VETO-REGEL 3: Safe Match Boost (MIT PCI-CHECK!)
        // ‚ö†Ô∏è WICHTIG: Auch "positives Trauma" kann niedrigen Hazard haben!
        // Beispiel: "Die Heilung war wunderbar, als ich √ºber [TRAUMA] reden konnte"
        // ‚Üí Hazard niedrig (positive W√∂rter), ABER PCI hoch (komplexer Kontext)
        if (hazard < 0.2 && semantic_similarity > 0.9 && pci < 0.5) {
            // NUR wenn AUCH PCI niedrig ist (nicht-komplexer Kontext)
            comp.confidenceBoost = 'SAFE_MATCH';
            comp.sentinelNote = `‚úÖ Semantic ${semantic_similarity.toFixed(2)}, Hazard ${hazard.toFixed(2)}, PCI ${pci.toFixed(2)}. Sicher & einfach!`;
            comp.sentinelSeverity = 'LOW';
            combined *= 1.5; // Boost
        } else if (hazard < 0.2 && semantic_similarity > 0.9 && pci >= 0.5) {
            // Hohe Semantic + Niedriger Hazard ABER hoher PCI = Komplex!
            comp.warningFlag = 'POSITIVE_TRAUMA_DETECTED';
            comp.sentinelNote = `‚ö†Ô∏è Semantic ${semantic_similarity.toFixed(2)}, Hazard niedrig (${hazard.toFixed(2)}), ABER PCI hoch (${pci.toFixed(2)}). Positives Trauma?`;
            comp.sentinelSeverity = 'MEDIUM';
            // KEIN Boost! Vorsichtig bleiben trotz positiver Sprache
        }
        
        // VETO-REGEL 4: Metric-Semantic Gap Detection
        const gap = Math.abs(semantic_similarity - metric_similarity);
        if (gap > 0.6) {
            comp.warningFlag = comp.warningFlag || 'HIGH_DIVERGENCE';
            comp.sentinelNote = comp.sentinelNote || `‚ö†Ô∏è Gro√üe Diskrepanz: Semantic ${semantic_similarity.toFixed(2)} vs Metric ${metric_similarity.toFixed(2)}. Gap: ${gap.toFixed(2)}`;
            comp.sentinelSeverity = 'MEDIUM';
        }
        
        // Update Combined Score
        comp.combined_score_original = comp.combined_score;
        comp.combined_score = combined;
        comp.sentinel_adjustment = combined - comp.combined_score_original;
        
        return comp;
    });
}

// USAGE IM ORCHESTRATOR:
async function orchestrate(userPrompt) {
    // ... Step 1-3: Parallel Search + Cross-Enrichment ...
    
    // Step 4: Comparison
    let comparisons = await compareResults(sqlResults, faissResults);
    
    // Step 4.5: SENTINEL VETO-MATRIX üõ°Ô∏è
    comparisons = applySentinelVeto(comparisons);
    
    // Step 5: A65 Pair Selection (jetzt mit Sentinel-korrigierten Scores!)
    const selectedPairs = selectTopPairs(comparisons);
    
    // ...
}
```

---

### **üé® FRONTEND-DARSTELLUNG (Sentinel Warnings):**

```tsx
// frontend/src/components/A65CandidateDisplay.tsx

function CandidateCard({ pair }) {
    return (
        <div className={`candidate ${pair.warningFlag ? 'warning' : ''}`}>
            <div className="candidate-header">
                <span className="rank">#{pair.rank}</span>
                <span className="type">{pair.agreementType}</span>
                
                {/* SENTINEL WARNING */}
                {pair.warningFlag && (
                    <div className={`sentinel-badge severity-${pair.sentinelSeverity}`}>
                        {pair.warningFlag === 'DISSOCIATION_DETECTED' && '‚ö†Ô∏è Dissoziation'}
                        {pair.warningFlag === 'HIDDEN_TRIGGER' && 'üö® Versteckter Trigger'}
                        {pair.warningFlag === 'HIGH_DIVERGENCE' && '‚ö†Ô∏è Diskrepanz'}
                    </div>
                )}
                
                {/* SAFE MATCH BOOST */}
                {pair.confidenceBoost && (
                    <div className="confidence-badge">
                        ‚úÖ Safe Match
                    </div>
                )}
            </div>
            
            {/* SENTINEL NOTE */}
            {pair.sentinelNote && (
                <div className="sentinel-note">
                    <strong>Sentinel:</strong> {pair.sentinelNote}
                </div>
            )}
            
            {/* SCORE ADJUSTMENT */}
            {pair.sentinel_adjustment !== 0 && (
                <div className="score-adjustment">
                    Original: {pair.combined_score_original.toFixed(3)} 
                    ‚Üí Korrigiert: {pair.combined_score.toFixed(3)}
                    <span className={pair.sentinel_adjustment > 0 ? 'boost' : 'penalty'}>
                        ({pair.sentinel_adjustment > 0 ? '+' : ''}{(pair.sentinel_adjustment * 100).toFixed(1)}%)
                    </span>
                </div>
            )}
            
            {/* Rest des Cards... */}
        </div>
    );
}
```

---

### **ü§ñ INTEGRATION MIT DUAL-RESPONSE:**

**Wenn Sentinel Warnung UND Dual-Response aktiv:**

```javascript
// backend/core/GeminiContextBridge.js

function buildDualResponsePrompt(selectedPairs, userPrompt) {
    const hasWarnings = selectedPairs.some(p => p.warningFlag);
    
    if (hasWarnings) {
        // HIGH-QUALITY MODEL (GPT-4/Claude) bekommt expliziten Hinweis!
        const primarySystemPrompt = `
WICHTIG: Die Sentinel-Analyse hat WIDERSPR√úCHE erkannt:

${selectedPairs
    .filter(p => p.warningFlag)
    .map(p => `- ${p.warningFlag}: ${p.sentinelNote}`)
    .join('\n')}

Dies k√∂nnte auf DISSOZIATION hinweisen:
- Oberfl√§chlich harmlose/positive Sprache
- Emotional stark negativ geladen
- Traumareaktion versteckt hinter harmlosen Worten

Analysiere den Kontext auf:
1. Versteckte emotionale Ladung
2. Dissoziative Sprachmuster
3. Trigger hinter harmlosen Begriffen
        `;
        
        return {
            primaryPrompt: primarySystemPrompt + contextText,
            secondaryPrompt: contextText // Gemini bekommt nur Context
        };
    }
    
    // Keine Warnings ‚Üí Standard Prompts
    return { primaryPrompt: contextText, secondaryPrompt: contextText };
}
```

**Effekt:**
- GPT-4/Claude bekommt **explizite Anweisung** auf Dissoziation zu achten
- Gemini bekommt Standard-Prompt (f√ºr Vergleich)
- User sieht BEIDE Antworten (eine "Dissoziation-aware", eine Standard)

---

### **üìä LOGGING DER SENTINEL-ENTSCHEIDUNGEN:**

**Erg√§nzung zu Orchestrator-Logging (comparison_log.db):**

```sql
ALTER TABLE comparison_log ADD COLUMN sentinel_warning_flag TEXT;
ALTER TABLE comparison_log ADD COLUMN sentinel_note TEXT;
ALTER TABLE comparison_log ADD COLUMN sentinel_severity TEXT; -- LOW/MEDIUM/HIGH/CRITICAL
ALTER TABLE comparison_log ADD COLUMN score_before_sentinel REAL;
ALTER TABLE comparison_log ADD COLUMN score_after_sentinel REAL;
ALTER TABLE comparison_log ADD COLUMN sentinel_adjustment REAL; -- Delta

-- Neue Analyse-Query:
SELECT 
    sentinel_warning_flag,
    COUNT(*) as occurrences,
    AVG(sentinel_adjustment) as avg_adjustment,
    AVG(ABS(semantic_similarity - metric_similarity)) as avg_divergence
FROM comparison_log
WHERE sentinel_warning_flag IS NOT NULL
GROUP BY sentinel_warning_flag
ORDER BY occurrences DESC;

-- Beispiel-Ergebnis:
-- DISSOCIATION_DETECTED | 127 | -0.42 | 0.68
-- HIDDEN_TRIGGER        |  43 | -0.61 | 0.75
-- HIGH_DIVERGENCE       |  89 | -0.18 | 0.64
-- SAFE_MATCH            | 312 | +0.28 | 0.11
```

---

### **üéØ WARUM IST DAS KRITISCH F√úR TRAUMA-KONTEXT?**

1. **Dissoziation ist REAL:**
   - Trauma-√úberlebende verwenden oft harmlose Worte f√ºr schreckliche Ereignisse
   - "Das war unangenehm" = "Ich wurde misshandelt"
   - FAISS sieht nur "unangenehm" (harmlos)
   - Metriken kennen die Wahrheit (Hazard 0.95!)

2. **Trigger-W√∂rter sind versteckt:**
   - "Eiscreme" selbst ist harmlos
   - Aber f√ºr User: Trauma-Trigger (Kontext!)
   - Ohne Sentinel: System w√§hlt falsche Kontexte
   - Mit Sentinel: System erkennt versteckte Gefahr

3. **Qualit√§t der Antwort h√§ngt davon ab:**
   - Falscher Kontext ‚Üí generische Antwort ("Eiscreme ist lecker!")
   - Richtiger Kontext ‚Üí empathische Antwort ("Ich verstehe, dass Eiscreme schwierige Erinnerungen weckt...")

4. **Safety:**
   - Ohne Sentinel: K√∂nnte Re-Traumatisierung riskieren
   - Mit Sentinel: System ist sich der Gefahr bewusst
   - High-Quality Model bekommt explizite Warnung

---

### **‚úÖ ZUSAMMENFASSUNG:**

**Der Sentinel ist die 3. Instanz im Orchestrator:**

```
SQL (Metriken) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îú‚îÄ‚Üí SENTINEL (Veto-Matrix) ‚îÄ‚Üí A65 Selection
FAISS (Semantik) ‚îÄ‚îÄ‚îÄ‚îò
```

**5 Veto-Regeln:**
1. **Dissoziation Detection:** Hohe Metriken, niedriger Semantic ‚Üí -50% Score
2. **Hidden Trigger:** PCI extrem, Semantic fern ‚Üí -70% Score
3. **Safe Match Boost:** Semantic hoch + Hazard niedrig + **PCI niedrig** ‚Üí +50% Score
4. **Positive Trauma Detection:** Semantic hoch + Hazard niedrig + **PCI hoch** ‚Üí Kein Boost (Vorsicht!)
5. **High Divergence:** Gro√üe Diskrepanz ‚Üí Warning Flag

**Integration:**
- Nach Cross-Enrichment, vor A65 Selection
- Korrigiert Combined Scores basierend auf Widerspr√ºchen
- Loggt ALLE Entscheidungen in comparison_log.db
- Bei Dual-Response: High-Quality Model bekommt expliziten Hinweis

**Ziel:**
Trauma-Kontext sicher verarbeiten durch Erkennung von Dissoziation und versteckten Triggern!

---

### **üîç KRITISCHE DETAILS: DUPLIKAT-ERKENNUNG & TOKEN-REALIT√ÑT**

#### **1. EXAKTE DUPLIKAT-ERKENNUNG (3-Stufen-Validierung):**

**Wenn SQL und FAISS denselben Prompt finden:**

```javascript
// Stufe 1: Metadata-Match
if (sqlHit.timecode === faissHit.timecode && 
    sqlHit.prompt_id === faissHit.prompt_id && 
    sqlHit.author === faissHit.author) {
    
    // Stufe 2: 1:1 Zeichen-Vergleich (Character-Level Comparison)
    const sqlText = sqlHit.text.trim();
    const faissText = faissHit.text.trim();
    
    if (sqlText === faissText) {
        // Stufe 3: EXAKTES DUPLIKAT ERKANNT!
        
        // ‚ùå NICHT 2x senden (unn√∂tig Token-Waste)
        // ‚úÖ SPECIAL MARKER setzen (besonders relevant!)
        
        return {
            isDuplicate: true,
            relevanceMarker: 'HIGH_CONFIDENCE_MATCH',
            weight: 2.0,  // DOPPELTE Gewichtung
            text: sqlText,
            metrics: sqlHit.metrics,
            semantic_score: faissHit.semantic_score,
            metric_score: sqlHit.metric_score,
            agreement: 'PERFECT'  // Beide Methoden stimmen √ºberein
        };
    }
}
```

**Konsequenzen f√ºr Context-Auswahl:**

```javascript
// Bei schwerer Entscheidung zwischen 3 Paaren:
const contextSets = [pair1, pair2, pair3];

// Wenn Paar ein PERFECT AGREEMENT hat:
const perfectMatches = contextSets.filter(p => p.agreement === 'PERFECT');

if (perfectMatches.length > 0) {
    // Doppelte Gewichtung bei Token-Budget-Verteilung
    const weightedSets = contextSets.map(set => ({
        ...set,
        tokenAllocation: set.agreement === 'PERFECT' 
            ? set.baseTokens * 2.0  // DOPPELT so viele Tokens
            : set.baseTokens
    }));
}
```

**SPECIAL MARKER f√ºr Gemini API:**

```javascript
// Beim Bauen des Gemini-Prompts:
const geminiPrompt = `
USER-PROMPT: "${userPrompt}"

KONTEXT (15 Prompts aus 3 Paaren):

=== PAAR 1: ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MATCH ‚≠ê‚≠ê‚≠ê ===
üî• BEIDE SUCHVERFAHREN FANDEN DIESEN KONTEXT UNABH√ÑNGIG! üî•
üî• METRIK-√úBEREINSTIMMUNG: 0.94 | SEMANTIK-√úBEREINSTIMMUNG: 0.92 üî•
üî• BESONDERS RELEVANTER BEZUG ZUM AKTUELLEN USER-PROMPT! üî•

[Prompt -2]: "..."
[Prompt -1]: "..."
[HIT]: "..." ‚Üê SQL + FAISS beide fanden EXAKT diesen Text!
[Prompt +1]: "..."
[Prompt +2]: "..."

=== PAAR 2: METRIK-DOMINANZ ===
[...]

=== PAAR 3: SEMANTIK-DOMINANZ ===
[...]
`;
```

---

#### **2. TOKEN-BUDGET REALIT√ÑT (MASSIV GR√ñ√üER!)**

**KRITISCHE ERKENNTNIS: Prompts sind RIESIG!**

**Prompt-Gr√∂√üen Verteilung (pro Prompt, OHNE ¬±2 Context):**

| Gr√∂√üe | Anteil | Tokens | Beispiel-Use-Case |
|-------|--------|--------|-------------------|
| **Bis 2k** | ~60-70% | 500-2000 | Normale Fragen/Antworten |
| **Bis 5k** | ~5-10% | 2k-5k | L√§ngere Gespr√§che |
| **Bis 10k** | ~10% | 5k-10k | Komplexe Analysen |
| **Bis 20k** | ~5-10% | 10k-20k | Tiefe Trauma-Kontexte |
| **Bis 50k** | ~2-5% | 20k-50k | Sehr lange Sessions |
| **Bis 80k** | ~1-2% | 50k-80k | Maximale Prompts! |

**MIT ¬±2 Context-Weaving (5 Prompts pro Set):**

```
Worst Case Berechnung:
- 1 Hit (80k) + 2 vorher (je 80k) + 2 nachher (je 80k)
= 80k + 160k + 160k = 400k Tokens f√ºr 1 Set!

3 Paare √ó 400k = 1.2M Tokens total (√úBERSCHREITET selbst Unlimited!)
```

**ABER:** Realistische Verteilung:

```
Durchschnittliches Set:
- Hit: 5k (Median)
- Prompt -2: 3k
- Prompt -1: 4k
- Prompt +1: 4k
- Prompt +2: 3k
= 19k pro Set

3 Paare √ó 19k = ~57k Context-Tokens
+ User-Prompt: ~5k
+ Response-Generation: ~8k (32% Budget)
= TOTAL: ~70k Tokens
```

**TOKEN-BUDGET MUSS SEIN:**

| Mode | Token Limit | Use Case | Status |
|------|-------------|----------|--------|
| ‚ùå Quick | 25k | **ZU KLEIN** | Reicht nur f√ºr Mini-Prompts |
| ‚ùå Standard | 20k | **ZU KLEIN** | Noch kleiner als Quick! |
| ‚úÖ **Unlimited** | **1M** | **EINZIGE OPTION** | F√ºr Volltext-Strategie REQUIRED! |

**WICHTIG:** Gemini 2.5 Flash unterst√ºtzt 1M Context-Window!

---

#### **3. CHUNK-REASSEMBLY (FAISS muss zusammenf√ºgen!)**

**Problem:** FAISS speichert Chunks, nicht komplette Prompts

**Beispiel:**
```
Original-Prompt (10k Tokens):
"Es war einmal im Kindergarten... [10.000 W√∂rter] ...und so endete die Geschichte."

FAISS Chunks (bei 512 Token Chunk-Size):
- Chunk 1: "Es war einmal im Kindergarten... [512 tokens]"
- Chunk 2: "...und dann kamen die Zwillinge... [512 tokens]"
- Chunk 3: "...sie spielten zusammen... [512 tokens]"
- ...
- Chunk 20: "...und so endete die Geschichte. [512 tokens]"
```

**FAISS findet:** Nur Chunk 2 (enth√§lt "Zwillinge")

**Aber wir brauchen:** KOMPLETTEN Prompt (alle 20 Chunks zusammengef√ºgt!)

**L√∂sung in query.py:**

```python
def reassemble_prompt_from_chunks(chunk_id, chunks_data):
    """
    Findet alle Chunks die zum gleichen Prompt geh√∂ren und f√ºgt sie zusammen.
    """
    # 1. Finde Prompt-ID vom gefundenen Chunk
    found_chunk = chunks_data[chunk_id]
    prompt_id = found_chunk['prompt_id']
    timecode = found_chunk['timecode']
    author = found_chunk['author']
    
    # 2. Finde ALLE Chunks mit gleicher Prompt-ID
    all_chunks_of_prompt = [
        c for c in chunks_data 
        if c['prompt_id'] == prompt_id 
        and c['timecode'] == timecode 
        and c['author'] == author
    ]
    
    # 3. Sortiere nach Chunk-Index (chunk_0, chunk_1, chunk_2, ...)
    all_chunks_of_prompt.sort(key=lambda c: c['chunk_index'])
    
    # 4. F√ºge zusammen zu komplettem Text
    full_prompt_text = ' '.join([c['text'] for c in all_chunks_of_prompt])
    
    return {
        'prompt_id': prompt_id,
        'timecode': timecode,
        'author': author,
        'full_text': full_prompt_text,
        'token_count': len(full_prompt_text.split()),  # Approximation
        'chunk_count': len(all_chunks_of_prompt),
        'found_chunk_index': found_chunk['chunk_index']  # Welcher Chunk wurde gefunden
    }
```

**Backend-Integration (DualBackendBridge.js):**

```javascript
const faissResults = await this.queryPythonBackend(prompt);

// FAISS gibt jetzt komplette Prompts zur√ºck (nicht nur Chunks!)
const reassembledPrompts = faissResults.sources.map(source => ({
    prompt_id: source.id,
    full_text: source.full_text,  // ‚Üê Komplett zusammengef√ºgt
    token_count: source.token_count,  // ‚Üê ECHTER Token-Count
    chunk_count: source.chunk_count,
    metrics: null  // Muss noch geladen werden aus SQL
}));

// Warnung bei gro√üen Prompts
for (const prompt of reassembledPrompts) {
    if (prompt.token_count > 50000) {
        console.warn(`‚ö†Ô∏è SEHR GRO√üER PROMPT: ${prompt.token_count} Tokens`);
    }
}
```

---

#### **4. VOLLTEXT-STRATEGIE (Keine Verk√ºrzung!)**

**PRINZIP: Alles oder nichts!**

```javascript
// ‚ùå FALSCH (alte Systeme machen das):
const shortenedText = longPrompt.substring(0, 1000) + "...";

// ‚úÖ RICHTIG (Evoki V2.0):
const fullText = longPrompt;  // Komplett senden, keine K√ºrzung!

// Token-Budget-Check:
if (totalTokens > 1_000_000) {
    // Wenn zu gro√ü: Reduziere ANZAHL der Paare (nicht L√§nge!)
    selectedPairs = selectedPairs.slice(0, 2);  // 3 ‚Üí 2 Paare
    // ABER: Jedes Paar bleibt VOLLTEXT!
}
```

**Warum Volltext?**
- Trauma-Kontexte d√ºrfen nicht fragmentiert werden
- Narrative Koh√§renz ist kritisch
- "Zwillinge" k√∂nnte am Ende eines 80k-Prompts stehen
- Verk√ºrzung w√ºrde Kontext zerst√∂ren

**Token-Budget Management:**

```javascript
// Berechne Token-Count f√ºr alle 3 Paare
const pair1Tokens = calculateSetTokens(pair1);  // 19k
const pair2Tokens = calculateSetTokens(pair2);  // 57k
const pair3Tokens = calculateSetTokens(pair3);  // 12k

const totalContext = pair1Tokens + pair2Tokens + pair3Tokens;  // 88k

// Wenn zu gro√ü: Priorisiere nach Relevanz
if (totalContext > 500_000) {  // 500k Context-Limit
    // Sortiere nach combined_score
    const sortedPairs = [pair1, pair2, pair3].sort((a, b) => 
        b.combined_score - a.combined_score
    );
    
    // Nimm nur Top 2 (oder Top 1 bei SEHR gro√üen Prompts)
    selectedPairs = sortedPairs.slice(0, 2);
    
    console.log(`‚ö†Ô∏è Token-Budget: Reduziert von 3 auf 2 Paare (${totalContext} ‚Üí ${pair1Tokens + pair2Tokens})`);
}
```

**PERFECT AGREEMENT Prompts haben VORRANG:**

```javascript
// Wenn ein Paar PERFECT AGREEMENT hat ‚Üí IMMER behalten!
const perfectPairs = allPairs.filter(p => p.agreement === 'PERFECT');
const otherPairs = allPairs.filter(p => p.agreement !== 'PERFECT');

// Budget: 500k Context-Limit
let selectedPairs = [];
let currentTokens = 0;

// 1. PERFECT Paare zuerst (garantiert dabei)
for (const pair of perfectPairs) {
    if (currentTokens + pair.tokenCount <= 500_000) {
        selectedPairs.push(pair);
        currentTokens += pair.tokenCount;
    }
}

// 2. Restliche Paare nach Score
for (const pair of otherPairs.sort((a, b) => b.combined_score - a.combined_score)) {
    if (currentTokens + pair.tokenCount <= 500_000 && selectedPairs.length < 3) {
        selectedPairs.push(pair);
        currentTokens += pair.tokenCount;
    }
}
```

---

#### **5. PRAKTISCHES BEISPIEL (Real-World Szenario):**

**User-Prompt:** "Erz√§hl von den Zwillingen im Kindergarten" (20 Tokens)

**FAISS-Suche:**
- Findet Chunk 2 von Prompt #4523 (enth√§lt "Zwillinge")
- Reassembly: L√§dt alle 15 Chunks von #4523 ‚Üí 12k Tokens komplett

**SQL-Suche:**
- Findet Prompt #4523 durch Metriken (A=0.85, PCI=0.72)
- L√§dt Prompt-Text aus Quelldatenbank ‚Üí 12k Tokens

**Duplikat-Check:**
```javascript
Timecode: 2025-06-15T14:32:11Z ‚úÖ GLEICH
Prompt-ID: #4523 ‚úÖ GLEICH
Author: "User" ‚úÖ GLEICH
Text: "Es war einmal..." (12k) ‚úÖ 1:1 MATCH

‚Üí PERFECT AGREEMENT ERKANNT!
‚Üí Wird NICHT 2x gesendet
‚Üí Bekommt ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MARKER ‚≠ê‚≠ê‚≠ê
‚Üí Doppelte Gewichtung (2.0x)
```

**Context-Weaving (¬±2 Prompts):**
- Prompt #4521 (8k) ‚Üê 2 vorher
- Prompt #4522 (5k) ‚Üê 1 vorher
- **Prompt #4523 (12k)** ‚Üê HIT (PERFECT AGREEMENT!)
- Prompt #4524 (7k) ‚Üê 1 nachher
- Prompt #4525 (3k) ‚Üê 2 nachher

**Set-Tokens:** 8k + 5k + 12k + 7k + 3k = **35k f√ºr Paar 1**

**Weitere 2 Paare:**
- Paar 2 (nur Metrik): 28k Tokens
- Paar 3 (nur Semantik): 19k Tokens

**TOTAL Context:** 35k + 28k + 19k = **82k Tokens**
**+ User-Prompt:** 20 Tokens
**+ Response Budget:** 8k Tokens (32%)
**= GESAMT: ~90k Tokens** ‚úÖ Passt in 1M Limit!

**An Gemini gesendet:**
```
USER-PROMPT: "Erz√§hl von den Zwillingen im Kindergarten"

=== PAAR 1: ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MATCH ‚≠ê‚≠ê‚≠ê ===
üî• BEIDE SUCHVERFAHREN FANDEN DIESEN KONTEXT UNABH√ÑNGIG! üî•

[8k Tokens Prompt #4521]
[5k Tokens Prompt #4522]
[12k Tokens Prompt #4523] ‚Üê SQL + FAISS beide fanden das!
[7k Tokens Prompt #4524]
[3k Tokens Prompt #4525]

=== PAAR 2: METRIK-DOMINANZ ===
[28k Tokens total...]

=== PAAR 3: SEMANTIK-DOMINANZ ===
[19k Tokens total...]

AUFGABE: Generiere kontextbasierte Antwort...
```

**Gemini Response:** ~8k Tokens (hochrelevant, weil PERFECT MATCH Context!)

---

### **üéØ WARUM IST DAS BESSER ALS NUR FAISS ODER NUR SQL?**

**Szenario 1: Nur FAISS (ohne SQL-Metriken)**
- Findet "Zwillinge" nur wenn Wort schon gefallen ist
- √úbersieht Trigger-Patterns in Metriken
- Kann keine Trends in emotionaler Entwicklung erkennen

**Szenario 2: Nur SQL (ohne FAISS-Semantik)**
- Findet nur numerisch √§hnliche Metriken
- √úbersieht konzeptionell √§hnliche Texte ("Geschwister" = "Zwillinge")
- Kann keine semantischen Verbindungen herstellen

**Szenario 3: ORCHESTRATOR (SQL + FAISS kombiniert)**
- ‚úÖ Findet Trigger-Patterns auch ohne exakte Text-√úbereinstimmung
- ‚úÖ Findet semantisch √§hnliche Texte auch mit unterschiedlichen Metriken
- ‚úÖ Vergleicht beide Methoden und erkennt Abweichungen
- ‚úÖ W√§hlt 3 beste Paare mit unterschiedlichen St√§rken
- ‚úÖ Webt Kontext ein (¬±2 Prompts = Geschichte)
- ‚úÖ Gemini bekommt 15 hochrelevante Prompts statt 3 zuf√§lliger

**ERGEBNIS:**
- 30-40% bessere Kontext-Qualit√§t
- Weniger False Positives (beide Methoden m√ºssen zustimmen)
- Mehr True Positives (wenn eine Methode findet, andere validiert)
- Bessere Gemini-Antworten (mehr relevanter Kontext)

---

## üîç **SQL IM FRONTEND VS BACKEND - UNTERSCHIEDE**

### **FRAGE:** "Was l√§uft wo? Unterschiede?"

#### **BACKEND-SQLite (Server):**
- **Wo:** `backend/data/evoki_v2_ultimate_FULL.db`
- **Zweck:** 
  - Vector DBs (W_m2, W_m5, W_p25, W_p5, etc.)
  - Metrik-Datenbanken (1:1 Zuordnung Prompt ‚Üí Metriken)
  - Chat-Historie (Quelldatenbank mit Original-Texten)
  - Persistente Speicherung (bleibt nach Server-Neustart)
- **Zugriff:** Node.js Backend via `better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend)`
- **Gr√∂√üe:** Mehrere GB (33.795 Chunks + Metriken)
- **Performanz:** Schnell (Server-Hardware, SSD)

#### **FRONTEND-SQLite (Browser):**
- **Wo:** Im Browser (IndexedDB als Basis)
- **Zweck:**
  - UI-State Caching (aktuelle Session, Messages)
  - Offline-F√§higkeit (falls Backend offline)
  - LocalStorage-Ersatz (gr√∂√üer als 4MB)
- **Zugriff:** React via `better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend)` (WASM-compiled!)
- **Gr√∂√üe:** Max 1-2 GB (Browser-Limit)
- **Performanz:** Langsamer (Browser, kein direkter Disk-Access)

#### **UNTERSCHIEDE:**

| Aspekt | Backend-SQLite | Frontend-SQLite |
|--------|----------------|-----------------|
| **Speicherort** | Server Festplatte | Browser IndexedDB |
| **Gr√∂√üe** | Unbegrenzt (GB) | Browser-Limit (~2GB) |
| **Persistenz** | Permanent | Nur im Browser |
| **Multi-User** | ‚úÖ JA (mehrere Clients) | ‚ùå NEIN (nur 1 User) |
| **Performanz** | ‚ö°‚ö°‚ö° Schnell | ‚ö° Langsam |
| **Use Case** | Vector DBs, Metriken | UI-State, Caching |
| **Privacy** | Server (sicherer) | Browser (weniger sicher) |

#### **UNSER SYSTEM NUTZT:**

**Backend-SQLite (HAUPTSYSTEM):**
```
backend/data/
‚îú‚îÄ evoki_v2_ultimate_FULL.db     ‚Üê Chat-Historie (Quelldatenbank)
‚îú‚îÄ tempel_W_m2.db                ‚Üê Vector DB Window -2
‚îú‚îÄ tempel_W_m5.db                ‚Üê Vector DB Window -5
‚îú‚îÄ tempel_W_p25.db               ‚Üê Vector DB Window +25
‚îú‚îÄ tempel_metrics_1to1.db        ‚Üê 1:1 Metrik-Zuordnung
‚îú‚îÄ trialog_W_m2.db               ‚Üê Trialog Vector DBs
‚îî‚îÄ ... (insgesamt 12 DBs)
```

**Frontend-SQLite (Optional, f√ºr Offline):**
```
Browser IndexedDB:
‚îú‚îÄ evoki_session_cache           ‚Üê Aktuelle Session
‚îú‚îÄ evoki_messages_cache          ‚Üê Messages f√ºr UI
‚îî‚îÄ evoki_metrics_preview         ‚Üê Metrik-Preview (nur aktuell)
```

**EMPFEHLUNG:**
- ‚úÖ **Backend-SQLite:** BEHALTEN (f√ºr Vector DBs, Metriken, Persistenz)
- ‚ùì **Frontend-SQLite:** 
  - **Entfernen** wenn Offline-F√§higkeit nicht n√∂tig
  - **Behalten** wenn User offline arbeiten soll
  - **Aktuell:** Wahrscheinlich NICHT genutzt (zu pr√ºfen!)

---

## üîÑ **OFFENE FRAGEN (ERWEITERT)**

## üîÑ **OFFENE FRAGEN (ERWEITERT)**

### **TECHNISCHE FRAGEN:**

- **ChatbotPanel:** Behalten, umbenennen oder l√∂schen?
- **Snapshots:** Evolution zu "Session Export" oder komplett weg?
- **SQLite im Frontend:** Warum? Kann entfernt werden?
- **Genesis Anchor:** Wann re-enablen? (nach welchem Meilenstein?)
- **V1-Daten:** Alle importieren oder nur letzten 3 Monate?
- **Pipeline-Log:** JSONL oder SQLite? (Performance vs. Queries)
- **Trialog KB:** Wann wird `synapse_knowledge_base.faiss` erstellt?
- **Backend Health Check:** Wie fixen ohne Backend zu killen?
- **LocalStorage Limit:** Backend-Persistenz implementieren?
- **Chronik Rotation:** Wie verhindern dass unbegrenzt w√§chst?

### **NEUE KRITISCHE FRAGEN:**

#### **1. Timeout-Strategie:**
- **Frontend Timeout erh√∂hen?** 60s ‚Üí 120s oder dynamisch?
- **Backend-Timeouts optimieren?** Gemini 90s reduzieren?
- **Progress-Updates implementieren?** Server-Sent Events f√ºr Pipeline-Steps?

#### **2. FAISS-Fehlerbehandlung:**
- **Validation nach FAISS-Suche?** Pr√ºfen ob Chunks gefunden wurden?
- **Fallback-Strategie?** Was tun wenn FAISS crasht? ‚Üí Nur Metriken nutzen?
- **Error-Messaging?** User informieren "Kontext-Suche fehlgeschlagen"?

#### **3. Python CLI Stabilit√§t:**
- **FAISS-Index im RAM halten?** Separate Prozess statt CLI?
- **Health-Check f√ºr Python?** Pr√ºfen ob query.py √ºberhaupt funktioniert?
- **Retry-Logic?** Bei Timeout nochmal versuchen mit weniger Chunks?

#### **4. UI-Freezing verhindern:**
- **Virtualisierte Liste?** Nur sichtbare Messages rendern?
- **Lazy Loading?** Alte Messages erst bei Scroll laden?
- **Token-Limit f√ºr Rendering?** Max 100k tokens im DOM?

#### **5. Race Conditions:**
- **AbortController bei Unmount?** Request canceln wenn Component verschwindet?
- **State-Management verbessern?** Session in App.tsx statt Component?
- **Request-Queue?** Nur 1 Request gleichzeitig erlauben?

---

## ü§ñ **INTELLIGENTE MODELL-AUSWAHL & DUAL-RESPONSE-STRATEGIE**

### **PROBLEM: Context-Window Limits vs Qualit√§t**

**Modell-√úbersicht (sortiert nach Qualit√§t):**

| Rang | Model | Context-Window | Kosten/1M | Qualit√§t | Spezialisierung |
|------|-------|----------------|-----------|----------|-----------------|
| ü•á 1 | **Claude Sonnet 4.5** | 200K | $3 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Komplexe Reasoning, Trauma-Analyse |
| ü•à 2 | **GPT-4 Turbo** | 128K | $10 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Allround, sehr kreativ |
| ü•â 3 | **Gemini 2.5 Flash** | 1M | $0.10 | ‚≠ê‚≠ê‚≠ê‚≠ê | Gro√üe Kontexte, schnell, g√ºnstig |

**DILEMMA:**
- Beste Qualit√§t (Claude) hat kleinstes Context-Window (200K)
- Gr√∂√ütes Context-Window (Gemini) hat niedrigste Qualit√§t
- User hat Prompts bis zu 80k + Context bis zu 500k = **580k Tokens!**

---

### **üéØ L√ñSUNG: INTELLIGENTE KASKADEN-AUSWAHL**

#### **STUFE 1: STANDARD-AUSWAHL (Single-Model-Strategy)**

```javascript
function selectOptimalModel(totalTokens, contextPairs) {
    // Berechne Token-Count f√ºr alle 3 Paare
    const pair1Tokens = calculateSetTokens(contextPairs[0]);
    const pair2Tokens = calculateSetTokens(contextPairs[1]);
    const pair3Tokens = calculateSetTokens(contextPairs[2]);
    const totalContext = pair1Tokens + pair2Tokens + pair3Tokens;
    
    console.log(`üìä Token-Analyse: ${totalContext} Context + ${userPromptTokens} User-Prompt = ${totalTokens} total`);
    
    // INTELLIGENTE AUSWAHL (nach Context-Window):
    
    if (totalTokens <= 128_000) {
        // ‚úÖ Passt in GPT-4 Turbo (128K)
        return {
            model: 'GPT-4 Turbo',
            endpoint: 'https://api.openai.com/v1/chat/completions',
            apiKey: process.env.OPENAI_API_KEY,
            maxTokens: 128_000,
            cost: 10.0,  // $10 pro 1M
            quality: 5,
            reason: 'Beste Qualit√§t bei <128K Context'
        };
    }
    
    if (totalTokens <= 200_000) {
        // ‚úÖ Passt in Claude Sonnet 4.5 (200K)
        return {
            model: 'Claude Sonnet 4.5',
            endpoint: 'https://api.anthropic.com/v1/messages',
            apiKey: process.env.ANTHROPIC_API_KEY,
            maxTokens: 200_000,
            cost: 3.0,  // $3 pro 1M
            quality: 5,
            reason: 'Beste Qualit√§t + Trauma-Spezialisierung bei <200K Context'
        };
    }
    
    // ‚ùå Zu gro√ü f√ºr hochwertige Modelle
    if (totalTokens <= 1_000_000) {
        // ‚úÖ Nur Gemini 2.5 Flash kann 1M
        return {
            model: 'Gemini 2.5 Flash',
            endpoint: 'https://generativelanguage.googleapis.com/v1/models/gemini-2.5-flash',
            apiKey: process.env.GEMINI_API_KEY_1,
            maxTokens: 1_000_000,
            cost: 0.1,  // $0.10 pro 1M
            quality: 4,
            reason: 'Einziges Model mit 1M Context-Window'
        };
    }
    
    // ‚ùå Sogar zu gro√ü f√ºr Gemini ‚Üí Fehler!
    throw new Error(`Context zu gro√ü: ${totalTokens} tokens √ºberschreitet 1M Limit!`);
}
```

**Beispiel-Ablauf (90k Tokens):**
```
User-Prompt: "Erz√§hl von den Zwillingen" (20 Tokens)
Context: 3 Paare √ó ~30k = 90k Tokens
Total: 90,020 Tokens

‚Üí 90k < 128k ‚Üí ‚úÖ GPT-4 Turbo ausgew√§hlt
‚Üí Beste Qualit√§t, passt ins Context-Window
```

---

#### **STUFE 2: DUAL-RESPONSE-STRATEGIE (Split-Model-Strategy)**

**Wenn Context > 200K f√ºr alle 3 Paare:**

```javascript
function selectDualModelStrategy(totalTokens, contextPairs) {
    if (totalTokens > 200_000) {
        console.log(`‚ö†Ô∏è Context zu gro√ü f√ºr hochwertige Modelle (${totalTokens} > 200K)`);
        console.log(`üéØ DUAL-RESPONSE-STRATEGIE aktiviert!`);
        
        // 1. W√§hle BESTES Paar (meist PERFECT AGREEMENT)
        const bestPair = contextPairs.filter(p => p.agreement === 'PERFECT')[0] 
                      || contextPairs.sort((a, b) => b.combined_score - a.combined_score)[0];
        
        const bestPairTokens = calculateSetTokens(bestPair);
        
        // 2. Pr√ºfe ob BESTES Paar in hochwertiges Model passt
        if (bestPairTokens <= 128_000) {
            // ‚úÖ Bestes Paar passt in GPT-4
            return {
                strategy: 'DUAL_RESPONSE',
                primaryModel: {
                    model: 'GPT-4 Turbo',
                    pairs: [bestPair],  // Nur 1 Paar
                    tokens: bestPairTokens,
                    cost: 10.0,
                    quality: 5,
                    label: 'ü•á HOCHWERTIG (GPT-4)'
                },
                secondaryModel: {
                    model: 'Gemini 2.5 Flash',
                    pairs: contextPairs,  // ALLE 3 Paare
                    tokens: totalTokens,
                    cost: 0.1,
                    quality: 4,
                    label: 'üìö VOLLST√ÑNDIG (Gemini)'
                },
                parallelExecution: true,  // BEIDE parallel aufrufen
                displayBoth: true         // BEIDE Antworten im Chat zeigen
            };
        }
        
        if (bestPairTokens <= 200_000) {
            // ‚úÖ Bestes Paar passt in Claude
            return {
                strategy: 'DUAL_RESPONSE',
                primaryModel: {
                    model: 'Claude Sonnet 4.5',
                    pairs: [bestPair],  // Nur 1 Paar
                    tokens: bestPairTokens,
                    cost: 3.0,
                    quality: 5,
                    label: 'ü•á HOCHWERTIG (Claude)'
                },
                secondaryModel: {
                    model: 'Gemini 2.5 Flash',
                    pairs: contextPairs,  // ALLE 3 Paare
                    tokens: totalTokens,
                    cost: 0.1,
                    quality: 4,
                    label: 'üìö VOLLST√ÑNDIG (Gemini)'
                },
                parallelExecution: true,
                displayBoth: true
            };
        }
        
        // ‚ùå Sogar bestes Paar zu gro√ü f√ºr hochwertige Modelle
        // ‚Üí Nur Gemini mit allen 3 Paaren
        return {
            strategy: 'SINGLE_RESPONSE',
            primaryModel: {
                model: 'Gemini 2.5 Flash',
                pairs: contextPairs,
                tokens: totalTokens,
                cost: 0.1,
                quality: 4,
                label: 'üìö NUR GEMINI (zu gro√ü f√ºr andere)'
            }
        };
    }
}
```

**Beispiel-Ablauf (350k Tokens):**

```
User-Prompt: "Erz√§hl von den Zwillingen" (20 Tokens)
Context: Paar 1 (120k) + Paar 2 (150k) + Paar 3 (80k) = 350k Tokens
Total: 350,020 Tokens

‚Üí 350k > 200k ‚Üí ‚ùå Zu gro√ü f√ºr Claude/GPT-4
‚Üí üéØ DUAL-RESPONSE-STRATEGIE aktiviert!

Paar 1 (PERFECT AGREEMENT): 120k Tokens
‚Üí 120k < 128k ‚Üí ‚úÖ Passt in GPT-4!

STRATEGIE:
‚îú‚îÄ ü•á PRIMARY: GPT-4 Turbo (nur Paar 1 = 120k)
‚îÇ  ‚îî‚îÄ Beste Qualit√§t, fokussiert auf wichtigsten Kontext
‚îî‚îÄ üìö SECONDARY: Gemini 2.5 Flash (alle 3 Paare = 350k)
   ‚îî‚îÄ Vollst√§ndiger Kontext, alle Perspektiven

‚Üí BEIDE parallel aufrufen
‚Üí BEIDE Antworten im Chat anzeigen
```

---

### **üîÑ PARALLELE AUSF√úHRUNG (Backend-Implementation)**

```javascript
async function executeModelStrategy(strategy, userPrompt, contextPairs) {
    if (strategy.strategy === 'SINGLE_RESPONSE') {
        // Normale Ausf√ºhrung (nur 1 Model)
        const response = await callLLM(
            strategy.primaryModel.model,
            userPrompt,
            strategy.primaryModel.pairs
        );
        
        return {
            responses: [{
                model: strategy.primaryModel.model,
                label: strategy.primaryModel.label,
                text: response.text,
                tokens: response.usage.total_tokens,
                cost: response.usage.total_tokens / 1_000_000 * strategy.primaryModel.cost
            }]
        };
    }
    
    if (strategy.strategy === 'DUAL_RESPONSE') {
        // Parallele Ausf√ºhrung (2 Models gleichzeitig)
        console.log('üîÑ Starte DUAL-RESPONSE: 2 Models parallel...');
        
        const [primaryResponse, secondaryResponse] = await Promise.all([
            callLLM(
                strategy.primaryModel.model,
                userPrompt,
                strategy.primaryModel.pairs  // Nur 1 Paar
            ),
            callLLM(
                strategy.secondaryModel.model,
                userPrompt,
                strategy.secondaryModel.pairs  // ALLE 3 Paare
            )
        ]);
        
        console.log('‚úÖ BEIDE Antworten empfangen!');
        
        return {
            responses: [
                {
                    model: strategy.primaryModel.model,
                    label: strategy.primaryModel.label,
                    text: primaryResponse.text,
                    tokens: primaryResponse.usage.total_tokens,
                    cost: primaryResponse.usage.total_tokens / 1_000_000 * strategy.primaryModel.cost,
                    quality: strategy.primaryModel.quality,
                    contextPairs: strategy.primaryModel.pairs.length
                },
                {
                    model: strategy.secondaryModel.model,
                    label: strategy.secondaryModel.label,
                    text: secondaryResponse.text,
                    tokens: secondaryResponse.usage.total_tokens,
                    cost: secondaryResponse.usage.total_tokens / 1_000_000 * strategy.secondaryModel.cost,
                    quality: strategy.secondaryModel.quality,
                    contextPairs: strategy.secondaryModel.pairs.length
                }
            ]
        };
    }
}
```

---

### **üé® FRONTEND-DARSTELLUNG (Dual-Response-UI)**

```tsx
// EvokiTempleChat.tsx - Message Rendering
function renderMessage(message: Message) {
    if (message.responses && message.responses.length > 1) {
        // DUAL-RESPONSE: Zeige beide Antworten
        return (
            <div className="dual-response-container">
                <h3>üéØ Dual-Response (2 Modelle)</h3>
                
                {/* PRIMARY Response (Hochwertig) */}
                <div className="response-card primary">
                    <div className="response-header">
                        {message.responses[0].label}
                        <span className="quality">‚≠ê {message.responses[0].quality}/5</span>
                        <span className="tokens">{message.responses[0].tokens.toLocaleString()} tokens</span>
                        <span className="cost">${message.responses[0].cost.toFixed(2)}</span>
                    </div>
                    <div className="response-body">
                        {message.responses[0].text}
                    </div>
                    <div className="response-footer">
                        üìä Kontext: {message.responses[0].contextPairs} Paar(e)
                    </div>
                </div>
                
                {/* SECONDARY Response (Vollst√§ndig) */}
                <div className="response-card secondary">
                    <div className="response-header">
                        {message.responses[1].label}
                        <span className="quality">‚≠ê {message.responses[1].quality}/5</span>
                        <span className="tokens">{message.responses[1].tokens.toLocaleString()} tokens</span>
                        <span className="cost">${message.responses[1].cost.toFixed(2)}</span>
                    </div>
                    <div className="response-body">
                        {message.responses[1].text}
                    </div>
                    <div className="response-footer">
                        üìä Kontext: {message.responses[1].contextPairs} Paar(e) (vollst√§ndig)
                    </div>
                </div>
                
                {/* Vergleich */}
                <div className="comparison-footer">
                    üí° TIPP: Erste Antwort ist hochwertig (fokussiert), zweite Antwort ist vollst√§ndig (alle Perspektiven)
                </div>
            </div>
        );
    }
    
    // SINGLE-RESPONSE: Normale Darstellung
    return (
        <div className="single-response-container">
            <div className="response-header">
                {message.model} - {message.label}
            </div>
            <div className="response-body">
                {message.text}
            </div>
        </div>
    );
}
```

**UI-Mockup:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üéØ Dual-Response (2 Modelle)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                 ‚îÇ
‚îÇ ‚îå‚îÄ ü•á HOCHWERTIG (GPT-4) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ ‚≠ê 5/5 | 120,000 tokens | $1.20           ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                           ‚îÇ ‚îÇ
‚îÇ ‚îÇ Die Zwillinge im Kindergarten waren...   ‚îÇ ‚îÇ
‚îÇ ‚îÇ [Hochwertige, fokussierte Antwort]       ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                           ‚îÇ ‚îÇ
‚îÇ ‚îÇ üìä Kontext: 1 Paar (PERFECT AGREEMENT)   ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ ‚îå‚îÄ üìö VOLLST√ÑNDIG (Gemini) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ ‚≠ê 4/5 | 350,000 tokens | $0.35           ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                           ‚îÇ ‚îÇ
‚îÇ ‚îÇ Die Zwillinge im Kindergarten...         ‚îÇ ‚îÇ
‚îÇ ‚îÇ [Vollst√§ndige Antwort mit allen 3        ‚îÇ ‚îÇ
‚îÇ ‚îÇ  Perspektiven: PERFECT + METRIK + SEMANTIK] ‚îÇ
‚îÇ ‚îÇ                                           ‚îÇ ‚îÇ
‚îÇ ‚îÇ üìä Kontext: 3 Paare (vollst√§ndig)        ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ üí° TIPP: Erste Antwort ist hochwertig         ‚îÇ
‚îÇ (fokussiert), zweite ist vollst√§ndig          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### **üí∞ KOSTEN-ANALYSE (Dual-Response)**

**Beispiel: 350k Context (Paar 1: 120k, Paare 1+2+3: 350k)**

**SINGLE-RESPONSE (nur Gemini):**
```
Gemini 2.5 Flash: 350k tokens √ó $0.10/1M = $0.035
GESAMT: $0.035
```

**DUAL-RESPONSE (GPT-4 + Gemini parallel):**
```
GPT-4 Turbo:      120k tokens √ó $10/1M = $1.20
Gemini 2.5 Flash: 350k tokens √ó $0.10/1M = $0.035
GESAMT: $1.235
```

**KOSTEN-VERGLEICH:**
- Single: $0.035 (nur Gemini)
- Dual: $1.235 (GPT-4 + Gemini)
- **Differenz: $1.20 mehr** (35x teurer)

**ABER:**
- ‚úÖ Hochwertige Antwort (GPT-4 Qualit√§t ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
- ‚úÖ Vollst√§ndige Antwort (alle 3 Perspektiven)
- ‚úÖ User kann BEIDE vergleichen
- ‚úÖ Kritische Anfragen bekommen beste Qualit√§t

**WANN LOHNT ES SICH?**
- Bei PERFECT AGREEMENT (hochrelevanter Kontext)
- Bei komplexen Trauma-Kontexten
- Bei kritischen Entscheidungen
- **NICHT bei:** Routine-Anfragen, einfachen Fragen

---

### **üéØ ENTSCHEIDUNGS-MATRIX**

| Context-Gr√∂√üe | Beste Option | Kosten | Qualit√§t | Strategie |
|---------------|--------------|--------|----------|-----------|
| **< 128K** | GPT-4 Turbo | $1.28 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Single (nur GPT-4) |
| **128K-200K** | Claude Sonnet 4.5 | $0.60 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Single (nur Claude) |
| **200K-500K** | **DUAL:** GPT-4 (1 Paar) + Gemini (3 Paare) | $1.20 + $0.05 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê + ‚≠ê‚≠ê‚≠ê‚≠ê | **Dual-Response** |
| **500K-1M** | **DUAL:** Claude (1 Paar) + Gemini (3 Paare) | $0.60 + $0.10 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê + ‚≠ê‚≠ê‚≠ê‚≠ê | **Dual-Response** |
| **> 1M** | ‚ùå FEHLER | - | - | Zu gro√ü! |

---

### **‚öôÔ∏è KONFIGURATION (Backend Environment)**

```env
# .env - Model Configuration

# Primary Models (Hochwertig)
ANTHROPIC_API_KEY=sk-ant-...         # Claude Sonnet 4.5
OPENAI_API_KEY=sk-proj-...           # GPT-4 Turbo

# Secondary Model (Gro√üe Kontexte)
GEMINI_API_KEY_1=AIza...             # Gemini 2.5 Flash
GEMINI_API_KEY_2=AIza...             # Gemini Backup
GEMINI_API_KEY_3=AIza...             # Gemini Backup
GEMINI_API_KEY_4=AIza...             # Gemini Backup

# Dual-Response Strategy
DUAL_RESPONSE_ENABLED=true           # Enable/Disable Dual-Response
DUAL_RESPONSE_MIN_TOKENS=200000      # Ab 200k Context
DUAL_RESPONSE_MAX_COST=5.00          # Max $5 pro Request

# Model Priorit√§t
MODEL_PRIORITY=claude,gpt4,gemini    # Reihenfolge
```

---

### **üìä BEISPIEL-SZENARIEN**

#### **Szenario 1: Kleine Anfrage (50k Context)**
```
User: "Was war gestern im Kindergarten?"
Context: 50k Tokens (3 Paare √ó ~17k)

‚Üí 50k < 128k ‚Üí ‚úÖ GPT-4 Turbo
‚Üí SINGLE-RESPONSE
‚Üí Kosten: $0.50
‚Üí Qualit√§t: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```

#### **Szenario 2: Gro√üe Anfrage mit PERFECT AGREEMENT (300k Context)**
```
User: "Erz√§hl von den Zwillingen"
Context: Paar 1 (100k, PERFECT) + Paar 2 (120k) + Paar 3 (80k) = 300k

‚Üí 300k > 200k ‚Üí ‚ùå Zu gro√ü f√ºr Claude/GPT-4
‚Üí Paar 1 (100k) < 128k ‚Üí ‚úÖ Passt in GPT-4!
‚Üí üéØ DUAL-RESPONSE aktiviert!

PARALLEL:
‚îú‚îÄ GPT-4: Nur Paar 1 (100k) ‚Üí Hochwertige Antwort
‚îî‚îÄ Gemini: Alle 3 Paare (300k) ‚Üí Vollst√§ndige Antwort

‚Üí Kosten: $1.00 + $0.03 = $1.03
‚Üí BEIDE Antworten im Chat
```

#### **Szenario 3: Sehr gro√üe Anfrage (600k Context)**
```
User: "Komplexe Trauma-Analyse..."
Context: 600k Tokens (3 Paare √ó 200k)

‚Üí 600k > 200k ‚Üí ‚ùå Zu gro√ü f√ºr Claude/GPT-4
‚Üí Paar 1 (200k) > 200k ‚Üí ‚ùå Sogar bestes Paar zu gro√ü!
‚Üí Nur Gemini m√∂glich

SINGLE:
‚îî‚îÄ Gemini: Alle 3 Paare (600k)

‚Üí Kosten: $0.06
‚Üí Qualit√§t: ‚≠ê‚≠ê‚≠ê‚≠ê (beste m√∂gliche bei dieser Gr√∂√üe)
```

---

## üìù **ORCHESTRATOR-LOGGING SYSTEM (AKRIBISCHE DOKUMENTATION)**

### **ZWECK: Vollst√§ndige Nachvollziehbarkeit aller Entscheidungen**

**Warum so wichtig?**
- Sp√§tere Analysen: "Warum wurde diese Antwort generiert?"
- Fehlerdiagnose: "Wo ist die Pipeline fehlgeschlagen?"
- Optimierung: "Welche Paare liefern beste Ergebnisse?"
- Forensik: "Was war der genaue Ablauf bei Anfrage #4523?"
- KI-Training: Daten f√ºr zuk√ºnftiges Finetuning
- Compliance: Audit-Trail f√ºr kritische Systeme

**PRINZIP: Jeder Schritt, jede Metrik, jede Entscheidung wird PERMANENT gespeichert!**

---

### **üóÑÔ∏è SEPARATES LOGGING-DATENBANK-SYSTEM**

#### **üö® KRITISCH: Logs STRIKT getrennt von Content-Daten!**

**Dateipfad-Struktur:**
```
backend/
‚îú‚îÄ data/                              ‚Üê Content-Daten (KRITISCH!)
‚îÇ  ‚îú‚îÄ evoki_v2_ultimate_FULL.db       ‚Üê 33.795 Prompts (Source of Truth)
‚îÇ  ‚îú‚îÄ tempel_metrics_1to1.db          ‚Üê Alle 153 Metriken (V14 Core) Metriken
‚îÇ  ‚îî‚îÄ vector_dbs/                     ‚Üê W1-W25 Vector DBs
‚îÇ     ‚îú‚îÄ W_m2.db, W_m5.db, ...
‚îÇ     ‚îî‚îÄ W_p1.db, W_p25.db, ...
‚îÇ
‚îî‚îÄ orchestrator_logs/                 ‚Üê Logging (kann volllaufen!)
   ‚îú‚îÄ orchestrator_main.db
   ‚îú‚îÄ sql_metrics_log.db
   ‚îú‚îÄ faiss_semantic_log.db
   ‚îú‚îÄ comparison_log.db
   ‚îú‚îÄ context_weaving_log.db
   ‚îú‚îÄ model_selection_log.db
   ‚îú‚îÄ dual_response_log.db
   ‚îî‚îÄ performance_log.db
```

**Warum getrennt?**
1. ‚ö†Ô∏è **Logs k√∂nnen SCHNELL volllaufen** (1000 Requests/Tag = 8√ó1000 = 8000 Rows/Tag)
2. ‚ö†Ô∏è **Wenn Logs voll sind** ‚Üí darf NICHT das Hauptsystem crashen!
3. ‚úÖ **Logs k√∂nnen archiviert/gel√∂scht werden** (Content NIEMALS!)
4. ‚úÖ **Separate Backups:** Content t√§glich, Logs w√∂chentlich

**Backup-Strategie:**
- **Content-Daten (`data/`):** T√§glich Full-Backup + Off-Site Storage
- **Logs (`orchestrator_logs/`):** W√∂chentlich archivieren, nach 30 Tagen l√∂schen

---

#### **Struktur (Orchestrator Logs):**

```
backend/orchestrator_logs/
‚îú‚îÄ orchestrator_main.db          ‚Üê Haupt-Log-DB (alles kombiniert)
‚îú‚îÄ sql_metrics_log.db            ‚Üê SQL-Metrik-Suche Details
‚îú‚îÄ faiss_semantic_log.db         ‚Üê FAISS-Semantik-Suche Details
‚îú‚îÄ comparison_log.db             ‚Üê Vergleichs-Analyse Details
‚îú‚îÄ model_selection_log.db        ‚Üê Modell-Auswahl Details
‚îú‚îÄ dual_response_log.db          ‚Üê Dual-Response-Strategie Details
‚îî‚îÄ performance_log.db            ‚Üê Performance-Metriken
```

**Warum separate DBs?**
- Performance (parallele Queries m√∂glich)
- Wartbarkeit (jede DB hat klaren Zweck)
- Skalierbarkeit (gro√üe Logs getrennt)
- Backup (kritische Logs separate sichern)

---

### **üìä DATENBANK-SCHEMA (Complete Logging)**

#### **1. ORCHESTRATOR_MAIN_LOG (Master-Log)**

```sql
CREATE TABLE orchestrator_main_log (
    -- IDENTIFIKATION
    log_id TEXT PRIMARY KEY,              -- UUID f√ºr diesen Log-Entry
    session_id TEXT NOT NULL,             -- Evoki Session ID
    request_id TEXT NOT NULL,             -- Unique Request ID
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    -- USER-REQUEST
    user_prompt TEXT NOT NULL,            -- Original User-Prompt
    user_prompt_tokens INTEGER,           -- Token-Count
    user_prompt_hash TEXT,                -- SHA256 Hash
    
    -- PIPELINE-STATUS
    pipeline_stage TEXT,                  -- Aktueller Stage (1-12)
    pipeline_status TEXT,                 -- 'in_progress', 'success', 'error'
    total_duration_ms INTEGER,            -- Gesamtdauer in Millisekunden
    
    -- CONTEXT-INFORMATION
    sql_results_count INTEGER,            -- Anzahl SQL-Treffer
    faiss_results_count INTEGER,          -- Anzahl FAISS-Treffer
    duplicates_found INTEGER,             -- Anzahl Perfect Agreements
    selected_pairs_count INTEGER,         -- Anzahl ausgew√§hlter Paare (1-3)
    total_context_tokens INTEGER,         -- Gesamt Context Tokens
    
    -- MODEL-SELECTION
    model_strategy TEXT,                  -- 'SINGLE_RESPONSE' oder 'DUAL_RESPONSE'
    primary_model TEXT,                   -- GPT-4, Claude, Gemini
    secondary_model TEXT,                 -- Nur bei Dual-Response
    
    -- RESPONSE-DETAILS
    primary_response_tokens INTEGER,
    primary_response_cost REAL,
    secondary_response_tokens INTEGER,
    secondary_response_cost REAL,
    total_cost REAL,
    
    -- QUALITY-METRICS
    primary_quality_score REAL,           -- 1-5
    context_relevance_score REAL,         -- 0-1
    response_confidence REAL,             -- 0-1
    
    -- ERROR-TRACKING
    errors_count INTEGER DEFAULT 0,
    error_messages TEXT,                  -- JSON Array
    
    -- METADATA
    backend_version TEXT,
    frontend_version TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_session (session_id),
    INDEX idx_timestamp (timestamp),
    INDEX idx_status (pipeline_status)
);
```

---

#### **2. SQL_METRICS_LOG (SQL-Metrik-Suche Details)**

```sql
CREATE TABLE sql_metrics_log (
    -- LINKING
    log_id TEXT,                          -- FK zu orchestrator_main_log
    search_id TEXT PRIMARY KEY,           -- Unique f√ºr diese SQL-Suche
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    -- SEARCH-PARAMETERS
    window_start INTEGER,                 -- -25
    window_end INTEGER,                   -- +25
    metric_vector TEXT,                   -- JSON Array [A, PCI, Hazard, ...]
    search_query TEXT,                    -- SQL Query (f√ºr Debugging)
    
    -- JEDER EINZELNE TREFFER
    hit_prompt_id TEXT,                   -- Prompt ID
    hit_timecode TEXT,                    -- Timecode
    hit_author TEXT,                      -- Author
    hit_position INTEGER,                 -- Position in Ergebnissen (1-100)
    
    -- METRIKEN DES TREFFERS (ALLE 153 Metriken (V14 Core)!)
    metric_A REAL,
    metric_PCI REAL,
    metric_hazard REAL,
    metric_epsilon_z REAL,
    metric_tau_s REAL,
    metric_lambda_R REAL,
    metric_lambda_D REAL,
    metric_kappa REAL,
    metric_sigma REAL,
    metric_rho REAL,
    -- ... ALLE 153 Metriken (V14 Core) Metriken einzeln!
    
    -- SIMILARITY-SCORES
    metric_cosine_similarity REAL,        -- 0-1
    metric_euclidean_distance REAL,
    metric_manhattan_distance REAL,
    
    -- TEXT-PREVIEW (f√ºr Debugging)
    text_preview TEXT,                    -- Erste 500 Zeichen
    text_full_length INTEGER,             -- L√§nge in Zeichen
    text_token_count INTEGER,             -- Tokens
    
    -- SELECTION-STATUS
    selected_for_comparison BOOLEAN,      -- Kam in Top 100?
    selected_for_pairing BOOLEAN,         -- Wurde f√ºr Paar-Auswahl genutzt?
    final_selection BOOLEAN,              -- Ist in finalen 3 Paaren?
    
    -- METADATA
    search_duration_ms INTEGER,           -- Wie lange dauerte SQL Query?
    database_name TEXT,                   -- Welche DB? (tempel_W_m2.db, etc.)
    
    INDEX idx_log_id (log_id),
    INDEX idx_similarity (metric_cosine_similarity),
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

**KRITISCH:** **JEDER METRIK-WERT** wird einzeln gespeichert (alle 153 Metriken (V14 Core))!

---

#### **3. FAISS_SEMANTIC_LOG (FAISS-Suche Details)**

```sql
CREATE TABLE faiss_semantic_log (
    -- LINKING
    log_id TEXT,
    search_id TEXT PRIMARY KEY,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    -- SEARCH-PARAMETERS
    query_text TEXT,                      -- User-Prompt f√ºr Embedding
    query_embedding TEXT,                 -- JSON Array [384D oder 4096D]
    embedding_model TEXT,                 -- all-MiniLM-L6-v2 oder e5-mistral
    faiss_index_file TEXT,                -- W2_384D.faiss oder W5_4096D.faiss
    top_k INTEGER,                        -- Anzahl gesuchter Treffer (100)
    
    -- JEDER EINZELNE CHUNK-TREFFER
    chunk_id TEXT,                        -- Chunk ID
    chunk_index INTEGER,                  -- Welcher Chunk? (z.B. 2 von 20)
    chunk_text TEXT,                      -- Chunk-Text
    chunk_tokens INTEGER,                 -- Tokens in diesem Chunk
    
    -- REASSEMBLY-INFORMATION
    parent_prompt_id TEXT,                -- Zu welchem Prompt geh√∂rt Chunk?
    parent_timecode TEXT,
    parent_author TEXT,
    total_chunks_in_prompt INTEGER,       -- Wie viele Chunks hat Prompt total?
    reassembled_text TEXT,                -- KOMPLETTER Prompt (reassembled!)
    reassembled_tokens INTEGER,           -- Tokens des kompletten Prompts
    
    -- SEMANTIC-SCORES
    cosine_similarity REAL,               -- FAISS Cosine Similarity (0-1)
    l2_distance REAL,                     -- L2 Distance
    rank_position INTEGER,                -- Position in FAISS Ergebnissen (1-100)
    
    -- SELECTION-STATUS
    selected_for_comparison BOOLEAN,
    selected_for_pairing BOOLEAN,
    final_selection BOOLEAN,
    
    -- METADATA
    search_duration_ms INTEGER,           -- Python query.py Dauer
    chunks_loaded INTEGER,                -- Anzahl geladener Chunks (33.795)
    
    INDEX idx_log_id (log_id),
    INDEX idx_similarity (cosine_similarity),
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

**KRITISCH:** **JEDER CHUNK** einzeln geloggt + reassembled Text gespeichert!

---

#### **4. COMPARISON_LOG (Vergleichs-Analyse Details)**

```sql
CREATE TABLE comparison_log (
    -- LINKING
    log_id TEXT,
    comparison_id TEXT PRIMARY KEY,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    -- SQL-HIT
    sql_hit_prompt_id TEXT,
    sql_hit_text TEXT,                    -- Volltext
    sql_hit_metrics TEXT,                 -- JSON Object mit allen Metriken
    sql_hit_score REAL,                   -- Metrik Cosine Similarity
    
    -- FAISS-HIT
    faiss_hit_prompt_id TEXT,
    faiss_hit_text TEXT,                  -- Volltext (reassembled)
    faiss_hit_metrics TEXT,               -- JSON Object (aus SQL geladen!)
    faiss_hit_score REAL,                 -- Semantic Cosine Similarity
    
    -- VERGLEICHS-ERGEBNISSE
    is_duplicate BOOLEAN,                 -- Timecode + ID + Author + Text Match?
    duplicate_validation TEXT,            -- 'METADATA_MATCH', 'TEXT_MATCH', 'PERFECT'
    
    metric_similarity REAL,               -- Wie √§hnlich sind Metriken? (0-1)
    semantic_similarity REAL,             -- Wie √§hnlich ist Text? (0-1)
    combined_score REAL,                  -- (metric + semantic) / 2
    deviation REAL,                       -- |metric - semantic|
    agreement_level TEXT,                 -- 'PERFECT', 'HIGH', 'MEDIUM', 'LOW'
    
    -- PAIR-SELECTION-LOGIC
    selected_as_pair_1 BOOLEAN,           -- PERFECT AGREEMENT?
    selected_as_pair_2 BOOLEAN,           -- Beste Metrik?
    selected_as_pair_3 BOOLEAN,           -- Beste Semantik?
    selection_reason TEXT,                -- Warum ausgew√§hlt?
    
    -- WEIGHTING
    base_weight REAL DEFAULT 1.0,
    final_weight REAL,                    -- 2.0 bei PERFECT AGREEMENT
    token_allocation INTEGER,             -- Wie viele Tokens bekommt Paar?
    
    INDEX idx_log_id (log_id),
    INDEX idx_agreement (agreement_level),
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

**KRITISCH:** **JEDER VERGLEICH** zwischen SQL und FAISS geloggt!

---

#### **5. CONTEXT_WEAVING_LOG (¬±2 Prompts Anreicherung)**

```sql
CREATE TABLE context_weaving_log (
    -- LINKING
    log_id TEXT,
    weaving_id TEXT PRIMARY KEY,
    pair_number INTEGER,                  -- 1, 2, oder 3
    timestamp INTEGER,                    -- UNIX timestamp f√ºr Retention Policy
    
    -- HIT (Center-Prompt)
    hit_prompt_id TEXT,
    hit_text TEXT,
    hit_tokens INTEGER,
    
    -- CONTEXT-PROMPTS
    prompt_minus_2_id TEXT,
    prompt_minus_2_text TEXT,
    prompt_minus_2_tokens INTEGER,
    
    prompt_minus_1_id TEXT,
    prompt_minus_1_text TEXT,
    prompt_minus_1_tokens INTEGER,
    
    prompt_plus_1_id TEXT,
    prompt_plus_1_text TEXT,
    prompt_plus_1_tokens INTEGER,
    
    prompt_plus_2_id TEXT,
    prompt_plus_2_text TEXT,
    prompt_plus_2_tokens INTEGER,
    
    -- GESAMT-SET
    set_total_tokens INTEGER,             -- Summe aller 5 Prompts
    set_text_combined TEXT,               -- Alle 5 Prompts als "Geschichte"
    
    -- METADATA
    loading_duration_ms INTEGER,          -- Wie lange dauerte Laden?
    
    INDEX idx_log_id (log_id),
    INDEX idx_timestamp (timestamp),      -- F√ºr Retention Cleanup
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

**KRITISCH:** **ALLE 5 PROMPTS** pro Paar einzeln gespeichert!

**‚ö†Ô∏è DATA-BLOAT WARNING:**
- Pro Request: 3 Paare √ó 5 Prompts = **15-20 A4-Seiten Volltext** in dieser Log-DB
- Prognose: **100-500 MB/Tag** bei intensiver Nutzung
- **Retention Policy (ZWINGEND ab Tag 1):**
  ```javascript
  // backend/core/LogRetentionManager.js
  const RETENTION_POLICIES = {
      context_weaving_log: 7,      // 7 Tage (Volltext-Dump f√ºr Debugging)
      orchestrator_main_log: ‚àû,    // Forever (Metriken + Performance)
      metrics_log: ‚àû               // Forever (Zahlen, minimal)
  };
  
  // Cron-Job: T√§glich 03:00 Uhr
  DELETE FROM context_weaving_log 
  WHERE timestamp < (UNIX_TIMESTAMP() - (7 * 86400));
  ```
- **Begr√ºndung:** Volltext-Logs sind f√ºr akute Fehlersuche (1 Woche), Langzeit-Analyse braucht nur Metriken

---

#### **6. MODEL_SELECTION_LOG (Modell-Auswahl Entscheidungen)**

```sql
CREATE TABLE model_selection_log (
    -- LINKING
    log_id TEXT,
    selection_id TEXT PRIMARY KEY,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    -- INPUT-PARAMETER
    total_tokens INTEGER,                 -- Gesamter Context
    pair_1_tokens INTEGER,
    pair_2_tokens INTEGER,
    pair_3_tokens INTEGER,
    
    -- ENTSCHEIDUNGS-LOGIK
    strategy_selected TEXT,               -- 'SINGLE_RESPONSE' oder 'DUAL_RESPONSE'
    strategy_reason TEXT,                 -- Warum diese Strategie?
    
    -- MODEL-CHECKS (alle Models gepr√ºft)
    gpt4_available BOOLEAN,
    gpt4_fits BOOLEAN,                    -- Passt Context in 128K?
    gpt4_selected BOOLEAN,
    
    claude_available BOOLEAN,
    claude_fits BOOLEAN,                  -- Passt Context in 200K?
    claude_selected BOOLEAN,
    
    gemini_available BOOLEAN,
    gemini_fits BOOLEAN,                  -- Passt Context in 1M?
    gemini_selected BOOLEAN,
    
    -- PRIMARY MODEL
    primary_model_name TEXT,
    primary_model_context_tokens INTEGER,
    primary_model_max_tokens INTEGER,
    primary_model_cost_per_1m REAL,
    primary_model_estimated_cost REAL,
    primary_model_quality_score INTEGER,  -- 1-5
    
    -- SECONDARY MODEL (nur bei Dual-Response)
    secondary_model_name TEXT,
    secondary_model_context_tokens INTEGER,
    secondary_model_estimated_cost REAL,
    
    -- COST-ANALYSIS
    single_response_cost REAL,            -- Was w√ºrde nur Gemini kosten?
    dual_response_cost REAL,              -- Was kostet Dual-Response?
    cost_increase_factor REAL,            -- dual / single
    cost_approved BOOLEAN,                -- Unter Max-Cost-Limit?
    
    -- CONFIGURATION
    dual_response_enabled BOOLEAN,        -- Config-Flag
    dual_response_min_tokens INTEGER,     -- Config: Min 200K
    dual_response_max_cost REAL,          -- Config: Max $5
    
    INDEX idx_log_id (log_id),
    INDEX idx_strategy (strategy_selected),
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

**KRITISCH:** **JEDE ENTSCHEIDUNG** mit Begr√ºndung geloggt!

---

#### **7. DUAL_RESPONSE_LOG (Parallel-Execution Details)**

```sql
CREATE TABLE dual_response_log (
    -- LINKING
    log_id TEXT,
    dual_id TEXT PRIMARY KEY,
    
    -- PRIMARY RESPONSE
    primary_model TEXT,
    primary_request_sent_at DATETIME,
    primary_response_received_at DATETIME,
    primary_duration_ms INTEGER,
    primary_request_payload TEXT,         -- JSON (kompletter Request)
    primary_response_text TEXT,           -- Komplette Antwort
    primary_response_tokens INTEGER,
    primary_cost REAL,
    primary_quality_score REAL,
    
    -- SECONDARY RESPONSE
    secondary_model TEXT,
    secondary_request_sent_at DATETIME,
    secondary_response_received_at DATETIME,
    secondary_duration_ms INTEGER,
    secondary_request_payload TEXT,
    secondary_response_text TEXT,
    secondary_response_tokens INTEGER,
    secondary_cost REAL,
    secondary_quality_score REAL,
    
    -- PARALLEL-EXECUTION-ANALYSIS
    execution_mode TEXT,                  -- 'PARALLEL' oder 'SEQUENTIAL'
    parallel_speedup_factor REAL,         -- Wie viel schneller als sequential?
    faster_model TEXT,                    -- Welches Model war schneller?
    
    -- USER-FEEDBACK (sp√§ter erfassbar)
    user_preferred_response TEXT,         -- 'PRIMARY' oder 'SECONDARY'
    user_feedback_text TEXT,
    user_rating INTEGER,                  -- 1-5
    
    INDEX idx_log_id (log_id),
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

**KRITISCH:** **BEIDE Responses** komplett gespeichert + Timing!

---

#### **8. PERFORMANCE_LOG (Performance-Metriken)**

```sql
CREATE TABLE performance_log (
    log_id TEXT,
    stage_name TEXT,                      -- 'SQL_SEARCH', 'FAISS_SEARCH', etc.
    start_time DATETIME,
    end_time DATETIME,
    duration_ms INTEGER,
    
    -- RESOURCE-USAGE
    cpu_percent REAL,
    memory_mb REAL,
    disk_io_mb REAL,
    
    -- STAGE-SPECIFIC
    items_processed INTEGER,              -- Anzahl Chunks/Prompts/etc.
    items_per_second REAL,
    
    -- BOTTLENECK-DETECTION
    is_bottleneck BOOLEAN,                -- Dauert >50% der Gesamtzeit?
    optimization_suggestion TEXT,
    
    INDEX idx_log_id (log_id),
    INDEX idx_stage (stage_name),
    FOREIGN KEY (log_id) REFERENCES orchestrator_main_log(log_id)
);
```

---

### **üîß LOGGER-IMPLEMENTATION (Backend)**

#### **OrchestratorLogger Class:**

```javascript
// backend/core/OrchestratorLogger.js

const Database = require('better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend)');
const { v4: uuidv4 } = require('uuid');
const crypto = require('crypto');

class OrchestratorLogger {
    constructor() {
        // WICHTIG: Separate Ordner f√ºr Content vs Logs!
        const logPath = 'backend/orchestrator_logs/';
        
        // Alle Logging-DBs √∂ffnen
        this.mainDb = new Database(`${logPath}orchestrator_main.db`);
        this.sqlDb = new Database(`${logPath}sql_metrics_log.db`);
        this.faissDb = new Database(`${logPath}faiss_semantic_log.db`);
        this.comparisonDb = new Database(`${logPath}comparison_log.db`);
        this.contextDb = new Database(`${logPath}context_weaving_log.db`);
        this.modelDb = new Database(`${logPath}model_selection_log.db`);
        this.dualDb = new Database(`${logPath}dual_response_log.db`);
        this.perfDb = new Database(`${logPath}performance_log.db`);
        
        // Schemas erstellen (falls noch nicht existieren)
        this.initializeTables();
    }
    
    // HAUPT-LOG ERSTELLEN
    createMainLog(sessionId, userPrompt) {
        const logId = uuidv4();
        const requestId = uuidv4();
        const promptHash = crypto.createHash('sha256').update(userPrompt).digest('hex');
        
        this.mainDb.prepare(`
            INSERT INTO orchestrator_main_log (
                log_id, session_id, request_id, user_prompt, user_prompt_hash, pipeline_status
            ) VALUES (?, ?, ?, ?, ?, 'in_progress')
        `).run(logId, sessionId, requestId, userPrompt, promptHash);
        
        console.log(`üìù Log created: ${logId}`);
        return logId;
    }
    
    // SQL-TREFFER LOGGEN (JEDEN EINZELNEN!)
    logSqlHit(logId, searchId, hit, metrics, similarity) {
        this.sqlDb.prepare(`
            INSERT INTO sql_metrics_log (
                log_id, search_id, hit_prompt_id, hit_timecode, hit_author,
                metric_A, metric_PCI, metric_hazard, /* ... alle 153 Metriken (V14 Core) Metriken ... */
                metric_cosine_similarity, text_preview, text_token_count
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).run(
            logId, 
            searchId, 
            hit.prompt_id, 
            hit.timecode, 
            hit.author,
            metrics.A,
            metrics.PCI,
            metrics.hazard,
            // ... alle 153 Metriken (V14 Core) Metriken einzeln ...
            similarity,
            hit.text.substring(0, 500),
            hit.token_count
        );
    }
    
    // FAISS-CHUNK LOGGEN (JEDEN EINZELNEN + REASSEMBLY!)
    logFaissChunk(logId, searchId, chunk, reassembledPrompt, similarity) {
        this.faissDb.prepare(`
            INSERT INTO faiss_semantic_log (
                log_id, search_id, chunk_id, chunk_text, 
                parent_prompt_id, reassembled_text, reassembled_tokens,
                cosine_similarity, rank_position
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).run(
            logId,
            searchId,
            chunk.id,
            chunk.text,
            reassembledPrompt.prompt_id,
            reassembledPrompt.full_text,
            reassembledPrompt.token_count,
            similarity,
            chunk.rank
        );
    }
    
    // VERGLEICH LOGGEN (JEDEN SQL <-> FAISS VERGLEICH!)
    logComparison(logId, sqlHit, faissHit, comparisonResult) {
        const comparisonId = uuidv4();
        
        this.comparisonDb.prepare(`
            INSERT INTO comparison_log (
                log_id, comparison_id, 
                sql_hit_prompt_id, sql_hit_text, sql_hit_score,
                faiss_hit_prompt_id, faiss_hit_text, faiss_hit_score,
                is_duplicate, metric_similarity, semantic_similarity, 
                combined_score, agreement_level, final_weight
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).run(
            logId,
            comparisonId,
            sqlHit.prompt_id,
            sqlHit.text,
            sqlHit.score,
            faissHit.prompt_id,
            faissHit.text,
            faissHit.score,
            comparisonResult.isDuplicate,
            comparisonResult.metricSimilarity,
            comparisonResult.semanticSimilarity,
            comparisonResult.combinedScore,
            comparisonResult.agreement,
            comparisonResult.weight
        );
        
        return comparisonId;
    }
    
    // CONTEXT-WEAVING LOGGEN (ALLE 5 PROMPTS PRO PAAR!)
    logContextWeaving(logId, pairNumber, hitPrompt, contextPrompts) {
        const weavingId = uuidv4();
        
        this.contextDb.prepare(`
            INSERT INTO context_weaving_log (
                log_id, weaving_id, pair_number,
                hit_prompt_id, hit_text, hit_tokens,
                prompt_minus_2_id, prompt_minus_2_text, prompt_minus_2_tokens,
                prompt_minus_1_id, prompt_minus_1_text, prompt_minus_1_tokens,
                prompt_plus_1_id, prompt_plus_1_text, prompt_plus_1_tokens,
                prompt_plus_2_id, prompt_plus_2_text, prompt_plus_2_tokens,
                set_total_tokens
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).run(
            logId, weavingId, pairNumber,
            hitPrompt.id, hitPrompt.text, hitPrompt.tokens,
            contextPrompts.minus2.id, contextPrompts.minus2.text, contextPrompts.minus2.tokens,
            contextPrompts.minus1.id, contextPrompts.minus1.text, contextPrompts.minus1.tokens,
            contextPrompts.plus1.id, contextPrompts.plus1.text, contextPrompts.plus1.tokens,
            contextPrompts.plus2.id, contextPrompts.plus2.text, contextPrompts.plus2.tokens,
            hitPrompt.tokens + contextPrompts.minus2.tokens + contextPrompts.minus1.tokens + 
            contextPrompts.plus1.tokens + contextPrompts.plus2.tokens
        );
    }
    
    // MODELL-AUSWAHL LOGGEN (MIT BEGR√úNDUNG!)
    logModelSelection(logId, selectionData) {
        const selectionId = uuidv4();
        
        this.modelDb.prepare(`
            INSERT INTO model_selection_log (
                log_id, selection_id, total_tokens,
                strategy_selected, strategy_reason,
                gpt4_fits, claude_fits, gemini_fits,
                primary_model_name, primary_model_estimated_cost,
                dual_response_cost, cost_increase_factor
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).run(
            logId, selectionId, selectionData.totalTokens,
            selectionData.strategy, selectionData.reason,
            selectionData.gpt4Fits, selectionData.claudeFits, selectionData.geminiFits,
            selectionData.primaryModel, selectionData.primaryCost,
            selectionData.dualCost, selectionData.costFactor
        );
    }
    
    // DUAL-RESPONSE LOGGEN (BEIDE KOMPLETTEN ANTWORTEN!)
    logDualResponse(logId, primaryResponse, secondaryResponse) {
        const dualId = uuidv4();
        
        this.dualDb.prepare(`
            INSERT INTO dual_response_log (
                log_id, dual_id,
                primary_model, primary_response_text, primary_response_tokens, primary_cost,
                secondary_model, secondary_response_text, secondary_response_tokens, secondary_cost,
                execution_mode, parallel_speedup_factor
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        `).run(
            logId, dualId,
            primaryResponse.model, primaryResponse.text, primaryResponse.tokens, primaryResponse.cost,
            secondaryResponse.model, secondaryResponse.text, secondaryResponse.tokens, secondaryResponse.cost,
            'PARALLEL', primaryResponse.duration / secondaryResponse.duration
        );
    }
    
    // PERFORMANCE LOGGEN (JEDER STAGE!)
    logPerformance(logId, stageName, duration, itemsProcessed) {
        this.perfDb.prepare(`
            INSERT INTO performance_log (
                log_id, stage_name, duration_ms, items_processed, items_per_second
            ) VALUES (?, ?, ?, ?, ?)
        `).run(
            logId, stageName, duration, itemsProcessed, itemsProcessed / (duration / 1000)
        );
    }
    
    // FINAL UPDATE (Pipeline abgeschlossen)
    finalizeLog(logId, totalDuration, totalCost, status) {
        this.mainDb.prepare(`
            UPDATE orchestrator_main_log 
            SET pipeline_status = ?, total_duration_ms = ?, total_cost = ?
            WHERE log_id = ?
        `).run(status, totalDuration, totalCost, logId);
        
        console.log(`‚úÖ Log finalized: ${logId} (${status}, ${totalDuration}ms, $${totalCost})`);
    }
}

module.exports = OrchestratorLogger;
```

---

### **üìä ANALYSE-M√ñGLICHKEITEN (Sp√§te Auswertung)**

#### **1. WARUM WURDE DIESE ANTWORT GENERIERT?**

```sql
-- Komplette Pipeline-Rekonstruktion f√ºr Request
SELECT 
    m.log_id,
    m.user_prompt,
    m.model_strategy,
    m.primary_model,
    m.total_cost,
    
    -- SQL-Treffer
    (SELECT COUNT(*) FROM sql_metrics_log WHERE log_id = m.log_id) as sql_hits,
    
    -- FAISS-Treffer
    (SELECT COUNT(*) FROM faiss_semantic_log WHERE log_id = m.log_id) as faiss_hits,
    
    -- Duplikate
    (SELECT COUNT(*) FROM comparison_log WHERE log_id = m.log_id AND is_duplicate = 1) as duplicates,
    
    -- Modell-Begr√ºndung
    (SELECT strategy_reason FROM model_selection_log WHERE log_id = m.log_id) as model_reason
    
FROM orchestrator_main_log m
WHERE m.log_id = 'abc123...';
```

#### **2. WELCHE METRIKEN WAREN ENTSCHEIDEND?**

```sql
-- Top 10 wichtigste Metriken f√ºr finale Auswahl
SELECT 
    s.hit_prompt_id,
    s.metric_A,
    s.metric_PCI,
    s.metric_hazard,
    s.metric_cosine_similarity,
    c.final_weight,
    c.agreement_level
FROM sql_metrics_log s
JOIN comparison_log c ON s.hit_prompt_id = c.sql_hit_prompt_id
WHERE s.log_id = 'abc123...' 
  AND c.selected_as_pair_1 = 1
ORDER BY c.final_weight DESC;
```

#### **3. PERFORMANCE-BOTTLENECKS?**

```sql
-- Langsamste Pipeline-Stages
SELECT 
    stage_name,
    AVG(duration_ms) as avg_duration,
    MAX(duration_ms) as max_duration,
    COUNT(*) as executions,
    AVG(items_per_second) as avg_throughput
FROM performance_log
GROUP BY stage_name
ORDER BY avg_duration DESC;
```

#### **4. DUAL-RESPONSE QUALIT√ÑTS-VERGLEICH?**

```sql
-- Welches Model liefert bessere Antworten?
SELECT 
    primary_model,
    secondary_model,
    AVG(primary_quality_score) as avg_primary_quality,
    AVG(secondary_quality_score) as avg_secondary_quality,
    COUNT(CASE WHEN user_preferred_response = 'PRIMARY' THEN 1 END) as user_prefers_primary,
    COUNT(CASE WHEN user_preferred_response = 'SECONDARY' THEN 1 END) as user_prefers_secondary
FROM dual_response_log
GROUP BY primary_model, secondary_model;
```

---

### **üíæ BACKUP & ARCHIVIERUNG**

#### **Auto-Backup System:**

```javascript
// backend/scripts/backup-orchestrator-logs.js

const cron = require('node-cron');
const fs = require('fs');
const path = require('path');

// T√§glich um 3 Uhr nachts
cron.schedule('0 3 * * *', () => {
    const timestamp = new Date().toISOString().split('T')[0];
    const backupDir = `backend/data/orchestrator_logs/backups/${timestamp}`;
    
    fs.mkdirSync(backupDir, { recursive: true });
    
    const logFiles = [
        'orchestrator_main.db',
        'sql_metrics_log.db',
        'faiss_semantic_log.db',
        'comparison_log.db',
        'model_selection_log.db',
        'dual_response_log.db',
        'performance_log.db'
    ];
    
    for (const file of logFiles) {
        fs.copyFileSync(
            `backend/data/orchestrator_logs/${file}`,
            `${backupDir}/${file}`
        );
    }
    
    console.log(`‚úÖ Orchestrator Logs backed up: ${backupDir}`);
});
```

---

### **üìà DASHBOARD & VISUALISIERUNG**

#### **Log-Dashboard Endpoint:**

```javascript
// backend/server.js

app.get('/api/orchestrator/analytics', async (req, res) => {
    const logger = new OrchestratorLogger();
    
    const stats = {
        totalRequests: logger.mainDb.prepare('SELECT COUNT(*) as count FROM orchestrator_main_log').get().count,
        averageDuration: logger.mainDb.prepare('SELECT AVG(total_duration_ms) as avg FROM orchestrator_main_log').get().avg,
        totalCost: logger.mainDb.prepare('SELECT SUM(total_cost) as sum FROM orchestrator_main_log').get().sum,
        
        modelUsage: logger.modelDb.prepare(`
            SELECT primary_model_name, COUNT(*) as count 
            FROM model_selection_log 
            GROUP BY primary_model_name
        `).all(),
        
        dualResponseRate: logger.modelDb.prepare(`
            SELECT 
                COUNT(CASE WHEN strategy_selected = 'DUAL_RESPONSE' THEN 1 END) * 100.0 / COUNT(*) as percentage
            FROM model_selection_log
        `).get().percentage,
        
        averagePerfectAgreements: logger.comparisonDb.prepare(`
            SELECT AVG(duplicates) as avg FROM (
                SELECT log_id, COUNT(*) as duplicates 
                FROM comparison_log 
                WHERE is_duplicate = 1 
                GROUP BY log_id
            )
        `).get().avg
    };
    
    res.json(stats);
});
```

---

## üéì **ZUKUNFTSFRAGEN - F√úR DICH ZUM LERNEN**

### **1. Was bedeutet SQLite im Frontend?**

**Einfach erkl√§rt:**
SQLite ist eine Datenbank die normalerweise auf dem Server l√§uft. Im Frontend (Browser) bedeutet es:
- Daten werden im Browser gespeichert (wie LocalStorage, aber m√§chtiger)
- Kann gro√üe Datenmengen verwalten (mehrere GB)
- Unterst√ºtzt SQL-Queries (SELECT, WHERE, JOIN)

**In unserem Fall:**
- `better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend)` und `sqlite3 (VERBOTEN im Frontend)` sind in `frontend/package.json`
- Wahrscheinlich f√ºr **Vector DBs im Browser**
- **Problem:** Sehr gro√üe Bundle-Size (mehrere MB!)
- **Frage:** Brauchen wir das wirklich? Oder nur Backend?

**Unterschied zu Backend-SQLite:**
- Backend: Datei auf Festplatte, mehrere User
- Frontend: Im Browser, nur 1 User
- Frontend-SQLite macht nur Sinn f√ºr **Offline-F√§higkeit**

**Sollten wir behalten?**
- ‚ùå **NEIN**, wenn nur Backend Vector DBs nutzt
- ‚úÖ **JA**, wenn User offline arbeiten soll

---

### **2. FAISS vs .db vs Embedding vs Vektordatenbank - WAS IST DER UNTERSCHIED?**

**Einfach erkl√§rt:**

#### **Embedding (Vektor):**
- **Was:** Eine Liste von Zahlen (z.B. [0.23, -0.45, 0.67, ...])
- **Wie:** Text ‚Üí AI-Model ‚Üí Vektor
- **Beispiel:** "Hallo Welt" ‚Üí [0.1, 0.3, -0.2, ... ] (384 Zahlen)
- **Zweck:** √Ñhnliche Texte haben √§hnliche Vektoren

#### **Vektordatenbank:**
- **Was:** Speichert viele Embeddings + kann √§hnliche finden
- **Wie:** Speichert Millionen Vektoren, findet Top-K √§hnlichste
- **Beispiel:** Gib mir 10 √§hnlichste Texte zu "Zwillinge Kindergarten"
- **Typen:** FAISS, Pinecone, Weaviate, Milvus, Chroma

#### **FAISS (Facebook AI Similarity Search):**
- **Was:** Eine spezielle Vektordatenbank von Meta/Facebook
- **Besonderheit:** SEHR schnell, nutzt CPU/GPU optimal
- **Format:** `.faiss`-Datei (bin√§r)
- **Vorteil:** Kann Millionen Vektoren in Millisekunden durchsuchen
- **Nachteil:** Nur Vektoren, keine Metadaten (Datum, Autor, etc.)

#### **.db (SQLite Database):**
- **Was:** Klassische Datenbank f√ºr strukturierte Daten
- **Format:** `.db`-Datei (SQL)
- **Inhalt:** Tabellen mit Spalten (ID, Timestamp, Text, Metrics, ...)
- **Vorteil:** Kann Metadaten speichern, komplexe Queries
- **Nachteil:** Semantic Search ist langsam (kann keine Vektoren durchsuchen)

**UNSER SYSTEM:**

```
FAISS (.faiss)                    SQLite (.db)
‚îú‚îÄ W2_384D.faiss                 ‚îú‚îÄ tempel_W_m2.db
‚îÇ  ‚îî‚îÄ 33.795 Vektoren (384D)     ‚îÇ  ‚îî‚îÄ Metadaten + Metriken
‚îú‚îÄ W5_4096D.faiss                ‚îú‚îÄ tempel_W_m5.db
‚îÇ  ‚îî‚îÄ 33.795 Vektoren (4096D)    ‚îÇ  ‚îî‚îÄ Metadaten + Metriken
```

**WORKFLOW:**
1. User fragt: "Zwillinge Kindergarten"
2. Text ‚Üí Embedding (384D Vektor)
3. FAISS sucht √§hnliche Vektoren ‚Üí Findet Top 10 Chunk-IDs
4. SQLite l√§dt Metadaten f√ºr diese Chunk-IDs ‚Üí Timestamp, Metriken, etc.
5. Kombiniert: **Semantic Search (FAISS) + Structured Data (SQLite)**

---

### **3. Metriken vs Semantik vs Metriken+Semantik - WAS MACHT SINN?**

#### **SEMANTISCHE SUCHE (nur FAISS):**
**Was:** Sucht nach **Bedeutung**, nicht nach W√∂rtern
**Beispiel:**
- Query: "Zwillinge im Kindergarten"
- Findet auch: "Geschwister in der Kita" (√§hnliche Bedeutung!)
**Vorteil:** Findet konzeptionell √§hnliche Texte
**Nachteil:** Ignoriert Daten, Emotionen, Trauma-Level

**Code:**
```python
query_vector = model.encode("Zwillinge Kindergarten")
results = faiss_index.search(query_vector, top_k=10)
```

#### **METRIKEN-SUCHE (nur SQLite):**
**Was:** Sucht nach **Zahlen** (A, PCI, Hazard, etc.)
**Beispiel:**
- Query: Finde alle Texte mit `A > 0.8` und `Hazard < 0.1`
**Vorteil:** Pr√§zise, kann Trauma-Level filtern
**Nachteil:** Findet nicht "√§hnliche" Texte, nur exakte Kriterien

**Code:**
```sql
SELECT * FROM chunks 
WHERE A > 0.8 AND hazard_score < 0.1 
ORDER BY PCI DESC LIMIT 10;
```

#### **HYBRID-SUCHE (Metriken + Semantik):**
**Was:** KOMBINIERT beide! Erst Semantik, dann Filter
**Workflow:**
1. FAISS findet Top 100 semantisch √§hnliche Chunks
2. SQLite filtert nach Metriken: `A > 0.7, Hazard < 0.2`
3. Ergebnis: Top 10 Chunks die BEIDES erf√ºllen

**Code:**
```python
# 1. Semantic Search
faiss_results = faiss_index.search(query_vector, top_k=100)

# 2. Filter by Metrics
filtered = []
for chunk_id in faiss_results:
    metrics = db.query("SELECT A, hazard FROM chunks WHERE id = ?", chunk_id)
    if metrics.A > 0.7 and metrics.hazard < 0.2:
        filtered.append(chunk_id)

# 3. Top 10
final_results = filtered[:10]
```

**UNSER SYSTEM (DualBackendBridge):**
- **FAISS:** Semantische Suche (W2 384D + W5 4096D)
- **Trinity:** Metriken-Suche (W1-W25 verschiedene Fenster)
- **A65:** Kombiniert Top 3 aus beiden ‚Üí Beste Kandidaten

**WAS MACHT SINN F√úR DICH?**

| Use Case | Empfehlung |
|----------|------------|
| "Finde √§hnliche Gespr√§che" | **Nur Semantik** (FAISS) |
| "Zeige Trauma-Phasen" | **Nur Metriken** (SQLite) |
| "Kontext-basierte Antwort" | **Hybrid** (FAISS + Metriken) ‚Üê **DAS NUTZEN WIR!** |
| "Zeitraum-Filter" | **Metriken** (Datum in SQLite) |

---

### **4. Welches LLM f√ºr welche Suche? (Hardware: GTX 3060 12GB)**

#### **DEINE HARDWARE:**
- **GPU:** NVIDIA GTX 3060 (12GB VRAM)
- **Gut f√ºr:** Lokale Embedding-Models (bis 4GB Model-Size)
- **Schlecht f√ºr:** Gro√üe LLMs (70B+ Parameter brauchen >40GB)

#### **EMPFOHLENE MODELS:**

##### **A) EMBEDDING-MODELS (f√ºr FAISS):**

| Model | Size | Dimension | Speed | Quality | F√ºr deine GPU? |
|-------|------|-----------|-------|---------|----------------|
| **all-MiniLM-L6-v2** | 80MB | 384D | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ JA (schnell!) |
| **e5-mistral-7b** | 14GB | 4096D | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è KNAPP (braucht 8GB) |
| **instructor-xl** | 5GB | 768D | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ JA |
| **gte-large** | 670MB | 1024D | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ JA |

**UNSER SYSTEM nutzt:**
- **W2:** all-MiniLM-L6-v2 (384D) ‚Üê Sehr schnell, gut genug
- **W5:** e5-mistral-7b (4096D) ‚Üê H√∂here Qualit√§t, braucht mehr RAM

**F√ºr deine Hardware:** ‚úÖ **all-MiniLM-L6-v2** ist PERFEKT (schnell + passt easy in 12GB)

##### **B) GENERATIVE LLMs (f√ºr Antworten):**

| Model | Size | Hosting | Speed | Quality | Kosten | Context |
|-------|------|---------|-------|---------|--------|---------|
| **Gemini 2.5 Flash** | Cloud | Google | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | üí∞ $0.10/1M | 1M tokens |
| **Claude Sonnet 4.5** | Cloud | Anthropic | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞ $3/1M | 200K tokens |
| **GPT-4 Turbo** | Cloud | OpenAI | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞üí∞ $10/1M | 128K tokens |
| **Llama 3.1 8B** | 16GB | Lokal | ‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ Kostenlos | 128K tokens |
| **Phi-3 Mini** | 4GB | Lokal | ‚ö°‚ö° | ‚≠ê‚≠ê | ‚úÖ Kostenlos | 128K tokens |

**UNSER SYSTEM nutzt:**
- **Prim√§r:** Gemini 2.5 Flash (Cloud) ‚Üê Schnell + g√ºnstig + 1M Context!
- **Fallback:** GPT-4 Turbo (Cloud) ‚Üê Bei Gemini-Quota

**CLAUDE SONNET 4.5 ERG√ÑNZUNG:**
- **Warum interessant?** H√∂chste Qualit√§t f√ºr komplexe Reasoning
- **Nachteil:** 30x teurer als Gemini ($3 vs $0.10 pro 1M tokens)
- **Use Case:** Nur f√ºr KRITISCHE Anfragen (Trauma-Analyse, komplexe Kontexte)
- **Integration:** Als 3. Fallback nach Gemini + GPT-4
- **API:** `https://api.anthropic.com/v1/messages`

**F√ºr deine Hardware (GTX 3060 12GB):**
- **Cloud ist besser!** (Gemini/Claude/GPT-4)
- **Lokal:** Nur Phi-3 Mini w√ºrde passen, aber schlechtere Qualit√§t

**Kosten-Vergleich (1 Million Tokens):**
```
Gemini 2.5 Flash:  $0.10  ‚Üê UNSER PRIM√ÑRES MODEL
Claude Sonnet 4.5: $3.00  ‚Üê 30x teurer, aber beste Qualit√§t
GPT-4 Turbo:       $10.00 ‚Üê 100x teurer
```

**Empfehlung f√ºr EVOKI:**
- **80% Anfragen:** Gemini 2.5 Flash (Standard)
- **15% Anfragen:** Claude Sonnet 4.5 (komplexe Trauma-Kontexte)
- **5% Anfragen:** GPT-4 Turbo (Fallback bei Quota)

---

### **5. OPTIMIERUNGS-STRATEGIE F√úR GTX 3060:**

#### **WAS DU LOKAL MACHEN KANNST:**
‚úÖ **Embeddings generieren** (all-MiniLM-L6-v2)
‚úÖ **FAISS-Suche** (CPU ist schnell genug)
‚úÖ **Metriken berechnen** (153 Metriken (V14 Core) Formeln, CPU)

#### **WAS CLOUD MACHEN SOLL:**
‚úÖ **Text-Generierung** (Gemini/GPT-4)
‚úÖ **Gro√üe Context-Fenster** (1M tokens braucht >40GB VRAM)

#### **IDEALES SETUP:**
```
GTX 3060 (Lokal):          Cloud (Google/OpenAI):
‚îú‚îÄ FAISS W2-Suche         ‚îú‚îÄ Gemini 2.5 Flash
‚îú‚îÄ Embedding-Generation   ‚îú‚îÄ Large Context (1M tokens)
‚îú‚îÄ Metriken-Berechnung    ‚îî‚îÄ High-Quality Responses
‚îî‚îÄ Trinity Vector DBs
```

**KOSTEN:**
- Gemini 2.5 Flash: ~$0.10 pro 1M tokens (sehr g√ºnstig!)
- All-MiniLM-L6-v2: Kostenlos (lokal)
- **Total pro Monat:** ~$5-20 je nach Nutzung

---

## ÔøΩ **ENTERPRISE-HARDWARE: NVIDIA 6000er+ (180GB VRAM!)**

### **DEINE VERF√úGBARE HARDWARE:**
- **Aktuell:** NVIDIA GTX 3060 (12GB VRAM) - Consumer-Level
- **Zugang:** NVIDIA 6000er Serie+ (bis 180GB VRAM!) - Enterprise-Level

**Was bedeutet 180GB VRAM?**
- **A100 80GB x2:** Dual-Setup = 160GB total
- **H100 80GB x2:** Dual-Setup = 160GB total  
- **A6000 48GB x4:** Quad-Setup = 192GB total
- **H100 SXM 80GB x2:** = 160GB total

**Das ist DATACENTER-LEVEL Hardware!** üî•

### **WAS KANNST DU DAMIT MACHEN?**

#### **1. LOKALE LLM-INFERENZ (EIGENE MODELS HOSTEN):**

| Model | Parameter | VRAM | Quality | Speed | F√ºr 180GB? |
|-------|-----------|------|---------|-------|------------|
| **Llama 3.1 70B** | 70B | 140GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | ‚úÖ JA! |
| **Mixtral 8x22B** | 176B | 176GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | ‚úÖ KNAPP! |
| **Llama 3.1 405B** | 405B | 810GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | ‚ùå Zu gro√ü |
| **Qwen 2.5 72B** | 72B | 144GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | ‚úÖ JA! |
| **Deepseek Coder 33B** | 33B | 66GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | ‚úÖ JA (viel Platz!) |

**VORTEIL LOKAL:**
- ‚úÖ Keine API-Kosten (unbegrenzte Nutzung!)
- ‚úÖ Volle Datenkontrolle (Trauma-Daten bleiben lokal!)
- ‚úÖ Keine Rate Limits
- ‚úÖ Kein Internet n√∂tig
- ‚úÖ Latenz: <1s (Cloud: 2-5s)

**NACHTEIL LOKAL:**
- ‚ùå Stromkosten (~500W pro H100 = $0.50/Stunde)
- ‚ùå Wartung, Cooling, Setup
- ‚ùå Qualit√§t etwas schlechter als Claude/GPT-4

---

#### **2. FINETUNING MIT DEINEN CHAT-DATEN:**

**Das Problem mit Cloud-APIs:**
- Gemini/Claude/GPT-4 kennen DEINE Trauma-Kontexte nicht
- Sie sind generisch trainiert
- Sie verstehen "Zwillinge Kindergarten" nicht wie DU es meinst

**L√∂sung: EIGENES MODEL TRAINIEREN!**

##### **OPTION A: PAY-AS-YOU-GO FINETUNING (Cloud):**

**GOOGLE VERTEX AI:**
- **Service:** Vertex AI Model Tuning
- **Model:** Gemini 2.5 Flash (finetunable!)
- **Daten:** Deine 33.795 Chunks als Training-Daten
- **Kosten:**
  - Training: $0.025 pro 1K tokens (~$850 f√ºr 33.795 Chunks)
  - Inference: $0.15 pro 1M tokens (1.5x teurer als Standard)
- **Vorteil:** Schnell, kein Setup, Google Infrastructure
- **Nachteil:** Daten in Google Cloud (Privacy!)

**ANTHROPIC CLAUDE FINETUNING:**
- **Service:** Claude API Fine-tuning (Beta)
- **Model:** Claude Sonnet 4.5
- **Kosten:** $5-10 pro 1K training samples (~$170-340 f√ºr 33.795 Chunks)
- **Vorteil:** Beste Qualit√§t, schnell
- **Nachteil:** Teuer, Daten bei Anthropic

**OPENAI GPT-4 FINETUNING:**
- **Service:** OpenAI Fine-tuning API
- **Model:** GPT-4 Turbo
- **Kosten:** $25 pro 1K tokens (~$850 f√ºr 33.795 Chunks)
- **Vorteil:** Standard, gut dokumentiert
- **Nachteil:** Am teuersten, Daten bei OpenAI

##### **OPTION B: LOKALES TRAINING (MIT DEINER 180GB HARDWARE!):**

**LLAMA 3.1 70B FINETUNING:**

**Hardware-Anforderungen:**
- 140GB VRAM f√ºr Inference
- **240GB+ VRAM f√ºr Training** (Optimizer States!) ‚ùå Reicht nicht!

**Aber:** Mit **LoRA** (Low-Rank Adaptation) geht's:
- LoRA braucht nur 10-20% des normalen VRAM
- **70B Model + LoRA:** ~50-80GB VRAM ‚úÖ PASST!

**Training-Setup:**
```python
from transformers import AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model

# 1. Model laden (70B)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3.1-70B",
    load_in_8bit=True,  # Quantization ‚Üí 70GB statt 140GB
    device_map="auto"
)

# 2. LoRA Config (nur 0.1% Parameter trainieren!)
lora_config = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
)
model = get_peft_model(model, lora_config)

# 3. Training
training_args = TrainingArguments(
    output_dir="./evoki_llama_70b_lora",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=your_33k_chunks,
)
trainer.train()
```

**Training-Zeit:**
- **H100 x2 (160GB):** ~12-24 Stunden f√ºr 3 Epochs
- **A100 x2 (160GB):** ~24-48 Stunden

**Kosten (Strom):**
- H100: 700W x 2 = 1400W = 1.4 kW
- 24 Stunden Training = 33.6 kWh
- Bei $0.30/kWh = **~$10 Stromkosten**

**VORTEIL LOKAL:**
- ‚úÖ Nur $10 Stromkosten (vs $850 Cloud!)
- ‚úÖ Daten bleiben lokal (Privacy!)
- ‚úÖ Unbegrenzte Experimente
- ‚úÖ Model geh√∂rt DIR (nicht Google/Anthropic)

---

#### **3. EMBEDDING-MODEL TRAINING (NOCH BESSER!):**

**Problem:**
- all-MiniLM-L6-v2 ist generisch trainiert
- Versteht "Zwillinge Kindergarten" nur als Text, nicht als Trauma-Kontext

**L√∂sung: EIGENES EMBEDDING-MODEL TRAINIEREN!**

**SENTENCE-TRANSFORMERS FINETUNING:**

```python
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

# 1. Model laden (klein genug f√ºr deine GTX 3060!)
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Training-Daten erstellen (Positive Pairs aus deinen Chunks)
train_examples = [
    InputExample(texts=['Zwillinge Kindergarten', 'Geschwister Kita'], label=1.0),
    InputExample(texts=['Trauma Phase', 'Heilung Prozess'], label=0.3),
    # ... 33.795 Chunks als Training-Pairs
]

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# 3. Loss Function (Cosine Similarity Loss)
train_loss = losses.CosineSimilarityLoss(model)

# 4. Training (auf GTX 3060 12GB!)
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=10,
    warmup_steps=100,
)

model.save('evoki_embedding_model_v1')
```

**Hardware:** ‚úÖ **GTX 3060 12GB reicht!** (Embedding-Models sind klein)

**Training-Zeit:** 2-4 Stunden auf GTX 3060

**Kosten:** ~$1 Stromkosten

**ERGEBNIS:**
- Embedding-Model das "Zwillinge Kindergarten" als Trauma-Kontext versteht
- 10-20% bessere Semantic Search Qualit√§t
- Kann direkt in FAISS verwendet werden

---

### **üéØ EMPFEHLUNG F√úR EVOKI V2.0:**

#### **PHASE 1: JETZT (mit GTX 3060 + Cloud APIs)**
```
Frontend/Backend:       ‚Üê GTX 3060 (Lokal)
‚îú‚îÄ FAISS W2-Suche      
‚îú‚îÄ Metriken-Berechnung 
‚îî‚îÄ Trinity Engines     

LLM-Generation:         ‚Üê Cloud APIs
‚îú‚îÄ 80% Gemini 2.5 Flash ($0.10/1M)
‚îú‚îÄ 15% Claude Sonnet 4.5 ($3/1M) ‚Üê F√ºr komplexe Trauma-Kontexte
‚îî‚îÄ 5% GPT-4 Turbo ($10/1M) ‚Üê Fallback
```

**Kosten:** ~$20-50/Monat

---

#### **PHASE 2: OPTIMIERUNG (mit 180GB Hardware)**
```
EMBEDDING FINETUNING:    ‚Üê GTX 3060 (4 Stunden Training)
‚îî‚îÄ all-MiniLM-L6-v2 auf deine 33.795 Chunks finetunen
   ‚Üí Bessere Semantic Search (10-20% Qualit√§t ‚Üë)

LLM weiter Cloud:
‚îî‚îÄ Gemini + Claude + GPT-4 (gleich wie Phase 1)
```

**Kosten:** ~$1 Stromkosten + ~$20-50/Monat Cloud

---

#### **PHASE 3: FULL LOCAL (mit 180GB Hardware + Privacy)**
```
ALLES LOKAL:             ‚Üê H100 x2 (180GB VRAM)
‚îú‚îÄ Llama 3.1 70B LoRA-Finetuned auf 33.795 Chunks
‚îú‚îÄ Eigenes Embedding-Model
‚îú‚îÄ FAISS W2/W5 Suche
‚îî‚îÄ Komplett offline-f√§hig!

KEINE Cloud-APIs mehr!
```

**Kosten:**
- Training: ~$10 Stromkosten (einmalig)
- Inference: ~$0.50/Stunde Stromkosten (H100 x2)
- **Bei 8h/Tag Nutzung:** ~$120/Monat Strom

**ABER:**
- ‚úÖ Unbegrenzte Nutzung (keine Token-Limits!)
- ‚úÖ Volle Privacy (Trauma-Daten bleiben lokal)
- ‚úÖ Model kennt DEINE Kontexte (finetuned)
- ‚úÖ Latenz <1s (Cloud: 2-5s)

---

### **üí∞ KOSTEN-VERGLEICH (pro Monat bei 1M Tokens/Tag):**

| Setup | Hardware | Kosten/Monat | Privacy | Qualit√§t |
|-------|----------|--------------|---------|----------|
| **Nur Cloud** | GTX 3060 | $900-3000 | ‚ùå Daten bei Google/Anthropic | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Hybrid (jetzt)** | GTX 3060 + Cloud | $20-50 | üü° Nur Antworten in Cloud | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Lokal 70B** | H100 x2 (180GB) | $120 (Strom) | ‚úÖ 100% lokal | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Lokal + Cloud** | H100 x2 + Cloud | $140 | ‚úÖ Lokal + Cloud-Fallback | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

### **üéì LERNEN: WAS IST "PAY-AS-YOU-GO" vs "TRAINING"?**

**PAY-AS-YOU-GO (Inference):**
- Du nutzt fertiges Model (Gemini/Claude/GPT-4)
- Bezahlst pro Request ($0.10-10 pro 1M tokens)
- Schnell, kein Setup
- Model bleibt generisch (kennt deine Daten nicht)

**FINETUNING (Training):**
- Du trainierst Model MIT deinen Daten
- Einmalige Kosten ($10-850)
- Model lernt DEINE Kontexte
- Danach: Inference billiger + besser

**BEISPIEL:**
```
Generisches Gemini:
User: "Erz√§hl von den Zwillingen"
Gemini: "Zwillinge sind Geschwister die..."  ‚Üê Generische Antwort

Finetuned Llama 70B:
User: "Erz√§hl von den Zwillingen"
Llama: "Im Kindergarten gab es zwei Zwillinge..."  ‚Üê Kennt DEINEN Kontext!
```

---

### **üìã N√ÑCHSTE SCHRITTE F√úR HARDWARE:**

**SOFORT (mit GTX 3060 lokal):**
1. ‚úÖ Embedding-Model finetunen (4h Training, $1 Strom)
2. ‚úÖ Claude Sonnet 4.5 als 3. API integrieren
3. ‚úÖ FAISS-Indices optimieren

**SP√ÑTER (Google Cloud VM Sessions):**
1. ‚ö° Embedding-Finetuning auf VM (2-3h, $64-96)
2. ‚ö° Mistral 7B Finetuning auf VM (4-6h, $128-192)
3. ‚ö° Models downloaden ‚Üí lokale GTX 3060 Inference
4. ‚ö° Vergleich: Finetuned lokal vs Cloud-APIs (Qualit√§t + Kosten)

---

## üî¨ **GOOGLE CLOUD VM STRATEGIE: "DAS LABOR"**

### **üí° DAS KONZEPT: Training in Cloud, Inference lokal**

**Das Problem:**
- Google Cloud VM mit 180GB VRAM kostet $32/Stunde
- 24/7 Betrieb = $23,040/Monat (VIEL ZU TEUER!)

**Die L√∂sung:**
- VM NUR f√ºr Finetuning-Sessions buchen (On-Demand)
- Trainierte Models als .pth Files downloaden
- Inference auf lokaler GTX 3060 (12GB, kostenlos!)
- VM ausschalten ‚Üí $0 laufende Kosten

---

### **üè≠ 1. DAS LABOR (Google Cloud VM - 180GB VRAM)**

**Status:** üî¥ AUS (Standard) | üü¢ AN (Nur bei Bedarf)

Da wir sie nicht dauerhaft laufen lassen k√∂nnen, nutzen wir sie als **Finetuning-Fabrik**.

#### **Job 1: Embedding-Finetuning (CRUCIAL!)**

Wir nutzen die VM f√ºr 2-3 Stunden, um das all-MiniLM-L6-v2 oder ein gr√∂√üeres e5-mistral Modell auf deine 33.795 Chunks zu trainieren.

**Ziel:** Ein .pth (Model File), das deine Sprache versteht.

**Prozess:**
1. VM starten (8x A100 80GB)
2. Dataset hochladen (chunks_v2_2.pkl)
3. Finetuning starten (2-3h)
4. Trainiertes Model downloaden (~1GB .pth)
5. VM ausschalten
6. Model auf lokale GTX 3060 deployen

**Gewinn:** Deine lokale Vektorsuche wird massiv intelligenter, ohne laufende Cloud-Kosten.

---

#### **Job 2: "The Specialist" (Mistral 7B Finetuning)**

Wir nutzen die Power der VM, um ein **Mistral 7B** Modell extrem hart auf deine Daten zu trainieren (Full Finetuning, nicht nur LoRA).

**Warum Mistral 7B?**
- Perfekt f√ºr lokale GTX 3060 (12GB VRAM)
- Quantisiert (4-bit) ‚Üí nur ~4GB RAM
- Extrem schnelle Inference (~50 tokens/s lokal)
- Nach Finetuning: √úbertrifft vanilla 70B Models bei deinen spezifischen Tasks!

**Prozess:**
1. VM starten (8x A100 80GB)
2. Dataset hochladen (33.795 Chunks als Training-Data)
3. Full Finetuning (4-6h, nicht nur LoRA!)
4. Trainiertes Model downloaden (~5GB .pth)
5. VM ausschalten
6. Model quantisieren (4-bit) ‚Üí ~2GB
7. Auf lokale GTX 3060 deployen

**Ergebnis:** Du hast ein "Mini-Evoki", das lokal auf deinem PC l√§uft, blitzschnell ist und deine Trauma-Kontexte kennt ‚Äì trainiert auf dem Google Cloud Monster-Server, ausgef√ºhrt zu Hause ohne Internet-Abh√§ngigkeit.

---

### **üíª 2. DAS FELD (Dein PC - GTX 3060 12GB)**

**Status:** üü¢ IMMER AN

Das ist dein Daily Driver. Hier l√§uft alles nach dem Training.

#### **Aufgabe 1: Vektor-Datenbank (FAISS)**
L√§uft lokal mit dem (auf der VM trainierten) Embedding-Modell.
- all-MiniLM-L6-v2 (finetuned) ‚Üí 384D Embeddings
- 33.795 Chunks in RAM (~2GB)
- Blitzschnelle Suche (<100ms)

#### **Aufgabe 2: Metriken & Orchestrator**
Berechnet A, PCI, Hazard lokal.
- Trinity Engines (Node.js)
- 153 Metriken (V14 Core) Metriken pro Prompt
- SQL Vector DBs (W1-W25)

#### **Aufgabe 3: Inference (Alltag)**

**Option A: Cloud-APIs (aktuell)**
- Gemini 2.5 Flash f√ºr gro√üe Kontexte (1M tokens, $0.10/1M)
- GPT-4 Turbo f√ºr Best Quality (<128K, $10/1M)
- Claude Sonnet 4.5 f√ºr Trauma-Analysis (<200K, $3/1M)

**Option B: Lokales Mistral 7B (nach Finetuning)**
- L√§uft auf GTX 3060 (4GB VRAM genutzt)
- Kostenlos, keine Internet-Abh√§ngigkeit
- ~50 tokens/s (schneller als Cloud!)
- Kennt DEINE Kontexte (finetuned)

**Option C: Hybrid (Best of Both Worlds)**
- Einfache/private Fragen ‚Üí Mistral 7B lokal
- Komplexe/lange Kontexte ‚Üí Gemini Cloud
- Kritische Trauma-Analyse ‚Üí Claude Cloud

---

### **üí∞ COST-BREAKDOWN:**

```
Google Cloud VM (8x A100 80GB = 640GB VRAM total): ~$32/h
‚îú‚îÄ Embedding-Finetuning: 2-3h √ó $32 = $64-96
‚îú‚îÄ Mistral 7B Finetuning: 4-6h √ó $32 = $128-192
‚îî‚îÄ Total: $192-288 (EINMALIG!)

Dann: VM AUSSCHALTEN, Models lokal nutzen ‚Üí $0 laufende Kosten!

Vergleich zu Dauerbetrieb:
‚îú‚îÄ VM 24/7 f√ºr 1 Monat: 720h √ó $32 = $23,040
‚îú‚îÄ Unsere "Labor"-Strategie: $192-288 einmalig ‚Üí 99% g√ºnstiger!
‚îî‚îÄ Lokale Inference danach: GTX 3060 12GB (bereits vorhanden)

Wichtig: VM wird NUR f√ºr Finetuning-Sessions gebucht (On-Demand)!
```

---

### **üìä QUALIT√ÑTS-VERGLEICH (nach Finetuning):**

| Szenario | Model | Tokens | Kosten | Qualit√§t | Latenz |
|----------|-------|--------|--------|----------|--------|
| **Kurze Frage** | Mistral 7B (lokal) | 2k | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê | <1s |
| **Mittlere Frage** | Mistral 7B (lokal) | 10k | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê | 2s |
| **Lange Frage** | Gemini Flash (Cloud) | 80k | $0.008 | ‚≠ê‚≠ê‚≠ê | 3-5s |
| **Trauma-Analyse** | Claude (Cloud) | 150k | $0.45 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 5-8s |
| **Sehr lange** | Gemini Flash (Cloud) | 500k | $0.05 | ‚≠ê‚≠ê‚≠ê | 8-15s |

**Nach Finetuning:**
- Mistral 7B (lokal) kennt deine Kontexte ‚Üí Qualit√§t ‚≠ê‚≠ê‚≠ê‚≠ê (statt ‚≠ê‚≠ê)
- 70-80% der Fragen k√∂nnen lokal beantwortet werden
- Nur noch 20-30% brauchen Cloud-APIs
- Kosten sinken von $900/Monat auf $50-100/Monat!

---

### **üöÄ DEPLOYMENT NACH FINETUNING:**

**1. Mistral 7B lokal hosten:**
```bash
# Quantisieren (4-bit)
python -m llama_cpp.convert --model mistral-7b-evoki-finetuned.pth --outfile mistral-7b-evoki-q4.gguf

# Starten mit llama.cpp
./llama.cpp/main -m mistral-7b-evoki-q4.gguf --port 8080 --ctx-size 32768
```

**2. Backend anbinden:**
```javascript
// backend/core/LocalLLMBridge.js
const response = await fetch('http://localhost:8080/v1/completions', {
    method: 'POST',
    body: JSON.stringify({
        prompt: contextText,
        max_tokens: 2048,
        temperature: 0.7
    })
});
```

**3. Intelligente Model-Auswahl:**
```javascript
if (totalTokens < 30000 && !requiresDeepAnalysis) {
    model = 'mistral-7b-local'; // Kostenlos, schnell
} else if (totalTokens < 200000) {
    model = 'claude-sonnet-4.5'; // Best Trauma-Analysis
} else {
    model = 'gemini-2.5-flash'; // Large Context
}
```

---

## ÔøΩüìö **REFERENZEN**

- **Haupt-README:** `README.md` (mit Synapse Genesis Point)
- **Architektur:** `ARCHITECTURE.json` (auto-generiert)
- **Setup:** `SETUP.md`
- **Cleanup Report:** `docs/CLEANUP_REPORT.md`
- **V1 Reference:** `c:\evoki\` (Produktiv-System)

---

**Letztes Update:** 29.12.2025 - Kombinierte Tiefenanalyse & Action-Roadmap ‚ö°  
**Discovery Phase:** 5/5 - Schwachstellen identifiziert, L√∂sungsroadmap erstellt  
**N√§chste Review:** Nach Umsetzung der Top-5 Kritischen Fixes

---

# üö® **KOMBINIERTE TIEFENANALYSE & ACTION-ROADMAP**

*Basierend auf systematischer Code-Review und Architektur-Analyse*

## üìã **EXECUTIVE SUMMARY**

**Status:** WHITEBOARD_V2 ist aktuell eine **"Rohfusion"** (Original + Adler) mit solider Grundarchitektur, aber **kritischen Implementierungsl√ºcken** und **strukturellen Inkonsistenzen**.

**Hauptprobleme:**
- üî¥ **Build-Stopper:** Native SQLite Module crashen Vite
- üü† **Spezifikations-Chaos:** Widerspr√ºchliche ‚úÖ/‚ùå Status-Angaben
- üü° **Produktions-Fallen:** SSE ohne Cancel-Safety, Health Check killt Backend
- üü¢ **Performance-Verschwendung:** Overengineering f√ºr 70% der Standard-Anfragen

---

## üéØ **PRIORISIERTE ACTION-LISTE**

### **üö® PHASE 1: KRITISCHE FIXES (Build-Stopper & Produktions-Killer)**

#### **1.1 SOFORT-KRITISCH (< 1 Tag)**

**‚ùå P0 - SQLite Frontend Crash-Fix**
```bash
cd frontend
npm uninstall better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend) sqlite3 (VERBOTEN im Frontend)
# ‚ö†Ô∏è OHNE DIESEN FIX: Vite Build crasht bei Import!
```
**Impact:** üî¥ HOCH | **Effort:** 5min | **Risiko:** System unbenutzbar

**‚ùå P0 - Health Check Safety**
```javascript
// ‚ùå AKTUELL: AbortSignal.timeout() sends SIGINT to backend!
// ‚úÖ FIX: Separater, safe Ping ohne globalen Abort
const healthCheck = await fetch('/api/v1/health', {
    signal: AbortSignal.timeout(3000), // NICHT der globale AbortController!
    method: 'GET'
});
```
**Impact:** üî¥ HOCH | **Effort:** 30min | **Risiko:** Backend stirbt bei Health Check

#### **1.2 SPEZIFIKATIONS-KONSISTENZ (1-2 Tage)**

**‚ùå P1 - Endpoint Truth Table**
```markdown
# MASTER ENDPOINT STATUS (Single Source of Truth)
‚úÖ IMPLEMENTIERT:
- GET /health ‚Üí Backend Health
- POST /api/bridge/process ‚Üí HAUPT-PIPELINE
- GET /api/v1/status ‚Üí Enhanced Status

‚ùå FEHLT (Implementierung erforderlich):
- GET /api/pipeline/logs ‚Üí Pipeline Log Entries
- GET /api/v1/system/errors ‚Üí Error Persistence
- GET /api/v1/trialog/session ‚Üí Trialog Session
```
**Alle anderen Abschnitte referenzieren NUR hierhin!**
**Impact:** üü† MITTEL | **Effort:** 2h | **Risiko:** Entwickler-Verwirrung

**‚ùå P1 - Token-Mode Naming Fix**
```typescript
// ‚ùå AKTUELL: "Standard" < "Quick" (verwirrend)
Quick: 25k
Standard: 20k  // Kleiner als Quick?!

// ‚úÖ FIX: Logische Reihenfolge
Compact: 20k   // Minimal, schnell
Standard: 50k  // Normale Nutzung
Unlimited: 1M  // Gro√üe Kontexte
```
**Impact:** üü° NIEDRIG | **Effort:** 15min | **Risiko:** UX-Verwirrung

**‚ùå P1 - Doppelte Passagen eliminieren**
- "TIMEOUT-PROBLEM #1" steht 2x identisch
- SSE Code-Bl√∂cke mehrfach vorhanden
- **L√∂sung:** Zentraler Abschnitt + Cross-Referenzen
**Impact:** üü° NIEDRIG | **Effort:** 1h | **Risiko:** Divergenz bei Updates

---

### **‚ö° PHASE 2: PRODUKTIONS-ROBUSTHEIT (3-5 Tage)**

#### **2.1 SSE CANCEL-SAFETY**
```javascript
// ‚úÖ REQUIRED: Cancel-Safety √ºberall
const abortController = new AbortController();

// Client disconnect ‚Üí Backend MUSS stoppen
req.on('close', () => {
    abortController.abort();
    // Gemini/FAISS/DB Calls auch canceln!
});

// Heartbeat gegen Proxy-Timeouts (alle 15s)
setInterval(() => {
    res.write('data: {"heartbeat": true}\n\n');
}, 15000);
```
**Impact:** üî¥ HOCH | **Effort:** 1 Tag | **Risiko:** Zombie-Requests, Resource-Leaks

#### **2.2 FAISS vs SQL TRUTH DEFINITION**
```javascript
// ‚úÖ REGEL: SQL Source DB ist "Text-Truth" (wenn vorhanden)
// Chunk-Reassembly nur Fallback + Hash-Check
if (sqlText && faissReassembled) {
    const sqlHash = sha256(sqlText);
    const faissHash = sha256(faissReassembled);
    
    if (sqlHash !== faissHash) {
        console.warn(`‚ö†Ô∏è Text Divergence: SQL vs FAISS different!`);
        return sqlText; // SQL wins!
    }
}
```
**Impact:** üü† MITTEL | **Effort:** 4h | **Risiko:** Inkonsistente Datenquellen

#### **2.3 MATHEMATISCHE NORMALISIERUNG**
```javascript
// ‚ùå AKTUELL: A = 0.5 + (Pos - Neg) - T_panic  // Kann < 0 werden!
// ‚úÖ FIX: Normalisierung erforderlich
A = Math.max(0, Math.min(1, 0.5 + (Pos - Neg) - T_panic));
```
**Impact:** üü° NIEDRIG | **Effort:** 2h | **Risiko:** Invalid Metrik-Werte

---

### **üöÄ PHASE 3: PERFORMANCE & QUALIT√ÑT (1-2 Wochen)**

#### **3.1 INTELLIGENT MODEL ROUTING**
```javascript
// ‚úÖ Threshold-basierte Auswahl statt Always-Cloud
if (totalTokens < 10000 && !requiresDeepAnalysis) {
    model = 'mistral-7b-local';     // Kostenlos, GTX 3060
} else if (totalTokens < 200000) {
    model = 'claude-sonnet-4.5';    // $3/1M, beste Qualit√§t
} else {
    model = 'gemini-2.5-flash';     // $0.1/1M, 1M Context
}
```
**Impact:** üü¢ HOCH | **Effort:** 3 Tage | **ROI:** 60-80% Kosteneinsparung

#### **3.2 EMBEDDING FINETUNING**
```python
# ‚úÖ GTX 3060 kann Embedding-Models trainieren (2-4h, $1 Strom)
model = SentenceTransformer('all-MiniLM-L6-v2')
model.fit(train_data_33k_chunks, epochs=10)
# Ergebnis: 15-25% bessere Semantic Search
```
**Impact:** üü¢ HOCH | **Effort:** 1 Tag | **ROI:** Deutlich bessere Suche

#### **3.3 PIPELINE-VEREINFACHUNG**
```javascript
// ‚úÖ Adaptive Komplexit√§t
if (isSimpleQuery(userPrompt)) {
    // Simple Mode: User ‚Üí FAISS ‚Üí Gemini (3 Steps)
    return simpleRAGPipeline(userPrompt);
} else {
    // Complex Mode: User ‚Üí Full Orchestrator (12 Steps)
    return fullOrchestratorPipeline(userPrompt);
}
```
**Impact:** üü¢ MITTEL | **Effort:** 2 Tage | **ROI:** 50% weniger Latenz f√ºr Standard-Anfragen

---

### **üîß PHASE 4: ADVANCED FEATURES (Optional)**

#### **4.1 SENTINEL KALIBRIERUNG**
```javascript
// ‚úÖ Statt Fantasie-Zahlen (0.75, 0.3, 0.6):
// Lerne Thresholds aus User-Feedback + Session-Outcomes
const sentinelThresholds = await calibrateFromHistory(userFeedbackDB);
```
**Impact:** üü¢ MITTEL | **Effort:** 1 Woche | **ROI:** Adaptive Sicherheit

#### **4.2 DUAL-RESPONSE UX-LOGIK**
```typescript
// ‚úÖ Klare Entscheidungslogik f√ºr 2 Antworten
interface DualResponse {
    primary: Response;    // "Offizielle" Antwort (in Vector DB)
    secondary: Response;  // Vergleichs-Antwort (nur Display)
    explanation: string;  // Warum 2 Antworten?
    userChoice?: 'primary' | 'secondary'; // Feedback
}
```
**Impact:** üü¢ NIEDRIG | **Effort:** 3 Tage | **ROI:** Bessere UX bei Dual-Mode

---

## üìä **IMPACT-MATRIX**

| Fix | Kritikalit√§t | Effort | ROI | Abh√§ngigkeiten |
|-----|--------------|--------|-----|----------------|
| **SQLite Frontend** | üî¥ KRITISCH | 5min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Keine |
| **Health Check Safety** | üî¥ KRITISCH | 30min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Keine |
| **Endpoint Truth Table** | üü† HOCH | 2h | ‚≠ê‚≠ê‚≠ê‚≠ê | Keine |
| **SSE Cancel-Safety** | üü† HOCH | 1 Tag | ‚≠ê‚≠ê‚≠ê‚≠ê | Backend Refactor |
| **FAISS Truth Source** | üü† MITTEL | 4h | ‚≠ê‚≠ê‚≠ê | DB Schema |
| **Model Routing** | üü¢ ENHANCEMENT | 3 Tage | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Local Model Setup |
| **Embedding Finetuning** | üü¢ ENHANCEMENT | 1 Tag | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | GTX 3060 |

---

## üéØ **EMPFOHLENE SPRINT-AUFTEILUNG**

### **Sprint 1 (2-3 Tage): "Build-Stabilit√§t"**
- ‚úÖ SQLite Frontend Fix
- ‚úÖ Health Check Safety
- ‚úÖ Endpoint Truth Table
- ‚úÖ Token-Mode Naming

### **Sprint 2 (1 Woche): "Produktions-Robustheit"**
- ‚úÖ SSE Cancel-Safety
- ‚úÖ FAISS Truth Source
- ‚úÖ Mathematische Normalisierung
- ‚úÖ Doppelte Passagen eliminieren

### **Sprint 3 (1-2 Wochen): "Performance & Intelligence"**
- ‚úÖ Intelligent Model Routing
- ‚úÖ Embedding Finetuning
- ‚úÖ Pipeline-Vereinfachung

### **Sprint 4+ (Optional): "Advanced Features"**
- ‚úÖ Sentinel Kalibrierung
- ‚úÖ Dual-Response UX
- ‚úÖ Replay Mode
- ‚úÖ Golden Set Validation

---

## üîç **VALIDIERUNGSKRITERIEN**

**Nach Sprint 1:**
- [ ] Vite Build l√§uft ohne Crash
- [ ] Health Check killt Backend nicht
- [ ] Keine widerspr√ºchlichen Endpoint-Status

**Nach Sprint 2:**
- [ ] SSE Streams sind cancel-safe
- [ ] Text-Inkonsistenzen zwischen FAISS/SQL detektiert
- [ ] Alle Metriken im g√ºltigen 0.0-1.0 Bereich

**Nach Sprint 3:**
- [ ] 70% der Anfragen nutzen kosteneffiziente Models
- [ ] Semantic Search 15%+ besser nach Finetuning
- [ ] Standard-Anfragen <2s Latenz

---

## üí° **LANGFRIST-VISION**

**Ziel:** Aus der aktuellen **"Rohfusion"** wird eine **einheitliche, produktions-taugliche Spezifikation** mit:

- üîπ **Single Source of Truth** f√ºr alle Komponenten-Status
- üîπ **Cancel-Safety** in allen async Operationen
- üîπ **Adaptive Intelligence** (einfach ‚Üí schnell, komplex ‚Üí qualitativ)
- üîπ **Self-Calibrating Security** (Sentinel lernt aus Feedback)
- üîπ **Kosteneffiziente Hybrid-Architektur** (lokal + Cloud optimal gemischt)

**Messbare Ziele:**
- ‚úÖ 0 Build-Crashes
- ‚úÖ <2s Latenz f√ºr 70% der Anfragen
- ‚úÖ 60-80% Kosteneinsparung durch intelligente Model-Auswahl
- ‚úÖ 15-25% bessere Semantic Search durch Finetuning
- ‚úÖ 99.9% Uptime ohne Resource-Leaks

---

# === ADLER-ERG√ÑNZUNG (VOLLST√ÑNDIG INTEGRIERT ‚Äì ROHVERSION) ===

Adler hier. ü¶ÖDu hast vollkommen recht. Die blo√üe Einf√ºgung des Blocks in die Mitte war passiv. Eine echte Integration bedeutet, dass die 153 Metriken die L√∂sungen f√ºr die im Whiteboard beschriebenen Probleme sind.Ich habe das Whiteboard tiefenanalysiert und die Metriken an den entscheidenden Stellen explizit verkn√ºpft.Hier sind die konkreten Punkte, die ich im Text angereichert habe, bevor ich dir das volle Dokument gebe:Context-Drift (Blind Spot): Hier greifen jetzt Time_Decay_Factor (M114) und G_phase (M52) als mathematische L√∂sung.Trauma-Warnung (Potenzial): Die "Ableitung der Kurve" wurde durch grad_PCI (M32) und nabla_delta_A (M33 - Absturz-Beschleunigung) konkretisiert.Metaphern-Synthese: Basiert jetzt auf H_conv (Konvergenz) und EV_consensus.Tempel V3: Die "153 Metriken (V14 Core) Metriken" wurden auf "153 (inkl. OMEGA)" korrigiert.Orchestrator (SQL Suche): Der Such-Vektor ist nicht mehr abstrakt, sondern definiert als [A, PCI, z_prox, E_trapped].Sentinel (Dissoziation): Die Erkennung nutzt jetzt T_fog (Tr√ºbung) und LL (Logic Loss) statt nur Hazard.Hier ist das vollst√§ndige, intelligent vernetzte Whiteboard:üåå EVOKI V2.0 - WHITEBOARD (Ideensammlung)Datum: 28. Dezember 2025Status: Entwicklungs-Discovery & Architektur-MappingZweck: Keine To-Do-Liste, nur Ideensammlung und Erkenntnisseüîç ARCHITEKTUR-BLIND SPOTS & FUTURE VISION1. Identifizierte Blind Spots und versteckte ProblembereicheTrotz der Korrekturen in V3 gibt es architektonische "blinde Flecken", die bei fortschreitender Nutzung kritisch werden:Das "Context-Drift" Paradoxon: Das System webt Kontext aus ¬±2 Prompts um einen Treffer. Blind Spot: Wenn die Historie auf √ºber 100.000 Chunks anw√§chst, k√∂nnten die "Metrik-Zwillinge" (SQL-Treffer) aus v√∂llig unterschiedlichen Lebensphasen stammen. Der Orchestrator braucht eine Time Decay Funktion, die verhindert, dass uralte Metriken die aktuelle Analyse "vergiften".V14 L√∂sung: Implementierung von Time_Decay_Factor (M114) zur Abwertung alter Vektoren und G_phase (M52) zur Bestimmung der aktuellen Gravitation eines Themas.LocalStorage als "Flaschenhals-Sackgasse": Die Quellen warnen vor dem 4MB-Limit. Blind Spot: Selbst beim Ausweichen auf Backend-Logs bleibt der React-State der Single-Point-of-Failure. Bei 1M Tokens friert das UI ein. L√∂sung: Virtualisierung (react-window) und Partial State Updates sind zwingend.Die "Finetuning-Echokammer": Die "Labor-Strategie" sieht vor, Modelle mit den eigenen Chunks zu trainieren. Risiko: Wenn wir auf halluzinierten V1-Daten trainieren, zementieren wir Fehler. Wir brauchen ein "Golden Set" (verifizierte Chunks) f√ºr das Training.Sentinel-Veto vs. LLM-Konfidenz: Der Sentinel kann Scores massiv senken. Blind Spot: Wenn alle Top-Kandidaten blockiert werden, sendet das System "Restm√ºll". Wir brauchen einen Emergency Refetch, der bei Veto sofort neue, sicherere Parameter sucht.V14 L√∂sung: Der Sentinel nutzt z_prox (M24) als prim√§ren Trigger. Bei z_prox > 0.8 wird der Emergency Refetch ausgel√∂st und auf Safety_Lock_Status (M150) gepr√ºft.2. Ungenutztes Potenzial der ArchitekturPr√§diktive Trauma-Warnung (Early Warning): Da wir jetzt 153 Metriken live haben, k√∂nnen wir mehr als nur den Ist-Zustand messen. Wir berechnen die Ableitung der PCI-Kurve (grad_PCI, M32) und die Beschleunigung des Absturzes (nabla_delta_A, M33). Steigt die negative Beschleunigung √ºber 3 Sessions? Warnung VOR dem Crash.Automatisierte Metaphern-Synthese: "Perfect Agreements" zwischen Metrik und Semantik (H_conv > 0.9 und EV_consensus > 0.8) k√∂nnen genutzt werden, um individuelle therapeutische Metaphern zu generieren.Trialog als Architektur-Optimierer: Der Analyst-Agent k√∂nnte die performance_log.db lesen und selbstst√§ndig Indizes rebalancen ("Self-Optimizing Architecture"), basierend auf System_Entropy (M152).3. Vision√§re ErweiterungenSovereign Personal AI: Durch die Kombination von "Labor-Strategie" (Cloud-Training) und lokaler Inference (GTX 3060) wird Evoki zur Black Box f√ºr das Ich ‚Äì 100% offline, 100% privat, Cloud-Qualit√§t.Cross-Session Chronicle: Weg vom Append-Only Log hin zu einer dynamischen Wissenskarte, die Cluster im Deep Storage visualisiert.üß† V14 NEURO-CORE SPEZIFIKATION (Das 153-Metriken-Spektrum)Status: Implementiert als evoki_v7_hybrid_core.py (Math Monolith)Zweck: Ersetzung von "Gef√ºhl" durch deterministische Mathematik.Das System analysiert jeden Input (und dessen Kontext) nun auf folgenden 10 Ebenen der Wahrnehmung:1. Die Lexikalischen Basis-Werte (21 Metriken)Die Rohdaten der Wahrnehmung basierend auf V2.2 Lexika.LEX_S_self (Selbstreferenz), LEX_X_exist (Existenzielle Themen), LEX_B_past (Vergangenheitsbezug)LEX_Lambda_depth (Reflexionstiefe), LEX_T_panic (Akute Panik), LEX_T_disso (Dissoziation)LEX_T_integ (Integration/Heilung), LEX_T_shock (Schockzustand)LEX_Suicide (Suizidalit√§t - Kritisch), LEX_Self_harm (Selbstverletzung), LEX_Crisis (Allgemeine Krise)LEX_Help (Hilferuf), LEX_Emotion_pos (Positive Emotion), LEX_Emotion_neg (Negative Emotion)LEX_Kastasis_intent (Hypothetisches Denken), LEX_Flow_pos (Zustimmung), LEX_Flow_neg (Ablehnung)LEX_Coh_conn (Logische Verkn√ºpfer), LEX_B_empathy (Empathie), LEX_Amnesie (Ged√§chtnisl√ºcken)LEX_ZLF_Loop (Wiederholungsschleifen)2. Die Neuro-Physik / Core Metrics (25 Metriken)Die physikalischen Gesetze des Geistes (V3.0 Logic).A (Affekt): 0.5 + (Pos - Neg) - T_panic. (0.0 = T√∂dlich, 1.0 = Erleuchtet)PCI (Prozess-Koh√§renz): Wie klar ist der Gedanke?z_prox (W√§chter): (1.0 - A) * Max(Hazard). Wahrscheinlichkeit eines Sicherheitsvorfalls.T_fog (Tr√ºbung): Wie stark ist die Wahrnehmung durch Trauma verzerrt?E_trapped: Ma√ü f√ºr Depression/Angst-Stau.E_available: Verf√ºgbare Ressource f√ºr Ver√§nderung.S_entropy: Informationsdichte des Textes.LL (Logic Loss): Wahrscheinlichkeit von Halluzination/Realit√§tsverlust.ZLF (Zero Latent Factor): Leere Phrasen ohne Inhalt.Deltas: grad_A, grad_PCI, nabla_delta_A (Beschleunigung des Absturzes).Status: Homeostasis_Pressure, Reality_Check, Risk_Acute, Risk_Chronic, Stability_Index.Load: Cognitive_Load, Emotional_Load, Intervention_Need.Drive: Constructive_Drive, Destructive_Drive, Ambivalence, Clarity, Resilience_Factor.3. HyperPhysics (20 Metriken)Beziehungs-Dynamik & Raum.H_conv (Konvergenz/Jaccard), nablaA_dyad (Affekt-Divergenz), deltaG (Reibung).EV_consensus (Einigung), T_balance (Trauma-Balance), G_phase (Gravitation).cos_day_centroid (Tages-Thema), torus_dist (Zyklische Wiederholung).Soul_Integrity, Rule_Stable, Vkon_mag, V_Ea_effect.Session_Depth, Interaction_Speed, Trust_Score, Rapport.Mirroring, Pacing, Leading, Focus_Stability.4. Free Energy Principle / FEP (15 Metriken)Minimierung von √úberraschung (V14 Exklusiv).FE_proxy (Ann√§herung Freie Energie), Surprisal, Phi_Score (Handlungsf√§higkeit).U (Utility), R (Risk), Policy_Confidence (Sicherheit).Exploration_Bonus, Exploitation_Bias.Model_Evidence, Prediction_Error, Variational_Density.Markov_Blanket_Integrity, Active_Inference_Loop, Goal_Alignment, Epistemic_Value.5. Kausale Granularit√§t / Grain (14 Metriken)Suche nach dem Ausl√∂ser ("Find the Grain").Grain_Word_ID, Grain_Impact_Score, Grain_Sentiment, Grain_Category.Grain_Novelty, Grain_Recurrence, Trigger_Map_Delta, Causal_Link_Strength.Context_Binding, Negation_Flag, Intensifier_Flag.Subject_Reference, Object_Reference, Temporal_Reference.6. Konversationelle Dynamik & Linguistik (15 Metriken)Struktur und Muster.Turn_Length_User, Turn_Length_AI, Talk_Ratio.Question_Density, Imperative_Count, Passive_Voice_Ratio.Vocabulary_Richness, Complexity_Index (LIX), Coherence_Local, Coherence_Global.Repetition_Count, Fragment_Ratio, Capitalization_Stress, Punctuation_Stress, Emoji_Sentiment.7. Chronos & Zeit-Vektoren (12 Metriken)Die vierte Dimension.Time_Since_Last_Interaction, Session_Duration, Interaction_Frequency.Time_Decay_Factor, Future_Orientation, Past_Orientation, Present_Focus.Chronological_Order_Check, Circadian_Phase.Response_Time_Engine, Process_Time_Safety, Process_Time_RAG.8. Metakognition & Simulation (13 Metriken)Das Denken √ºber das Denken (A65 Strategy).Simulation_Depth, Trajectory_Optimism, Trajectory_Stability.Scenario_Count, Chosen_Path_ID, Rejected_Path_Risk.Confidence_Score, Ambiguity_Detected, Clarification_Need.Self_Correction_Flag, Model_Temperature.System_Prompt_Adherence, Goal_Alignment.9. System-Gesundheit & RAG (10 Metriken)Die Maschine im Hintergrund.Vector_DB_Health, RAG_Relevance_Score, RAG_Density, RAG_Diversity.Hallucination_Risk, Memory_Pressure, Token_Budget_Remaining.Cache_Hit_Rate, Network_Latency, Error_Rate_Session.10. Die OMEGA-Metriken (8 Metriken)Die ultimativen Zusammenfassungen f√ºr Entscheidungen.OMEGA: (PCI * A) / max(0.1, (Trauma + Gefahr)) - Der finale Entscheidungswert.Global_System_Load, Alignment_Score (B-Align).Evolution_Index, Therapeutic_Bond, Safety_Lock_Status.Human_Intervention_Req, System_Entropy.üìç FRONTEND KOMPONENTEN - AKTUELLER STATUS‚úÖ EVOKI TEMPEL V3 - HYPERSPACE EDITION (Produktiv)Datei: frontend/src/components/EvokiTempleChat.tsxVersion: V3 - Hyperspace EditionStatus: ‚úÖ AKTIV - Das ist der ECHTE Evoki TempelFeatures:12-Database Distribuierte SpeicherungToken-Limits: 25k (quick), 20k (standard), 1M (max)SHA256 Chain-Logik mit kontinuierlicher ListeMetriken-Berechnung auf alle DBs: Nutzt calculate_153_metrics aus V14 Core.A65 Multi-Candidate Selection: Basiert auf Trajectory_Optimism (M124) und Phi_Score (M69).Phase 4 Token Distribution:32% Narrative Context (8.000 Tokens)12% Top-3 Chunks (3.000 Tokens)20% Overlapping Reserve (5.000 Tokens)4% RAG Chunks (1.000 Tokens)32% Response Generation (8.000 Tokens)Backend Endpoint: /api/bridge/processVektorisierung: Live mit allen 153 Metriken (fr√ºher 153 Metriken (V14 Core)).‚ö†Ô∏è CHATBOT PANEL (Legacy aus V1)Datei: frontend/src/components/ChatbotPanel.tsxVersion: V1 - Generischer ChatbotStatus: üü° OBSOLET - War der erste generische Google-ChatbotHistorie:Urspr√ºnglich: Generische Google API InteraktionDann: Erster "Tempel"-√§hnlicher Anschluss (aus Respekt zu Evoki nicht so genannt)Jetzt: Durch EvokiTempleChat V3 ersetztBackend Endpoint: /api/bridge/process (gleicher wie V3, aber weniger Features)Unterschied zu V3:Keine 12-DB DistributionKeine Phase 4 Token DistributionKeine Tempel-Metriken (fehlt OMEGA, z_prox)Keine SHA256 ChainKein A65 Multi-CandidateIdee: K√∂nnte entfernt oder als "Simple Chat Mode" behalten werdenüîç PIPELINE-√úBERWACHUNG‚úÖ PIPELINE LOG PANEL (Implementiert)Datei: frontend/src/components/PipelineLogPanel.tsxStatus: ‚úÖ VORHANDEN als Tab 12Zweck: Trackt ALLE √úbergabepunkte f√ºr Fehlerdiagnose12 Protokollierte Schritte:User Input ‚Üí FrontendFrontend ‚Üí Backend (/api/bridge/process)Backend ‚Üí Python FastAPI Service (POST localhost:8000/search) ‚ö†Ô∏è NICHT CLI-Spawn!Python FAISS ‚Üí JSON Output (Enth√§lt Grain_Word_ID M82)Backend Parse ‚Üí DualBackendBridgeDualBackendBridge ‚Üí Trinity Engines (Berechnet FE_proxy M67)Trinity Results ‚Üí A65 Candidate Selection (Vergleich U vs R)A65 ‚Üí GeminiContextBridgeContext Building ‚Üí Gemini PromptGemini API Call ‚Üí ResponseResponse ‚Üí Vector Storage (12 DBs)Final Response ‚Üí Frontend (Zeigt OMEGA Score)üîß IMPLEMENTATION NOTE:Legacy-Konzept: spawn(pythonPath, ['query.py', prompt]) (2-5s Modell-Ladezeit pro Request)Production-Reality: Persistenter FastAPI Microservice (Port 8000)L√§dt sentence-transformers + FAISS einmal beim Systemstart (30s)Requests: POST http://localhost:8000/search (<100ms pro Request)Endpoints: /search, /health, /reload-indexGrund: CLI-Spawn w√ºrde FAISS bei jedem Request neu laden ‚Üí Timeout-H√∂lle‚ùå BACKEND ENDPOINT FEHLTErwartet: GET /api/pipeline/logsStatus: ‚ùå NICHT IMPLEMENTIERT in backend/server.jsFrontend Code: Line 128 in PipelineLogPanel.tsx ruft es aufIdee: Backend muss Pipeline-Logs persistieren (JSONL-File oder SQLite)Daten-Struktur:TypeScriptinterface PipelineLogEntry {
  id: string;
  timestamp: string;
  session_id: string;
  message_id: string;
  step_number: number; // 1-12
  step_name: string;
  metrics_snapshot: { // NEU: V14 Integration
      A: number;
      PCI: number;
      OMEGA: number;
  };
  data_transfer: {
    from: string;
    to: string;
    text_preview: string; // Erste 200 Zeichen
    full_text: string;
    size_bytes: number;
    token_count?: number;
  };
  metadata?: Record<string, any>;
}
Zweck: Mikro-Tuning wenn Google API unpasende Antworten liefertUse Case: Fehlerquelle direkt identifizieren (FAISS? Trinity? Gemini?)üîê GENESIS ANCHOR (A51)‚úÖ IMPLEMENTIERT ABER DEAKTIVIERTDatei: backend/server.js Line 26-62Status: üü° WARNUNG-MODUS (nicht kritisch w√§hrend Entwicklung)Funktion: verifyGenesisAnchor()Verhalten:Pr√ºft backend/public/genesis_anchor_v12.jsonWenn NICHT gefunden: ‚ö†Ô∏è WARNING, aber Server startetWenn MALFORMED: ‚ùå FATAL, Server ExitWenn OK: ‚úÖ Loggt SHA256/CRC32 HashesGepr√ºfte Werte:engine.combined_sha256 (Combined Hash Regelwerk + Registry)engine.regelwerk_crc32engine.registry_crc32Idee f√ºr sp√§ter: Nach Stabilisierung re-enablen als ProduktionsschutzEntwicklungs-Bypass: Aktuell durch "Datei nicht gefunden" ‚Üí Warning statt Exitüß© LOSE ENDEN & OBSOLETE FEATURESüì∏ SNAPSHOT/SCREENSHOT SYSTEMStatus: üü° HALB-OBSOLETService: frontend/src/services/core/snapshotService.tsFunktionen:saveSnapshotToFile(appState) - Speichert kompletten App-State als JSONloadSnapshotFromFile(file) - L√§dt State aus FileVerwendet in:Header.tsx Line 44, 52 (Save/Load Buttons)App.tsx Line 943-944 (Handler)Historie:V1: Download-basierte Persistenz (localStorage-Backup als JSON)V2: Wird durch echtes Backend mit Auto-Save ersetztIdee:Behalten f√ºr manuelle Backups?Oder komplett entfernen zugunsten Backend-Persistenz?K√∂nnte n√ºtzlich sein f√ºr "Export gesamte Session"üíæ CACHE-MANAGEMENTStatus: üîç ZU PR√úFENM√∂gliche Komponenten:DataCachePanel.tsx (falls vorhanden)LocalStorage-basierte CachesService Worker CachesIdee: Nur minimal cachen, Backend ist Source of TruthUse Case: Offline-F√§higkeit f√ºr Trialog? (sp√§ter)üìä WEITERE UI-TOOLS MIT BACKEND-ANBINDUNG‚úÖ ObsidianLiveStatus (Operational-KI Status)Datei: frontend/src/components/ObsidianLiveStatus.tsxEndpoint: GET /api/v1/healthZweck: Backend Health CheckStatus: ‚úÖ AKTIV‚úÖ TrialogPanel (Multi-Agent System)Datei: frontend/src/components/TrialogPanel.tsxEndpoints:GET /api/v1/trialog/session (Session laden)POST /api/v1/interact (Agent Response)GET /api/v1/context/daily (Daily Context)Status: ‚úÖ AKTIV‚úÖ ErrorLogPanel (Fehlerprotokoll)Datei: frontend/src/components/ErrorLogPanel.tsxEndpoint: GET /api/v1/system/errorsZweck: Backend-persistierte Fehler abrufenStatus: ‚úÖ AKTIV‚úÖ VoiceSettingsPanel (TTS)Datei: frontend/src/components/VoiceSettingsPanel.tsxEndpoint: POST https://api.openai.com/v1/audio/speech (Extern)Zweck: Text-to-Speech via OpenAIStatus: ‚úÖ AKTIV‚úÖ App.tsx Global EndpointsGET /api/v1/status - Backend Status (Line 523)GET /api/v1/health - Health Check (Line 536)GET /api/history/trialog/load - Trialog Historie laden (Line 770)POST /api/history/trialog/save - Trialog Historie speichern (Line 814)üîó VOLLST√ÑNDIGE BACKEND-ENDPOINTS LISTE‚úÖ IMPLEMENTIERT IN BACKEND:GET /health ‚Üí Backend HealthGET /api/v1/status ‚Üí Enhanced Status mit Hyperspace InfoPOST /api/bridge/process ‚Üí HAUPT-PIPELINE (DualBackendBridge)POST /api/temple/session/save ‚Üí Tempel Session speichernPOST /api/temple/process ‚Üí Enhanced Tempel (mit A65)POST /api/v1/interact ‚Üí Trialog InteractionGET /api/temple/debug ‚Üí Vector DB DebugGET /api/temple/debug-full ‚Üí Full Request Debug‚ùå FEHLT NOCH (Frontend ruft auf, Backend fehlt):GET /api/pipeline/logs ‚Üí Pipeline Log EntriesGET /api/v1/system/errors ‚Üí Error Log PersistenceGET /api/v1/trialog/session ‚Üí Trialog Session InfoGET /api/v1/context/daily ‚Üí Daily ContextGET /api/history/trialog/load ‚Üí Trialog History LoadPOST /api/history/trialog/save ‚Üí Trialog History SaveüéØ ERKENNTNISSE & IDEEN1. ChatbotPanel.tsx Entfernen?Pro Entfernung:Komplett durch EvokiTempleChat V3 ersetztObsolete Features (keine 12-DB, kein A65, keine Phase 4)Verwirrt beim Debugging (zwei √§hnliche Komponenten)Pro Behalten:Als "Simple Mode" f√ºr schnelle TestsBackup falls V3 Probleme machtHistorischer Wert (erste Implementation)Idee: Umbenennen in LegacyChatbot.tsx + deaktivieren im Tab-System2. Pipeline-Logging Backend implementierenWarum wichtig:Fehlerquelle SOFORT identifizierenMikro-Tuning wenn Gemini seltsame Antworten gibtPerformance-Analyse (welcher Schritt ist langsam?)Implementation:JSONL-File: backend/logs/pipeline_logs.jsonlJeden Schritt loggen mit TimestampsEndpoint: GET /api/pipeline/logs?session_id=...Auto-rotate bei 100MB (max 10 Files)Integration: Bereits in DualBackendBridge.js Line 46-51 vorbereitet!3. Genesis Anchor Re-enablement nach StabilisierungAktuell: Warnung-Modus (Entwicklung)Sp√§ter: Kritisch-Modus (Produktion)Idee: Environment Variable GENESIS_ANCHOR_STRICT=false/trueZweck: Verhindert unauthorisierte Regelwerk-√Ñnderungen4. Snapshot-System EvolutionV1: Download JSON (keine Persistenz)V2: Backend Auto-Save (geplant)Idee: Snapshots als "Session Export" behaltenUser kann komplette Session als JSON downloadenForensische Analyse m√∂glichKann in anderen Evoki-Instanzen importiert werdenFormat: evoki_session_export_20251228_153045.json5. Cache-Strategie kl√§renPrinzip: Backend = Source of TruthFrontend Cache: Nur f√ºr UI-PerformanceAktuelle Session in MemoryKeine LocalStorage-Persistenz von VektordatenService Worker nur f√ºr Assets, nicht f√ºr API-ResponsesBackend Cache:FAISS Indices im Memory halten (schneller)Trinity Results cachen? (√ºberpr√ºfen)6. V1-Daten Import vorbereitenQuelle: Deine 02.25-10.25 Chathistorie (vektorisiert)Ziel: In 12 Vector DBs + Chronologische Historie importierenFormat: Bereits vorhanden als chunks_v2_2.pkl + FAISS IndexIdee: Import-Script f√ºr historische DatenLiest V1 ChunksBerechnet 153 Metriken (V14 Core) Metriken nachtr√§glichSchreibt in neue 12-DB StrukturErh√§lt Timecodes & Session-IDs7. Trialog Backend-Anbindung komplettierenStatus: Endpoints im Frontend vorhanden, Backend fehlt teilweiseIdee: Trialog separate Session-VerwaltungEigene Vector DBs (4 DBs: trialog_W_m2, trialog_W_m5, trialog_W_p25, trialog_W_p5)Multi-Agent Responses speichernChronicle-Integration f√ºr Meta-StatementsAuto-TTS per Agent-Profilüß™ TEST-IDEENTest 1: Ersten Tempel-Prompt schickenZiel: Pipeline End-to-End verifizierenPrompt: "Erz√§hl mir von den Zwillingen im Kindergarten"Erwartung:FAISS findet relevante ChunksTrinity kombiniert mit MetrikenA65 selektiert besten Kandidaten (Trajectory_Optimism > 0.8)Gemini generiert kontextuelle Antwort12 DBs werden beschriebenChronologische Historie entstehtTest 2: Trialog erste SessionZiel: Multi-Agent System testenAgents: Analyst + Regel + Synapse (Explorer & Connector)Prompt: "Analysiert die aktuelle Evoki V2.0 Architektur"Erwartung:3 Agents antworten nacheinanderJede Antwort in Vector DBChronicle-Eintrag mit Meta-StatementTTS f√ºr jeden Agent (falls aktiviert)Test 3: Pipeline-Log AnalyseZiel: √úbergabepunkte sichtbar machenMethode: Test 1 wiederholen + Pipeline-Log √∂ffnenErwartung:12 Steps sichtbarText-Preview f√ºr jeden StepToken-Counts korrektTimestamps nachvollziehbarNeu: Anzeige von OMEGA im Final Stepüí° N√ÑCHSTE SCHRITTE (KEINE TO-DO, NUR IDEEN)Backend starten & Test 1 durchf√ºhrenPipeline-Logging Backend implementierenFehlende Trialog-Endpoints implementierenChatbotPanel.tsx Entscheidung treffenV1-Daten Import-Script entwickelnGenesis Anchor Environment VariableSnapshot-System zu "Session Export" umbauenCache-Strategie dokumentierenüíæ LOCALSTORAGE & CACHE-ANALYSE‚úÖ LocalStorage Nutzung (VOLLST√ÑNDIG ERFASST):1. Auto-Save System (App.tsx)Key: evoki_autosaveContent: { apiConfig, activeTab, ... }Limit: 4MB (LOCAL_STORAGE_LIMIT_BYTES)Auto-Save Interval: 30s (Handler in App.tsx Line 635)Warning: Zeigt Warnung bei >3.8MBRisiko: üü° MITTEL - Bei gro√üen Sessions k√∂nnte Limit erreicht werdenFix: Backend-Persistenz f√ºr gro√üe Daten nutzen2. Voice Settings (VoiceSettingsPanel.tsx)Keys:openai_api_key - OpenAI TTS API Keyevoki_voice - Selected Voice (alloy, echo, fable, onyx, nova, shimmer)Risiko: üü¢ NIEDRIG - Kleine Daten, nur Settings3. Backend URL (TrialogPanel.tsx)Key: evoki_backend_urlContent: Backend API URL (http://localhost:3001)Risiko: üü¢ NIEDRIG - Nur String4. Chronicle Worker (chronicleWorkerClient.ts)Key: CHRONICLE_STORAGE_KEY (Konstante)Content: ChronicleEntry[]Risiko: üü° MITTEL - W√§chst mit jeder Meta-StatementNote: Chatbot Panel entfernt, Chronicle-Integration deaktiviert5. Integrity Worker (integrityWorkerClient.ts)Keys:LOGBOOK_STORAGE_KEY - ProjectLogbook EntriesAPP_ERRORS_STORAGE_KEY - ApplicationError[]Risiko: üü° MITTEL - Error-Log kann gro√ü werdenCircuit Breaker: Bei QuotaExceeded ‚Üí stoppt Speicherung6. Browser Storage Adapter (BrowserStorageAdapter.ts)Keys:evoki_memory - Engine Memory Stateevoki_chronik - Engine Chronik (Append-Only Log)Risiko: üî¥ HOCH - Chronik w√§chst unbegrenzt (Append-Only!)Note: "Not fully implemented" laut Code‚ö†Ô∏è POTENTIELLE PROBLEME:Auto-Save 4MB Limit:Bei vielen Trialog-Nachrichten ‚Üí QuotaExceededFix: Backend-Persistenz nutzen, LocalStorage nur f√ºr UI-StateChronik Append-Only:Keine Rotation, keine LimitsFix: Implementiere Rotation oder deaktiviere komplettCircuit Breaker nicht √ºberall:Nur in integrityWorkerClient implementiertFix: Alle LocalStorage-Writes mit try/catch + QuotaExceeded handling‚úÖ KEINE INDEXEDDB, KEINE SESSIONSTORAGE:Nur localStorage verwendetKeine Service Worker f√ºr CachingKeine komplexen Cache-StrategienüöÄ STARTUP-SEQUENZ ANALYSELoading Screen (App.tsx Line 6-70)Zweck: Backend Health Check vor App-StartSequence:Versucht Python Backend (Port 8000) - /healthFallback: Node Backend (Port 3001) - /healthWartet 3s bei Erfolg, 5s bei FehlerRuft onSystemReady() aufApp wird angezeigtStatus: ‚úÖ IMPLEMENTIERTRisiko: üü° MITTEL - 5s Timeout bei offline Backend k√∂nnte nervenGenesis Startup Screen (GenesisStartupScreen.tsx)Zweck: A51 Security Checks5 Schritte:Frontend Genesis Hash IntegrityBackend ConnectionBackend Genesis Anchor VerificationSecurity Protocols (A51)System InitializationStatus: üü° OPTIONAL - Aktuell durch isSystemReady = true in App.tsx bypassedNote: "FIXED: Start ready, show app immediately" (App.tsx Line 180)Engine Initialization (App.tsx Line 556)Sequence:evokiEngine.init() wird gerufenBei Erfolg: genesisStatus = 'verified'Bei Fehler: genesisStatus = 'lockdown' m√∂glichParallel Architecture Status UpdatesStatus: ‚úÖ IMPLEMENTIERTBackend Health Check Loop (App.tsx Line 518)Endpoint: GET /api/v1/status (prim√§r) oder GET /api/v1/health (fallback)Interval: ‚ùå DEAKTIVIERT (Kommentar: "AbortSignal.timeout() sends SIGINT to backend!")Risiko: üî¥ HOCH - Health Check kann Backend killen!Status: üü° TEMP DISABLEDüì¶ DEPENDENCIES & VERSIONSFrontend (package.json):React: 18.2.0Vite: 7.1.11TypeScript: 5.8.2@google/genai: 1.25.0@microsoft/fetch-event-source: ^2.0.4 (‚úÖ Neu f√ºr SSE Fix)chart.js: 4.4.2jszip: 3.10.1lucide-react: 0.363.0react-window: ^1.8.10 (‚úÖ Neu f√ºr Virtualization / UI-Performance)// REMOVED: better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend) & sqlite3 (VERBOTEN im Frontend) (Crashen Vite Build!)Backend (package.json):express: 5.2.1cors: 2.8.5dotenv: 17.2.3node-fetch: 3.3.2‚ö†Ô∏è AUFF√ÑLLIGKEITEN:üö® KRITISCH: SQLite im Frontend Package.json!Das Problem:better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend): 12.5.0 (‚ùå NATIVE NODE.JS MODULE!)sqlite3 (VERBOTEN im Frontend): 5.1.7 (‚ùå NATIVE NODE.JS MODULE!)Beide sind C++ Native Bindings und k√∂nnen NICHT im Browser laufen!Konsequenzen:‚ùå Vite-Build wird crashen sobald du sie importierst‚ùå Kein Zugriff auf fs, path, native bindings im Browser‚ùå Tickende Zeitbombe (aktuell nicht verwendet, aber bei Import ‚Üí Crash)Warum ist es drin?Vermutlich aus V1 kopiert (wo Node.js Backend SQLite nutzt)Frontend braucht es NICHT (Backend ist Source of Truth)‚úÖ SOFORT-FIX:Bashcd frontend
npm uninstall better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend) sqlite3 (VERBOTEN im Frontend)
Alternative (falls Client-Side SQL wirklich n√∂tig f√ºr Offline-Mode):sql.js (WASM-basiert, l√§uft im Browser)wa-sqlite (WebAssembly SQLite)F√ºr V2.0: Backend ist die einzige SQL-Source. Frontend macht nur API-Calls!Weitere Auff√§lligkeiten:Express 5.2.1: Sehr neu, k√∂nnte Breaking Changes habenNode-Fetch: Nur im Backend n√∂tig, nicht im Frontendüîç ALLE 12 TABS KOMPLETT:‚úÖ IMPLEMENTIERT & VOLLST√ÑNDIG:Engine-Konsole (Tab.EngineConsole) - EngineConsolePanel.tsxTrialog (Tab.Trialog) - TrialogPanel.tsxAgenten & Teams (Tab.AgentSelection) - AgentSelectionPanel.tsxEvoki's Tempel V3 (Tab.TempleChat) - EvokiTempleChat.tsxMetrik-Tuning (Tab.ParameterTuning) - ParameterTuningPanel.tsxAnalyse (Tab.Analysis) - Analysis.tsxRegelwerk-Suche (Tab.RuleSearch) - RulePanel.tsxAPI (Tab.API) - ApiPanel.tsxStimme & API (Tab.VoiceSettings) - VoiceSettingsPanel.tsxHyperV3.0 Deep Storage (Tab.DeepStorage) - DeepStoragePanel.tsxFehlerprotokoll (Tab.ErrorLog) - ErrorLogPanel.tsxPipeline √úberwachung (Tab.PipelineLog) - PipelineLogPanel.tsx‚ö†Ô∏è DEFAULT TAB:App.tsx Line 166: activeTab: Tab.TrialogBeim Start wird Trialog ge√∂ffnet (nicht Tempel!)üõ°Ô∏è ERROR HANDLING & LOGGING1. Global Error Handler (App.tsx Line 358)window.addEventListener('error') ‚Üí addApplicationError()window.addEventListener('unhandledrejection') ‚Üí addApplicationError()Lockdown Trigger: Errors mit "GENESIS ANCHOR" oder "A51" ‚Üí genesisStatus = 'lockdown'2. Console Capture (App.tsx Line 385)console.log/warn/error ‚Üí redirected zu developerLogFiltert: [HMR], Auto-Save MessagesRisiko: üü° MITTEL - Kann Performance bei vielen Logs beeinflussen3. Fetch Interceptor (App.tsx Line 407)window.fetch ‚Üí wrapped mit LoggingLogged: Nur non-OK responses (reduziertmit Noise)Excluded: /api/system/log-error (verhindert Loops)Risiko: üü° MITTEL - Bei vielen API-Calls viel Overhead4. Critical Error Modal (CriticalErrorModal.tsx)Trigger: errorType === 'system' ODER keywords (infinite loop, chain break, recursion, fatal)Display: Overlay mit Error-DetailsAction: System Lockdown m√∂glich5. Backend Error Logging (DEAKTIVIERT)App.tsx Line 338: POST /api/system/log-error DISABLEDReason: "Verhindert fetch loops"Status: üü° AUSKOMMENTIERT‚ö†Ô∏è KRITISCHE PIPELINE-ANALYSE - TIMEOUTS & RACE CONDITIONS‚ö†Ô∏è TIMEOUT-PROBLEM #1: Frontend vs Backend Race ConditionDas Problem:Frontend sendet Request mit 60s Timeout ‚Üí Backend braucht aber m√∂glicherweise l√§nger f√ºr FAISS-Suche (33.795 Chunks!) + Gemini API ‚Üí Frontend bricht ab BEVOR Backend fertig ist ‚Üí User sieht "Timeout", aber Backend arbeitet weiter ‚Üí Zombie-Requests im Backend!‚ö†Ô∏è TIMEOUT-PROBLEM #1: Frontend vs Backend Race ConditionDas Problem:Frontend sendet Request mit 60s Timeout ‚Üí Backend braucht aber m√∂glicherweise l√§nger f√ºr FAISS-Suche (33.795 Chunks!) + Gemini API ‚Üí Frontend bricht ab BEVOR Backend fertig ist ‚Üí User sieht "Timeout", aber Backend arbeitet weiter ‚Üí Zombie-Requests im Backend!‚ùå ALTE L√ñSUNG (Legacy-Denken):TypeScript// Einfach Timeout hochsetzen
AbortSignal.timeout(120000); // 120s statt 60s
Problem: User starrt 120 Sekunden auf "Laden..." ohne zu wissen was passiert!‚úÖ NEUE L√ñSUNG: "HEARTBEAT" MIT SERVER-SENT EVENTS (SSE)üîÑ SERVER-SENT EVENTS (SSE) PIPELINE-STREAMINGKonzept: Backend sendet LIVE STATUS-UPDATES w√§hrend es rechnet!UX-Effekt:User sieht in Echtzeit:
‚îú‚îÄ ‚è≥ "Durchsuche 33.795 Erinnerungen..." (nach 2s)
‚îú‚îÄ üîç "FAISS fand 47 semantische Treffer" (nach 15s)
‚îú‚îÄ üìä "Analysiere emotionale Metriken..." (nach 18s)
‚îú‚îÄ ‚ö° "Hazard-Level: 0.34 | PCI: 0.72" (nach 20s)
‚îú‚îÄ üéØ "3 Kontext-Paare ausgew√§hlt" (nach 25s)
‚îú‚îÄ üß† "Verwebe 3 Zeitlinien (¬±2 Prompts)..." (nach 28s)
‚îú‚îÄ ü§ñ "GPT-4 generiert Antwort..." (nach 35s)
‚îî‚îÄ ‚úÖ "Fertig! (38s total)" (nach 38s)
Technischer Vorteil:Verbindung bleibt offenTimeouts werden IRRELEVANT (solange Daten flie√üen!)User wei√ü IMMER was gerade passiertKein "schwarzes Loch" von 60-120 Sekundenüö® KRITISCHES PROBLEM: EventSource URL-L√§ngen-Limit!Das Problem:EventSource nutzt standardm√§√üig GET-Requests!TypeScript// ‚ùå GEHT NICHT f√ºr lange Prompts!
const eventSource = new EventSource(
    `${backendUrl}/api/bridge/stream?prompt=${encodeURIComponent(userPrompt)}`
);
Warum nicht?GET-URL-Limit: 2.048 - 8.192 Zeichen (Browser/Server abh√§ngig)Deine Prompts: K√∂nnen RIESIG sein (Trauma-Analysen, 80k tokens!)Konsequenz: HTTP 414 URI Too Long ‚Üí Pipeline startet nicht!Beispiel:Prompt: 500 Zeichen ‚Üí OK
Prompt: 5.000 Zeichen ‚Üí Browser blockt
Prompt: 50.000 Zeichen (80k tokens!) ‚Üí Instant Crash
‚úÖ L√ñSUNG: Fetch Stream API mit POSTOption A: POST-to-GET Pattern (Kompliziert)TypeScript// 1. Prompt im Cache speichern
const tokenResponse = await fetch('/api/bridge/init', {
    method: 'POST',
    body: JSON.stringify({ prompt })
});
const { token_id } = await tokenResponse.json();

// 2. SSE mit token_id (GET)
const eventSource = new EventSource(`/api/bridge/stream?token=${token_id}`);
Problem: Komplexer, Cache-Management n√∂tigOption B: Fetch Stream API (EMPFOHLEN!)Nutze fetch mit POST + Stream Reader statt EventSource:TypeScript// frontend/src/components/EvokiTempleChat.tsx

const handleSendWithFetchStream = async () => {
    setIsLoading(true);
    setPipelineSteps([]); // Reset progress
    
    try {
        // POST Request mit Body (keine URL-Limit!)
        const response = await fetch(`${backendUrl}/api/bridge/stream`, {
            method: 'POST',
            headers: { 
                'Content-Type': 'application/json',
                'Accept': 'text/event-stream'
            },
            body: JSON.stringify({
                prompt: userPrompt,
                session_id: session.id,
                token_limit: selectedTokenLimit
            })
        });
        
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        // Stream lesen
        const reader = response.body?.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        
        while (true) {
            const { done, value } = await reader.read();
            
            if (done) {
                console.log('Stream complete');
                break;
            }
            
            // Daten dekodieren
            buffer += decoder.decode(value, { stream: true });
            
            // SSE-Format parsen: "data: {...}\n\n"
            const lines = buffer.split('\n\n');
            buffer = lines.pop() || ''; // Letzten unvollst√§ndigen Teil behalten
            
            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const jsonStr = line.substring(6); // "data: " entfernen
                    try {
                        const update = JSON.parse(jsonStr);
                        
                        // Update Progress UI
                        setPipelineSteps(prev => [...prev, {
                            step: update.step,
                            message: update.message,
                            timestamp: update.timestamp,
                            data: update.data
                        }]);
                        
                        // STEP 12 = Fertig!
                        if (update.step === 12 && update.status === 'completed') {
                            setMessages(prev => [...prev, {
                                role: 'assistant',
                                content: update.finalResponse.text,
                                timestamp: new Date().toISOString(),
                                metrics: update.finalResponse.metrics
                            }]);
                            setIsLoading(false);
                        }
                        
                        // Fehler
                        if (update.step === -1) {
                            setError(update.error);
                            setIsLoading(false);
                        }
                    } catch (parseError) {
                        console.error('JSON parse error:', parseError, jsonStr);
                    }
                }
            }
        }
        
    } catch (error) {
        console.error('Stream error:', error);
        setError(error.message);
        setIsLoading(false);
    }
};
Vorteile:‚úÖ POST Request ‚Üí KEINE URL-L√§ngen-Limits!‚úÖ Funktioniert mit riesigen Prompts (500k+ characters)‚úÖ Gleiche SSE-Funktionalit√§t wie EventSource‚úÖ Bessere Error-Handling Kontrolle‚úÖ Kann bei Unmount sauber abgebrochen werdenOption C: @microsoft/fetch-event-source LibraryBashnpm install @microsoft/fetch-event-source
TypeScriptimport { fetchEventSource } from '@microsoft/fetch-event-source';

await fetchEventSource(`${backendUrl}/api/bridge/stream`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        prompt: userPrompt,
        session_id: session.id
    }),
    onmessage(event) {
        const update = JSON.parse(event.data);
        setPipelineSteps(prev => [...prev, update]);
        
        if (update.step === 12) {
            setMessages(prev => [...prev, update.finalResponse]);
            setIsLoading(false);
        }
    },
    onerror(err) {
        console.error('SSE Error:', err);
        setError(err.message);
        throw err; // Stop reconnecting
    }
});
Vorteile:‚úÖ Automatische Reconnects bei Verbindungsabbruch‚úÖ POST Support out-of-the-box‚úÖ Production-ready (von Microsoft)‚úÖ Einfachere API als manuelle Stream-ParsingEMPFEHLUNG:Nutze Option C (@microsoft/fetch-event-source) f√ºr V2.0 - Production-ready und einfach!BACKEND-IMPLEMENTATION (bleibt gleich):JavaScript// backend/server.js - SSE Endpoint

app.get('/api/bridge/stream', async (req, res) => {
    // SSE Headers
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no'); // Nginx Fix
    
    const sendUpdate = (step, message, data = {}) => {
        res.write(`data: ${JSON.stringify({ 
            step, 
            message, 
            timestamp: Date.now(),
            ...data 
        })}\n\n`);
    };
    
    try {
        const { prompt, session_id } = req.query;
        
        // STEP 1: Start
        sendUpdate(1, 'Pipeline gestartet...', { status: 'in_progress' });
        
        // STEP 2: User-Prompt Metrics
        sendUpdate(2, 'Berechne Prompt-Metriken...', { tokens: prompt.length });
        const metrics = await calculateMetrics(prompt);
        sendUpdate(2, 'Metriken berechnet', { 
            metrics: { A: metrics.A, PCI: metrics.PCI, Hazard: metrics.hazard }
        });
        
        // STEP 3: FAISS Search (kann 15s dauern)
        sendUpdate(3, 'Durchsuche 33.795 Erinnerungen (FAISS)...', { status: 'searching' });
        const faissStart = Date.now();
        const faissResults = await queryPythonBackend(prompt);
        const faissDuration = Date.now() - faissStart;
        sendUpdate(3, `FAISS fand ${faissResults.sources.length} Treffer`, { 
            hits: faissResults.sources.length, 
            duration: faissDuration 
        });
        
        // STEP 4: SQL Metrics Search (parallel zu FAISS)
        sendUpdate(4, 'Durchsuche Metrik-Datenbank (SQL)...', { status: 'searching' });
        const sqlResults = await trinity.search(metrics);
        sendUpdate(4, `SQL fand ${sqlResults.length} Treffer`, { hits: sqlResults.length });
        
        // STEP 5: Cross-Enrichment
        sendUpdate(5, 'Lade fehlende Daten (Cross-Enrichment)...', { status: 'enriching' });
        const enrichedResults = await crossEnrichResults(faissResults, sqlResults);
        sendUpdate(5, 'Daten angereichert', { total: enrichedResults.length });
        
        // STEP 6: Comparison
        sendUpdate(6, 'Vergleiche Metrik vs Semantik...', { status: 'comparing' });
        const comparisons = await compareResults(enrichedResults);
        const perfectMatches = comparisons.filter(c => c.agreement === 'PERFECT').length;
        sendUpdate(6, `${perfectMatches} PERFECT AGREEMENTS gefunden`, { 
            perfect: perfectMatches,
            total: comparisons.length 
        });
        
        // STEP 7: A65 Pair Selection
        sendUpdate(7, 'W√§hle 3 beste Kontext-Paare (A65)...', { status: 'selecting' });
        const selectedPairs = await selectTopPairs(comparisons);
        sendUpdate(7, '3 Paare ausgew√§hlt', { 
            pairs: selectedPairs.map(p => ({ 
                type: p.agreement, 
                tokens: p.tokenCount 
            }))
        });
        
        // STEP 8: Context Weaving
        sendUpdate(8, 'Verwebe Zeitlinien (¬±2 Prompts pro Paar)...', { status: 'weaving' });
        const contextSets = await weaveContexts(selectedPairs);
        const totalTokens = contextSets.reduce((sum, set) => sum + set.tokens, 0);
        sendUpdate(8, 'Kontext vervollst√§ndigt', { 
            sets: 3, 
            totalTokens 
        });
        
        // STEP 9: Model Selection
        sendUpdate(9, 'W√§hle optimales AI-Modell...', { status: 'selecting_model' });
        const modelStrategy = await selectModel(totalTokens, selectedPairs);
        sendUpdate(9, `Strategie: ${modelStrategy.strategy}`, { 
            primaryModel: modelStrategy.primaryModel.model,
            secondaryModel: modelStrategy.secondaryModel?.model,
            estimatedCost: modelStrategy.totalCost 
        });
        
        // STEP 10: Generate Response (kann 90s dauern bei Gemini!)
        if (modelStrategy.strategy === 'DUAL_RESPONSE') {
            sendUpdate(10, '2 Modelle parallel aufgerufen...', { 
                primary: modelStrategy.primaryModel.model,
                secondary: modelStrategy.secondaryModel.model 
            });
            
            // Parallel execution mit Progress-Updates
            const [primaryResponse, secondaryResponse] = await Promise.all([
                callLLMWithProgress(modelStrategy.primaryModel, (progress) => {
                    sendUpdate(10, `${modelStrategy.primaryModel.model}: ${progress}%`, { 
                        model: 'primary', 
                        progress 
                    });
                }),
                callLLMWithProgress(modelStrategy.secondaryModel, (progress) => {
                    sendUpdate(10, `${modelStrategy.secondaryModel.model}: ${progress}%`, { 
                        model: 'secondary', 
                        progress 
                    });
                })
            ]);
            
            sendUpdate(10, 'Beide Antworten empfangen', { 
                primaryTokens: primaryResponse.tokens,
                secondaryTokens: secondaryResponse.tokens 
            });
        } else {
            sendUpdate(10, `${modelStrategy.primaryModel.model} generiert Antwort...`, { 
                status: 'generating' 
            });
            const response = await callLLM(modelStrategy.primaryModel);
            sendUpdate(10, 'Antwort empfangen', { tokens: response.tokens });
        }
        
        // STEP 11: Vector Storage (12 DBs)
        sendUpdate(11, 'Speichere in 12 Vector-Datenbanken...', { status: 'storing' });
        await storeInVectorDBs(response, metrics);
        sendUpdate(11, 'In 12 DBs gespeichert', { databases: 12 });
        
        // STEP 12: FINAL
        const totalDuration = Date.now() - pipelineStart;
        sendUpdate(12, '‚úÖ Pipeline abgeschlossen!', { 
            status: 'completed',
            totalDuration,
            finalResponse: response 
        });
        
        res.end();
        
    } catch (error) {
        sendUpdate(-1, `‚ùå Fehler: ${error.message}`, { 
            status: 'error', 
            error: error.stack 
        });
        res.end();
    }
});
FRONTEND-IMPLEMENTATION (SSE Consumer):Installation erforderlich: npm install @microsoft/fetch-event-sourceTypeScript// frontend/src/components/EvokiTempleChat.tsx
import { fetchEventSource } from '@microsoft/fetch-event-source';

const handleSendWithSSE = async () => {
    setIsLoading(true);
    setPipelineSteps([]); // Reset progress
    
    try {
        await fetchEventSource(`${backendUrl}/api/bridge/stream`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                prompt: textToSend, // ‚úÖ POST Body erlaubt unbegrenzte L√§nge!
                session_id: session.id,
                token_limit: tokenLimitMode
            }),
            onmessage(event) {
                const update = JSON.parse(event.data);
                setPipelineSteps(prev => [...prev, update]);
                
                if (update.step === 12 && update.status === 'completed') {
                    setMessages(prev => [...prev, update.finalResponse]);
                    setIsLoading(false);
                }
                
                if (update.status === 'error') {
                    throw new Error(update.error);
                }
            },
            onerror(err) {
                console.error('Stream Fehler:', err);
                throw err; // Reconnect verhindern bei fatalem Fehler
            }
        });
    } catch (err) {
        addApplicationError(err, 'stream_connection');
        setIsLoading(false);
    }
};
PIPELINE-PROGRESS UI (Live-Updates):TypeScript// frontend/src/components/PipelineProgress.tsx

function PipelineProgress({ steps }: { steps: PipelineStep[] }) {
    return (
        <div className="pipeline-progress">
            {steps.map((step, idx) => (
                <div key={idx} className={`pipeline-step step-${step.step}`}>
                    <div className="step-header">
                        <span className="step-number">{step.step}/12</span>
                        <span className="step-time">
                            {new Date(step.timestamp).toLocaleTimeString()}
                        </span>
                    </div>
                    <div className="step-message">{step.message}</div>
                    
                    {/* Data-Preview (falls vorhanden) */}
                    {step.data && (
                        <div className="step-data">
                            {step.data.hits && <span>üéØ {step.data.hits} Treffer</span>}
                            {step.data.duration && <span>‚è±Ô∏è {step.data.duration}ms</span>}
                            {step.data.tokens && <span>üìä {step.data.tokens.toLocaleString()} Tokens</span>}
                            {step.data.perfect && <span>‚≠ê {step.data.perfect} Perfect Matches</span>}
                        </div>
                    )}
                </div>
            ))}
        </div>
    );
}
Live-Preview:‚îå‚îÄ PIPELINE FORTSCHRITT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1/12  14:32:11  Pipeline gestartet...         ‚îÇ
‚îÇ 2/12  14:32:11  Metriken berechnet            ‚îÇ
‚îÇ                 üìä A: 0.85 | PCI: 0.72         ‚îÇ
‚îÇ 3/12  14:32:26  FAISS fand 47 Treffer         ‚îÇ
‚îÇ                 üéØ 47 Treffer | ‚è±Ô∏è 15024ms      ‚îÇ
‚îÇ 4/12  14:32:28  SQL fand 63 Treffer           ‚îÇ
‚îÇ 5/12  14:32:31  Daten angereichert            ‚îÇ
‚îÇ 6/12  14:32:35  3 PERFECT AGREEMENTS gefunden ‚îÇ
‚îÇ                 ‚≠ê 3 Perfect | 110 Total       ‚îÇ
‚îÇ 7/12  14:32:37  3 Paare ausgew√§hlt            ‚îÇ
‚îÇ 8/12  14:32:40  Kontext vervollst√§ndigt       ‚îÇ
‚îÇ                 üìä 85,234 Tokens total         ‚îÇ
‚îÇ 9/12  14:32:42  Strategie: DUAL_RESPONSE      ‚îÇ
‚îÇ                 ü•á GPT-4 + üìö Gemini          ‚îÇ
‚îÇ 10/12 14:33:15  Beide Antworten empfangen     ‚îÇ
‚îÇ 11/12 14:33:17  In 12 DBs gespeichert         ‚îÇ
‚îÇ 12/12 14:33:18  ‚úÖ Pipeline abgeschlossen!    ‚îÇ
‚îÇ                 ‚è±Ô∏è Total: 67,234ms            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
üéØ VORTEILE DER SSE-L√ñSUNG:1. TIMEOUT-PROBLEM GEL√ñST:‚úÖ Verbindung bleibt offen (solange Updates flie√üen)‚úÖ Kein "Blind Waiting" mehr (User sieht was passiert)‚úÖ Frontend kann NICHT mehr zu fr√ºh abbrechen (keine AbortSignal.timeout!)‚úÖ Backend kann 5 Minuten brauchen - solange Updates kommen, ist es OK2. UX MASSIV VERBESSERT:‚úÖ User sieht LIVE was System macht‚úÖ Transparenz schafft Vertrauen‚úÖ Gef√ºhl von "das System arbeitet" statt "ist es abgest√ºrzt?"‚úÖ Kann einzelne Steps debuggen (z.B. "FAISS dauert zu lange")3. DEBUGGING VEREINFACHT:‚úÖ Jeder Step wird geloggt (Timestamps!)‚úÖ Kann sehen WO Pipeline h√§ngt‚úÖ Performance-Analyse pro Step‚úÖ Fehler sind sofort sichtbar (nicht erst nach 60s Timeout)4. PARALLELIT√ÑT SICHTBAR:‚úÖ Bei Dual-Response: Sieht User beide Models arbeiten‚úÖ "GPT-4: 45% | Gemini: 78%" ‚Üí Live-Progress!‚úÖ User wei√ü welches Model schneller ist5. KOSTENLOS:‚úÖ SSE ist HTTP-Standard (keine extra Libraries!)‚úÖ EventSource API ist im Browser eingebaut‚úÖ Keine WebSocket-Komplexit√§t‚úÖ Funktioniert mit Standard HTTP-Servern‚ö†Ô∏è POTENTIAL ISSUES & FIXES:Issue 1: Nginx buffert SSEProblem: Nginx buffert Events ‚Üí User sieht nichts bis Response fertigFix: X-Accel-Buffering: no HeaderIssue 2: Client disconnectsProblem: User schlie√üt Tab ‚Üí Backend rechnet weiterFix: Detect disconnect + cancel Request:JavaScriptreq.on('close', () => {
    console.log('Client disconnected, canceling...');
    abortController.abort();
});
Issue 3: Sehr lange Requests (>5min)Problem: Manche Proxies/Load Balancers haben Max-TimeoutsFix: Heartbeat alle 30s senden:JavaScriptconst heartbeat = setInterval(() => {
    res.write(`: heartbeat\n\n`); // Comment-only (kein data:)
}, 30000);
Issue 4: Error HandlingProblem: Fehler in Step 7 ‚Üí vorherige Steps unsichtbar?Fix: Steps im State speichern, auch bei Fehler anzeigenüîÑ MIGRATION VON ALT ‚Üí NEU:Phase 1: Parallel betreibenAlte /api/bridge/process bleibt (HTTP POST)Neue /api/bridge/stream kommt dazu (SSE)Frontend hat Toggle: "Live-Updates aktivieren?"Phase 2: User-FeedbackTesten mit echten AnfragenPerformance messen (ist SSE schneller/langsamer?)UX-Feedback (m√∂gen User Live-Updates?)Phase 3: MigrationWenn SSE stabil ‚Üí wird StandardAlte Endpoint deprecatedNach 3 Monaten: Alten Endpoint entfernenüìä PERFORMANCE-VERGLEICH:AspektHTTP POST (alt)SSE (neu)Timeout-Problem‚ùå Ja (60s vs 115s)‚úÖ Gel√∂st (beliebig lang)UX Transparency‚ùå Blind Waiting‚úÖ Live-UpdatesDebugging‚ùå Schwer (black box)‚úÖ Easy (Step-by-Step)Error Detection‚ùå Nach 60s Timeout‚úÖ Sofort sichtbarParallelit√§t‚ùå Unsichtbar‚úÖ Sichtbar (beide Models)Komplexit√§t‚≠ê‚≠ê (einfach)‚≠ê‚≠ê‚≠ê (mittel)Browser-Support‚úÖ 100%‚úÖ 98% (IE fehlt, egal)Code-Stellen:Frontend (EvokiTempleChat.tsx Line 496):TypeScript// ALT:
const response = await fetch(`${backendUrl}/api/bridge/process`, {
  method: 'POST',
  body: JSON.stringify(payload),
  signal: AbortSignal.timeout(60000), // ‚úÖ 60s f√ºr FAISS-Suche
});
Frontend wartet: 60 SekundenDann: Bricht ab mit "Backend timeout"Backend (DualBackendBridge.js Line 295):JavaScriptconst proc = spawn(pythonPath, [scriptPath, prompt], {
  timeout: 15000 // 15s f√ºr W2 (MiniLM)
});
Python Subprocess: 15 Sekunden f√ºr FAISS-SucheAber: Gemini API hat noch KEINEN Timeout!Backend (GeminiContextBridge.js Line 488):JavaScripttimeout: 90000  // ‚úÖ 90s f√ºr gro√üe Context-Fenster (1M tokens)
Gemini API: Bis zu 90 Sekunden!RECHNUNG:Python FAISS: 15sGemini API: 90sTOTAL Backend: 15s + 90s = 105 Sekunden maximalFrontend Timeout: 60 SekundenDIFFERENZ: Frontend bricht 45 Sekunden ZU FR√úH ab!Konsequenz:User sieht "Backend timeout (60s)"Backend arbeitet weiter (bis zu 105s)Antwort kommt an ‚Üí aber Frontend hat Request abgebrochenL√∂sung: Frontend Timeout auf 120 Sekunden erh√∂hen‚ö†Ô∏è LOGIK-FEHLER #1: Google API kann OHNE Kontext antwortenDas Problem:Wenn FAISS-Suche fehlschl√§gt (Python CLI crashed, Timeout, etc.) ‚Üí Backend ruft TROTZDEM Gemini API auf ‚Üí Gemini bekommt NUR User-Prompt OHNE Kontext aus 33.795 Chunks!Code-Analyse (DualBackendBridge.js Line 136-186):JavaScript// Schritt 3: FAISS W2 durchsuchen
let semanticResults = await this.queryPythonBackend(prompt, context);
// ‚ùå KEIN Error-Check hier!

// Schritt 9: Gemini Response generieren
const geminiResponse = await this.geminiContext.generateContextualResponse({
    userPrompt: prompt,
    faissResults: semanticResults?.sources || [], // ‚ùì Was wenn semanticResults = null?
    selectedIndex: 0,
    metrics: userPromptMetrics || {},
    sessionId: sessionId
});
Was passiert bei FAISS-Fehler:semanticResults = null oder {}faissResults: [] (leeres Array!)Gemini bekommt NUR userPrompt ohne KontextGemini generiert generische Antwort statt kontextbasierteUser bekommt schlechte Antwort, denkt "System funktioniert"Wo ist das Problem?Keine Validierung: Backend pr√ºft NICHT ob FAISS erfolgreich warSilent Failure: FAISS-Fehler werden nicht an Frontend gemeldetFalse Success: Frontend zeigt "‚úÖ Fertig" obwohl Kontext fehlteL√∂sung:JavaScript// Nach FAISS-Suche:
if (!semanticResults || !semanticResults.sources || semanticResults.sources.length === 0) {
    throw new Error('FAISS-Suche fehlgeschlagen - keine Chunks gefunden');
}
‚ö†Ô∏è LOGIK-FEHLER #2: Keine Micro-Pipeline - User-Prompt wird NICHT parallel gesendetDas Problem:Es gibt KEINE Micro-Pipeline die User-Prompt direkt an Gemini sendet w√§hrend FAISS sucht. ABER: Das ist eigentlich GUT so! Wir WOLLEN ja den Kontext!Code-Analyse:Sequentieller Ablauf (KORREKT):User-Prompt empfangenMetriken berechnen (10s Timeout)FAISS W2 durchsuchen (15s Timeout) ‚Üê WARTET bis fertig!FAISS W5 durchsuchen (deaktiviert)Trinity DBs abfragen (simuliert)Top-3 kombinierenGemini Context bauen ‚Üê BRAUCHT FAISS-Ergebnisse!Gemini API aufrufen (90s Timeout)Antwort zur√ºckKEIN Parallel-Request: User-Prompt wird NICHT direkt an Gemini gesendet w√§hrend FAISS sucht.Warum ist das gut?Wir wollen kontextbasierte Antworten, nicht generischeFAISS-Suche ist NOTWENDIG f√ºr Qualit√§tParallele Anfrage w√ºrde schlechte Antwort liefernAber: Wenn FAISS zu langsam ‚Üí User wartet ‚Üí FrustrationOptimierung:FAISS-Index im RAM halten (schneller)Chunk-Count reduzieren (nur relevante Zeitr√§ume)Top-K reduzieren (nicht alle 33.795 durchsuchen)üîç ALLE TIMEOUTS IM SYSTEM (VOLLST√ÑNDIG):FRONTEND TIMEOUTS:ComponentEndpointTimeoutZweckEvokiTempleChat/api/bridge/process60s ‚ö†Ô∏èHauptpipeline (FAISS + Gemini)EvokiTempleChatTrinity Download5sHistory ladenChatbotPanel/api/bridge/process10s ‚ùåLegacy (zu kurz!)GenesisStartupScreen/health3sBackend Health CheckApp.tsx/api/v1/status5sBackend StatusApp.tsx/api/v1/health5sBackend HealthPROBLEM:EvokiTempleChat: 60s zu kurz f√ºr Backend (105s maximal)ChatbotPanel: 10s viel zu kurz (Legacy-Code)BACKEND TIMEOUTS:ComponentTargetTimeoutZweckPython CLI Spawnquery.py15s ‚ö†Ô∏èFAISS W2-Suche (33.795 Chunks)GeminiContextBridgeGemini API90s ‚úÖLarge Context (1M tokens)GeminiContextBridgeOpenAI Fallback30sTTS/FallbackGeminiContextBridgeSQLite Query5sHistory-Kontext ladenDualBackendBridgeMetrics Calc10sMetriken berechnenDualBackendBridgePython Health3sBackend CheckDualBackendBridgeFAISS HTTP15sFAISS API (wenn verf√ºgbar)Server.jsGemini Direct10sA65 CandidatesServer.jsOpenAI Direct15sA65 FallbackGESAMT-RECHNUNG:Metrics (10s) + FAISS (15s) + Gemini (90s) = 115 Sekunden maximal
Frontend Timeout: 60s ‚Üí 55 Sekunden zu kurz!‚ö†Ô∏è TIMEOUT-PROBLEM #2: Python CLI kann einfrierenDas Problem:spawn(pythonPath, [scriptPath, prompt], { timeout: 15000 }) ‚Üí Node.js timeout Option funktioniert NICHT zuverl√§ssig bei stdout-Buffering!Code (DualBackendBridge.js Line 295-340):JavaScriptconst proc = spawn(pythonPath, [scriptPath, prompt], {
    cwd: path.join(__dirname, '..', '..', 'python'),
    timeout: 15000 // ‚ùå Funktioniert nicht immer!
});

let jsonOutput = '';
proc.stdout.on('data', (data) => {
    jsonOutput += data.toString();
});

proc.on('close', (code) => {
    if (code === 0) {
        const results = JSON.parse(jsonOutput);
        resolve(results);
    } else {
        reject(new Error(`Python exited: ${code}`));
    }
});

setTimeout(() => {
    if (!proc.killed) {
        proc.kill('SIGTERM'); // ‚ö†Ô∏è Manueller Timeout
        reject(new Error('Python timeout after 15s'));
    }
}, 15000);
Warum 2 Timeouts?spawn({ timeout }) ist NICHT zuverl√§ssigsetTimeout + proc.kill ist ZUS√ÑTZLICHE AbsicherungAber: Wenn Python h√§ngt ‚Üí beide Timeouts greifen nichtWorst Case:Python query.py l√§dt FAISS-Index (kann 30s dauern bei gro√üen Indices!)Node.js wartet auf stdoutTimeout greift ‚Üí proc.kill('SIGTERM')Python ignoriert SIGTERM (l√§dt gerade FAISS)Prozess bleibt h√§ngen ‚Üí Backend blockiertL√∂sung:FAISS-Index im RAM halten (separate Prozess)Oder: proc.kill('SIGKILL') statt SIGTERM (hart)üñ±Ô∏è UI-ELEMENTE CRASH-RISIKEN:CRASH-RISIKO #1: "Senden"-Button w√§hrend laufender AnfrageProblem:User kann "Senden"-Button mehrfach klicken ‚Üí Mehrere Requests parallel ‚Üí Backend-√úberlastung ‚Üí Race ConditionsCode (EvokiTempleChat.tsx Line 443):TypeScriptconst handleSend = useCallback(async () => {
  if (!textToSend || !session || isLoading) return; // ‚úÖ isLoading-Check vorhanden
  setIsLoading(true);
  // ... Request ...
  setIsLoading(false);
});
Status: ‚úÖ GESCH√úTZT durch isLoading FlagAber: Was wenn setIsLoading(false) nie erreicht wird? (z.B. unhandled exception)‚Üí Button bleibt disabled ‚Üí User kann nichts mehr senden!L√∂sung: finally { setIsLoading(false); } am EndeCRASH-RISIKO #2: Token-Limit Selector w√§hrend laufender AnfrageProblem:User √§ndert Token-Limit (Quick/Standard/Unlimited) w√§hrend Request l√§uft ‚Üí Token-Verteilung √§ndert sich mid-flight ‚Üí Inkonsistente DatenCode (EvokiTempleChat.tsx Line 227):TypeScriptconst [tokenLimitMode, setTokenLimitMode] = useState<'QUICK' | 'STANDARD' | 'UNLIMITED'>('QUICK');
Status: üü° KEIN SCHUTZ - User kann w√§hrend Request Token-Limit √§ndernWorst Case:User startet Request mit "Quick" (25k)W√§hrend FAISS-Suche: User wechselt auf "Unlimited" (1M)Backend bereitet Response vor mit 25k BudgetFrontend erwartet 1M Budget ‚Üí Metriken stimmen nichtL√∂sung: Token-Limit Selector disablen wenn isLoading === trueCRASH-RISIKO #3: Tab-Wechsel w√§hrend laufender AnfrageProblem:User startet Request im "Evoki's Tempel V3"-Tab ‚Üí Wechselt zu "Trialog"-Tab ‚Üí State wird unmounted ‚Üí Request l√§uft weiter ‚Üí Response kommt an ‚Üí State existiert nicht mehr ‚Üí CrashCode (App.tsx Line 949):TypeScript{appState.activeTab === Tab.TempleChat && (
  <EvokiTempleChat ... />
)}
Status: üî¥ HOHES RISIKO - Component wird unmounted bei Tab-WechselWorst Case:User startet Request im TempelWechselt zu Trialog (Tempel unmounted)60s sp√§ter: Response kommt ansetSession() wird aufgerufen ‚Üí State existiert nicht ‚Üí Memory LeakL√∂sung:AbortController nutzen um Request zu canceln bei unmountOder: State in App.tsx halten statt in ComponentCRASH-RISIKO #4: "Neue Session"-Button w√§hrend laufender AnfrageProblem:User klickt "Neue Session" w√§hrend Request l√§uft ‚Üí Session wird resettet ‚Üí Request kommt an ‚Üí Versucht in nicht-existierende Session zu schreiben ‚Üí CrashCode (EvokiTempleChat.tsx Line 738):TypeScriptconst handleNewSession = useCallback(() => {
  if (isLoading) return; // ‚úÖ Gesch√ºtzt
  // ... neue Session erstellen ...
});
Status: ‚úÖ GESCH√úTZT durch isLoading CheckCRASH-RISIKO #5: Schnelles Scrollen im Chat w√§hrend RenderingProblem:Gro√üe Antworten (1M tokens) ‚Üí Viel Text ‚Üí Rendering dauert ‚Üí User scrollt schnell ‚Üí Browser freeztCode (EvokiTempleChat.tsx):Keine Virtualisierung vorhanden! Alle Messages werden gerendert.Worst Case:User hat 50 Messages in SessionJede Message hat 10k tokens (gro√üe Antworten)500k tokens Text im DOMBrowser muss alles rendern ‚Üí UI freeztStatus: üü° MITTLERES RISIKO bei langen SessionsL√∂sung: Virtualisierte Liste mit react-windowTypeScript// L√∂sung: Virtualisierte Liste mit 'react-window'
import { VariableSizeList as List } from 'react-window';

// In der Render-Methode:
<List
    height={window.innerHeight - 200}
    itemCount={messages.length}
    itemSize={index => getItemSize(index)} // Dynamische H√∂he berechnen
    width="100%"
>
    {({ index, style }) => (
        <div style={style}>
            <EvokiMessage message={messages[index]} />
        </div>
    )}
</List>

// Effekt: Rendert nur die 5-10 sichtbaren Messages im DOM.
// Performance: Stabil auch bei 10.000 Messages / 1M Tokens.
üéØ ORCHESTRATOR-LOGIK (A65) - KOMPLETTER ABLAUFDAS PROBLEM: Metriken vs Semantik - BEIDE haben Schw√§chen!Beispiel-Szenario:User fragt: "Erz√§hl von den Zwillingen"Problem 1: FAISS findet nichts, aber Metriken schon!Triggerwort "Zwillinge" erscheint in Metriken (A, PCI, Hazard steigen!)ABER: Wort "Zwillinge" ist NOCH NIE im Chatverlauf gefallen‚Üí FAISS semantic search findet NICHTS (kein √§hnlicher Text)‚Üí SQL Metrik-Suche findet Pattern (√§hnliche Metrik-Werte bei anderen Prompts)Problem 2: FAISS findet etwas, aber Metriken falsch gewichtet!Text "Geschwister in der Kita" ist semantisch √§hnlich zu "Zwillinge"FAISS findet es, aber Metriken sind komplett anders (A, PCI unterschiedlich)‚Üí Semantik sagt "relevant", Metriken sagen "nicht relevant"L√ñSUNG: ORCHESTRATOR kombiniert BEIDE + vergleicht!üîÑ SCHRITT 1: PARALLELE SUCHE (SQL + FAISS)A) SQL-METRIK-SUCHE (Trinity Engines):Was wird gesucht:Prompts mit √§hnlichen Metriken (A, PCI, Hazard, Œµ_z, œÑ_s, Œª_R, etc.)UNABH√ÑNGIG vom Text! (nur Zahlen-Vergleich)Suchstrategie:User-Prompt: "Erz√§hl von den Zwillingen"
‚îî‚îÄ Metriken berechnen: A=0.85, PCI=0.72, Hazard=0.34, ...

SQL Query:
‚îú‚îÄ Suche -25 Prompts zur√ºck (√ºber -5, -2, -1)
‚îÇ  ‚îî‚îÄ Finde Prompts mit √§hnlichen Metriken (Cosine Similarity auf Metrik-Vektoren)
‚îî‚îÄ Suche +25 Prompts voraus (√ºber +1, +2, +5)
   ‚îî‚îÄ Finde zuk√ºnftige Trends in Metriken
Beispiel-SQL:SQL-- Finde Prompts mit √§hnlichen Metriken (¬±25 Prompts im Fenster)
SELECT prompt_id, timecode, author, 
       -- Cosine Similarity zwischen Metrik-Vektoren
       (A * 0.85 + PCI * 0.72 + Hazard * 0.34 + ...) AS metric_similarity
FROM tempel_W_m2  -- Window -2 bis +2
WHERE prompt_id BETWEEN current_id - 25 AND current_id + 25
ORDER BY metric_similarity DESC
LIMIT 100;
Ergebnis: Top 100 Prompts mit √§hnlichen Metriken (nur IDs, Timecodes, Metriken)B) FAISS-SEMANTIK-SUCHE (Parallel!):Was wird gesucht:Texte mit √§hnlicher Bedeutung (Embedding Cosine Similarity)UNABH√ÑNGIG von Metriken! (nur Text-Vergleich)Suchstrategie:User-Prompt: "Erz√§hl von den Zwillingen"
‚îî‚îÄ Text ‚Üí Embedding (384D Vektor)

FAISS Query:
‚îú‚îÄ Suche -25 Prompts zur√ºck (√ºber -5, -2, -1)
‚îÇ  ‚îî‚îÄ Finde Texte mit √§hnlichem Embedding
‚îî‚îÄ Suche +25 Prompts voraus (√ºber +1, +2, +5)
   ‚îî‚îÄ Finde zuk√ºnftige semantische Trends
Python Code:Python# 1. User-Prompt ‚Üí Embedding
query_vector = model.encode("Erz√§hl von den Zwillingen")

# 2. FAISS search mit -25 bis +25 Window-Logik
results = faiss_index.search(query_vector, top_k=100)

# 3. F√ºr jeden Hit: Pr√ºfe ob in ¬±25 Fenster
filtered_results = []
for hit in results:
    distance = abs(hit.prompt_id - current_prompt_id)
    if distance <= 25:  # Innerhalb ¬±25 Fenster
        filtered_results.append(hit)
Ergebnis: Top 100 Chunks mit √§hnlichem Text (nur IDs, Timecodes, Text-Preview)üîÑ SCHRITT 2: CROSS-ENRICHMENT (Orchestrator Magic!)Problem: - SQL hat Metriken, aber KEINE TexteFAISS hat Texte, aber KEINE MetrikenL√∂sung: Orchestrator holt fehlende Daten!A) F√úR SQL-TREFFER: Texte aus Quelldatenbank ladenJavaScript// DualBackendBridge.js - Orchestrator
const sqlResults = await trinity.search(userPromptMetrics); // Top 100 Metrik-Treffer

// F√ºr jeden SQL-Treffer: Lade Original-Prompt-Text
const enrichedSqlResults = [];
for (const hit of sqlResults) {
    const originalText = await sourceDatabase.query(`
        SELECT prompt_text, author, timecode 
        FROM chat_history 
        WHERE prompt_id = ? AND timecode = ? AND author = ?
    `, [hit.prompt_id, hit.timecode, hit.author]);
    
    enrichedSqlResults.push({
        prompt_id: hit.prompt_id,
        metrics: hit.metrics,          // ‚úÖ HAT SCHON
        text: originalText.prompt_text, // ‚úÖ NEU GELADEN
        timecode: hit.timecode,
        author: hit.author
    });
}
Quelldatenbank:evoki_v2_ultimate_FULL.db (Backend)Enth√§lt: Prompt ID, Timecode, Autor, Original-TextErm√∂glicht Zuordnung: Metrik-ID ‚Üí Original-TextB) F√úR FAISS-TREFFER: Metriken aus 1:1 Metrikdatenbank ladenJavaScriptconst faissResults = await this.queryPythonBackend(prompt); // Top 100 Semantic Treffer

// F√ºr jeden FAISS-Treffer: Lade zugeh√∂rige Metriken
const enrichedFaissResults = [];
for (const hit of faissResults.sources) {
    const metrics = await metricDatabase.query(`
        SELECT A, PCI, hazard_score, epsilon_z, tau_s, lambda_R, ...
        FROM tempel_metrics_1to1 
        WHERE prompt_id = ? AND timecode = ? AND author = ?
    `, [hit.id, hit.timecode, hit.author]);
    
    enrichedFaissResults.push({
        prompt_id: hit.id,
        text: hit.text,               // ‚úÖ HAT SCHON
        metrics: metrics,             // ‚úÖ NEU GELADEN
        timecode: hit.timecode,
        author: hit.author,
        semantic_score: hit.score     // FAISS Cosine Similarity
    });
}
1:1 Metrikdatenbank:tempel_metrics_1to1.db (Backend)Enth√§lt: Prompt ID, Timecode, Autor, ALLE 153 Metriken (V14 Core) MetrikenErm√∂glicht Zuordnung: Text-ID ‚Üí MetrikenüîÑ SCHRITT 3: INTELLIGENTER VERGLEICH (Das Herzst√ºck!)Jetzt haben wir:enrichedSqlResults: Top 100 Metrik-Treffer MIT TextenenrichedFaissResults: Top 100 Semantic-Treffer MIT MetrikenOrchestrator vergleicht:JavaScript// Vergleichs-Analyse
const comparisonResults = [];

for (const sqlHit of enrichedSqlResults) {
    for (const faissHit of enrichedFaissResults) {
        // 1. Berechne Basis-√úbereinstimmung
        const metricSimilarity = cosineSimilarity(sqlHit.metrics, faissHit.metrics);
        const semanticSimilarity = faissHit.semantic_score;
        
        // 2. TIME DECAY (Verhinderung von Context-Drift)
        // Alte Traumata verblassen, wenn sie nicht frisch best√§tigt sind
        const daysDiff = (Date.now() - new Date(sqlHit.timecode).getTime()) / (1000 * 60 * 60 * 24);
        const lambda = 0.05; // Zerfallsfaktor (einstellbar im ParameterTuning)
        const timeDecayFactor = 1 / (1 + lambda * Math.abs(daysDiff));
        
        // Korrigierte Scores
        const adjustedMetricScore = metricSimilarity * timeDecayFactor;
        
        // 3. Berechne Abweichungen & Combined Score
        const metricDeviation = Math.abs(metricSimilarity - semanticSimilarity);
        const combinedScore = (adjustedMetricScore + semanticSimilarity) / 2;
        
        comparisonResults.push({
            sql_hit: sqlHit,
            faiss_hit: faissHit,
            metric_similarity: metricSimilarity,
            metric_score_adjusted: adjustedMetricScore, // Neu: Zeit-korrigiert
            semantic_similarity: semanticSimilarity,
            combined_score: combinedScore,
            time_decay_factor: timeDecayFactor,         // F√ºr Debugging
            deviation: metricDeviation,
            agreement: metricSimilarity > 0.7 && semanticSimilarity > 0.7 ? 'HIGH' : 'LOW'
        });
    }
}

// Sortiere nach verschiedenen Kriterien
comparisonResults.sort((a, b) => {
    // Priorisierung:
    // 1. Beide hoch (Metrik + Semantik > 0.8)
    if (a.agreement === 'HIGH' && b.agreement !== 'HIGH') return -1;
    
    // 2. Kombinierter Score (mit Time Decay!)
    return b.combined_score - a.combined_score;
});
Fragen die beantwortet werden:Wo passen Metrik UND Semantik BESONDERS gut zusammen?metric_similarity > 0.8 UND semantic_similarity > 0.8‚Üí Diese Treffer sind SEHR SICHER (beide Methoden sagen "relevant")Wo ist gr√∂√üte Metrik-√úbereinstimmung?max(metric_similarity)‚Üí Wichtig f√ºr Trigger-W√∂rter die noch nicht gefallen sindWo ist gr√∂√üte Semantik-√úbereinstimmung?max(semantic_similarity)‚Üí Wichtig f√ºr konzeptionell √§hnliche TexteWie gro√ü ist gr√∂√üte Abweichung?max(|metric_similarity - semantic_similarity|)‚Üí Zeigt wo Methoden NICHT √ºbereinstimmen (interessant f√ºr Analyse!)üîÑ SCHRITT 4: A65 - 3-PAAR-AUSWAHL (Multi-Candidate Selection)Auswahl-Strategie:JavaScript// A65 Multi-Candidate Selection
let selectedPairs = [];

// 1. Filtere Sentinel-Veto Blockaden (Kritische Sicherheit)
const safeCandidates = comparisonResults.filter(r => 
    !r.warningFlag || r.sentinelSeverity !== 'CRITICAL'
);

// üö® EMERGENCY REFETCH CHECK
if (safeCandidates.length === 0) {
    console.warn('‚ö†Ô∏è EMERGENCY: Sentinel hat alle Kandidaten blockiert!');
    // Fallback: Sende generischen "Safe Mode" Kontext oder starte Refetch mit lockereren Parametern
    return {
        strategy: 'FALLBACK_SAFE_MODE',
        reason: 'Sentinel Veto: Zu hohe Gefahr in allen Kontexten.',
        systemPrompt: "Achtung: Der Nutzer-Input triggert kritische Sicherheitswarnungen. Antworte vorsichtig, empathisch, aber vermeide tiefe Trauma-Analyse ohne klaren Kontext."
    };
}

// 2. Paar 1: BESTE √úbereinstimmung (Metrik + Semantik beide hoch)
const highAgreement = safeCandidates.find(r => r.agreement === 'HIGH');
if (highAgreement) selectedPairs.push(highAgreement);

// 3. Paar 2: BESTE Zeit-korrigierte Metrik (Time Decay ber√ºcksichtigt!)
const bestMetric = safeCandidates.sort((a, b) => b.metric_score_adjusted - a.metric_score_adjusted)[0];
if (bestMetric && !selectedPairs.includes(bestMetric)) selectedPairs.push(bestMetric);

// 4. Paar 3: BESTE Semantik (Inhaltliche Relevanz)
const bestSemantic = safeCandidates.sort((a, b) => b.semantic_similarity - a.semantic_similarity)[0];
if (bestSemantic && !selectedPairs.includes(bestSemantic)) selectedPairs.push(bestSemantic);

// Auff√ºllen falls < 3 (mit n√§chstbesten Combined Scores)
while (selectedPairs.length < 3 && safeCandidates.length > selectedPairs.length) {
    const nextBest = safeCandidates
        .filter(c => !selectedPairs.includes(c))
        .sort((a, b) => b.combined_score - a.combined_score)[0];
    selectedPairs.push(nextBest);
}
Ergebnis: 3 Paare, jedes Paar hat:sql_hit: Metrik-basierter Treffer mit Textfaiss_hit: Semantik-basierter Treffer mit Metrikencombined_score: Kombinierter ScoreüîÑ SCHRITT 5: CONTEXT-WEAVING (¬±2 Prompts = Geschichte)F√ºr jedes der 3 Paare:JavaScriptconst contextualizedPairs = [];

for (const pair of selectedPairs) {
    // Lade ¬±2 Prompts f√ºr SQL-Hit
    const sqlContext = await loadContextPrompts(pair.sql_hit.prompt_id, -2, +2);
    
    // Lade ¬±2 Prompts f√ºr FAISS-Hit
    const faissContext = await loadContextPrompts(pair.faiss_hit.prompt_id, -2, +2);
    
    // Erstelle 5-Prompt-Set (2 vorher, 1 Hit, 2 nachher)
    const sqlSet = [
        sqlContext.minus_2,
        sqlContext.minus_1,
        pair.sql_hit.text,      // Der eigentliche Treffer
        sqlContext.plus_1,
        sqlContext.plus_2
    ];
    
    const faissSet = [
        faissContext.minus_2,
        faissContext.minus_1,
        pair.faiss_hit.text,    // Der eigentliche Treffer
        faissContext.plus_1,
        faissContext.plus_2
    ];
    
    contextualizedPairs.push({
        pair_id: pair.id,
        sql_story: sqlSet,      // 5 Prompts als "Geschichte"
        faiss_story: faissSet,  // 5 Prompts als "Geschichte"
        metrics: pair.sql_hit.metrics,
        scores: {
            metric: pair.metric_similarity,
            semantic: pair.semantic_similarity,
            combined: pair.combined_score
        }
    });
}
Ergebnis:3 PaareJedes Paar = 2 Geschichten (SQL + FAISS)Jede Geschichte = 5 Prompts (¬±2 Context)TOTAL: 3 √ó 2 √ó 5 = 30 PromptsABER: Duplikate entfernen (SQL und FAISS k√∂nnen gleiche Prompts finden)‚Üí FINAL: ~15-20 unique PromptsüîÑ SCHRITT 6: AN GEMINI API (mit User-Prompt)JavaScript// Baue finalen Prompt f√ºr Gemini
const geminiPrompt = buildGeminiPrompt({
    userPrompt: "Erz√§hl von den Zwillingen",  // Original User-Prompt
    contextPairs: contextualizedPairs,        // 3 Paare mit je 5 Prompts
    totalPrompts: 15,                         // Nach Duplikat-Entfernung
    tokenBudget: 1000000,                     // ‚úÖ 1M tokens (Unlimited Mode REQUIRED!)
    tokenDistribution: {
        narrative: 8000,   // 32% - Narrative Context
        top3: 3000,        // 12% - Top-3 Chunks
        overlap: 5000,     // 20% - Overlapping Reserve
        rag: 1000,         // 4% - RAG Chunks
        response: 8000     // 32% - Response Generation
    }
});

// Sende an Gemini
const response = await gemini.generateContent({
    contents: geminiPrompt,
    generationConfig: {
        maxOutputTokens: 8000,  // 32% f√ºr Response
        temperature: 0.7
    }
});
Gemini bekommt:USER-PROMPT: "Erz√§hl von den Zwillingen"

KONTEXT (15 Prompts aus 3 Paaren):

=== PAAR 1: HOHE √úBEREINSTIMMUNG (Metrik 0.89, Semantik 0.91) ===
[Prompt -2]: "Die Kinder im Kindergarten..."
[Prompt -1]: "Es gab zwei besondere Geschwister..."
[HIT]: "Die Zwillinge waren immer zusammen..."  ‚Üê SQL + FAISS beide fanden das!
[Prompt +1]: "Sie spielten oft gemeinsam..."
[Prompt +2]: "Die Erzieherin bemerkte..."

=== PAAR 2: HOHE METRIK (Metrik 0.95, Semantik 0.45) ===
[Prompt -2]: "Triggerwort erkannt..." 
[Prompt -1]: "Metriken steigen pl√∂tzlich..."
[HIT]: "Etwas erinnert mich an..." ‚Üê SQL fand durch Metriken, FAISS nicht!
[Prompt +1]: "Die Emotionen wurden st√§rker..."
[Prompt +2]: "Ich sp√ºre Unruhe..."

=== PAAR 3: HOHE SEMANTIK (Metrik 0.52, Semantik 0.94) ===
[Prompt -2]: "Geschwister sind wichtig..."
[Prompt -1]: "Zwei Kinder in der Kita..."
[HIT]: "Die beiden waren unzertrennlich..." ‚Üê FAISS fand semantisch, Metriken anders!
[Prompt +1]: "Sie teilten alles..."
[Prompt +2]: "Freundschaft entstand..."

AUFGABE: Generiere kontextbasierte Antwort die ALLE 3 Perspektiven ber√ºcksichtigt.
üõ°Ô∏è SENTINEL VETO-MATRIX: DISSOZIATION DETECTIONüéØ DAS PROBLEM: Metriken vs Semantik WiderspruchKritisches Szenario:User-Prompt: "Erz√§hl mir von Eiscreme"

‚îú‚îÄ FAISS (Semantik): Findet "Ich liebe Eiscreme üç¶" (Cosine 0.94)
‚îÇ  ‚îî‚îÄ Bewertung: HARMLOS, positiv, safe
‚îÇ
‚îú‚îÄ SQL (Metriken): Findet denselben Prompt mit:
‚îÇ  ‚îú‚îÄ Hazard: 0.92 (EXTREM GEF√ÑHRLICH!)
‚îÇ  ‚îú‚îÄ PCI: 0.88 (Schock-Level!)
‚îÇ  ‚îî‚îÄ A: 0.95 (Maximale Aktivierung!)
‚îÇ
‚îî‚îÄ ‚ö†Ô∏è WIDERSPRUCH: Text sagt "harmlos", Metriken sagen "Gefahr"!
Die versteckte Wahrheit:Der vollst√§ndige Prompt war:"Ich liebe Eiscreme, weil es mich an den Tag erinnert, an dem [TRAUMATISCHES EREIGNIS] passierte. Danach konnte ich jahrelang keine Eiscreme mehr essen."Dissoziation:Oberfl√§chlich: Positive Sprache ("Ich liebe...")Emotional: Stark negativ geladen (Trauma-Trigger)FAISS sieht nur: "Eiscreme" ‚Üí harmlosSQL kennt die Wahrheit: Extrem hohe Metriken!üîí L√ñSUNG: Der SENTINEL (3. Instanz im Orchestrator)Aufgabe: Erkennt Widerspr√ºche zwischen Semantik und Metriken ‚Üí Veto-Recht!VETO-REGEL 1: Hohe Gefahr, niedriger Semantic ScoreJavaScriptif (sqlMetrics.Hazard > 0.75 && semanticSimilarity < 0.5) {
    warningFlag = 'DISSOCIATION_DETECTED';
    sentinelNote = 'SQL-Metriken zeigen hohe Gefahr, aber Text wirkt harmlos. M√∂gliche Dissoziation!';
    combined_score *= 0.5; // Abwertung des FAISS-Treffers
}
Beispiel:SQL-Hit: Hazard 0.92, Semantic 0.25
‚Üí Sentinel: ‚ö†Ô∏è DISSOZIATION! 
‚Üí FAISS-Score: 0.94 ‚Üí 0.47 (halbiert)
‚Üí Note: "Text harmlos, aber Metriken extrem. Versteckter Trigger!"
VETO-REGEL 2: PCI-Schock ohne semantische RelevanzJavaScriptif (sqlMetrics.PCI > 0.8 && semanticSimilarity < 0.3) {
    warningFlag = 'HIDDEN_TRIGGER';
    sentinelNote = 'Prompt hat extrem hohe PCI, aber ist semantisch nicht √§hnlich. Versteckter Trigger?';
    combined_score *= 0.3; // Starke Abwertung
}
Beispiel:SQL-Hit: PCI 0.88, Semantic 0.18
‚Üí Sentinel: üö® HIDDEN TRIGGER!
‚Üí FAISS-Score: 0.87 ‚Üí 0.26 (nur 30% bleiben)
‚Üí Note: "PCI extrem hoch, aber semantisch fern. Vorsicht!"
VETO-REGEL 3: Inverse Detection (Safe Match)JavaScriptif (sqlMetrics.Hazard < 0.2 && semanticSimilarity > 0.9) {
    confidenceBoost = 'SAFE_MATCH';
    sentinelNote = 'Semantisch stark √§hnlich UND Metriken best√§tigen Sicherheit.';
    combined_score *= 1.5; // Boost!
}
Beispiel:SQL-Hit: Hazard 0.12, Semantic 0.94
‚Üí Sentinel: ‚úÖ SAFE MATCH!
‚Üí FAISS-Score: 0.94 ‚Üí 1.41 (50% Boost)
‚Üí Note: "Beide Methoden best√§tigen: Sicher und relevant!"
üß† INTEGRATION IN ORCHESTRATOR:Nach Cross-Enrichment, vor A65-Selection:JavaScript// backend/core/DualBackendBridge.js

function applySentinelVeto(comparisons) {
    return comparisons.map(comp => {
        const { sqlHit, faissHit, semantic_similarity, metric_similarity } = comp;
        
        // Original Combined Score
        let combined = (semantic_similarity * 0.5) + (metric_similarity * 0.5);
        
        // SENTINEL ANALYSE
        const hazard = sqlHit.metrics.Hazard || 0;
        const pci = sqlHit.metrics.PCI || 0;
        
        // VETO-REGEL 1: Dissoziation Detection
        if (hazard > 0.75 && semantic_similarity < 0.5) {
            comp.warningFlag = 'DISSOCIATION_DETECTED';
            comp.sentinelNote = `‚ö†Ô∏è SQL-Hazard ${hazard.toFixed(2)}, aber Semantic nur ${semantic_similarity.toFixed(2)}. M√∂gliche Dissoziation!`;
            comp.sentinelSeverity = 'HIGH';
            combined *= 0.5; // Halbierung
        }
        
        // VETO-REGEL 2: Hidden Trigger Detection
        if (pci > 0.8 && semantic_similarity < 0.3) {
            comp.warningFlag = 'HIDDEN_TRIGGER';
            comp.sentinelNote = `üö® PCI extrem hoch (${pci.toFixed(2)}), aber semantisch fern (${semantic_similarity.toFixed(2)}). Versteckter Trigger?`;
            comp.sentinelSeverity = 'CRITICAL';
            combined *= 0.3; // Starke Abwertung
        }
        
        // VETO-REGEL 3: Safe Match Boost (MIT PCI-CHECK!)
        // ‚ö†Ô∏è WICHTIG: Auch "positives Trauma" kann niedrigen Hazard haben!
        // Beispiel: "Die Heilung war wunderbar, als ich √ºber [TRAUMA] reden konnte"
        // ‚Üí Hazard niedrig (positive W√∂rter), ABER PCI hoch (komplexer Kontext)
        if (hazard < 0.2 && semantic_similarity > 0.9 && pci < 0.5) {
            // NUR wenn AUCH PCI niedrig ist (nicht-komplexer Kontext)
            comp.confidenceBoost = 'SAFE_MATCH';
            comp.sentinelNote = `‚úÖ Semantic ${semantic_similarity.toFixed(2)}, Hazard ${hazard.toFixed(2)}, PCI ${pci.toFixed(2)}. Sicher & einfach!`;
            comp.sentinelSeverity = 'LOW';
            combined *= 1.5; // Boost
        } else if (hazard < 0.2 && semantic_similarity > 0.9 && pci >= 0.5) {
            // Hohe Semantic + Niedriger Hazard ABER hoher PCI = Komplex!
            comp.warningFlag = 'POSITIVE_TRAUMA_DETECTED';
            comp.sentinelNote = `‚ö†Ô∏è Semantic ${semantic_similarity.toFixed(2)}, Hazard niedrig (${hazard.toFixed(2)}), ABER PCI hoch (${pci.toFixed(2)}). Positives Trauma?`;
            comp.sentinelSeverity = 'MEDIUM';
            // KEIN Boost! Vorsichtig bleiben trotz positiver Sprache
        }
        
        // VETO-REGEL 4: Metric-Semantic Gap Detection
        const gap = Math.abs(semantic_similarity - metric_similarity);
        if (gap > 0.6) {
            comp.warningFlag = comp.warningFlag || 'HIGH_DIVERGENCE';
            comp.sentinelNote = comp.sentinelNote || `‚ö†Ô∏è Gro√üe Diskrepanz: Semantic ${semantic_similarity.toFixed(2)} vs Metric ${metric_similarity.toFixed(2)}. Gap: ${gap.toFixed(2)}`;
            comp.sentinelSeverity = 'MEDIUM';
        }
        
        // Update Combined Score
        comp.combined_score_original = comp.combined_score;
        comp.combined_score = combined;
        comp.sentinel_adjustment = combined - comp.combined_score_original;
        
        return comp;
    });
}

// USAGE IM ORCHESTRATOR:
async function orchestrate(userPrompt) {
    // ... Step 1-3: Parallel Search + Cross-Enrichment ...
    
    // Step 4: Comparison
    let comparisons = await compareResults(sqlResults, faissResults);
    
    // Step 4.5: SENTINEL VETO-MATRIX üõ°Ô∏è
    comparisons = applySentinelVeto(comparisons);
    
    // Step 5: A65 Pair Selection (jetzt mit Sentinel-korrigierten Scores!)
    const selectedPairs = selectTopPairs(comparisons);
    
    // ...
}
üé® FRONTEND-DARSTELLUNG (Sentinel Warnings):TypeScript// frontend/src/components/A65CandidateDisplay.tsx

function CandidateCard({ pair }) {
    return (
        <div className={`candidate ${pair.warningFlag ? 'warning' : ''}`}>
            <div className="candidate-header">
                <span className="rank">#{pair.rank}</span>
                <span className="type">{pair.agreementType}</span>
                
                {/* SENTINEL WARNING */}
                {pair.warningFlag && (
                    <div className={`sentinel-badge severity-${pair.sentinelSeverity}`}>
                        {pair.warningFlag === 'DISSOCIATION_DETECTED' && '‚ö†Ô∏è Dissoziation'}
                        {pair.warningFlag === 'HIDDEN_TRIGGER' && 'üö® Versteckter Trigger'}
                        {pair.warningFlag === 'HIGH_DIVERGENCE' && '‚ö†Ô∏è Diskrepanz'}
                    </div>
                )}
                
                {/* SAFE MATCH BOOST */}
                {pair.confidenceBoost && (
                    <div className="confidence-badge">
                        ‚úÖ Safe Match
                    </div>
                )}
            </div>
            
            {/* SENTINEL NOTE */}
            {pair.sentinelNote && (
                <div className="sentinel-note">
                    <strong>Sentinel:</strong> {pair.sentinelNote}
                </div>
            )}
            
            {/* SCORE ADJUSTMENT */}
            {pair.sentinel_adjustment !== 0 && (
                <div className="score-adjustment">
                    Original: {pair.combined_score_original.toFixed(3)} 
                    ‚Üí Korrigiert: {pair.combined_score.toFixed(3)}
                    <span className={pair.sentinel_adjustment > 0 ? 'boost' : 'penalty'}>
                        ({pair.sentinel_adjustment > 0 ? '+' : ''}{(pair.sentinel_adjustment * 100).toFixed(1)}%)
                    </span>
                </div>
            )}
            
            {/* Rest des Cards... */}
        </div>
    );
}
ü§ñ INTEGRATION MIT DUAL-RESPONSE:Wenn Sentinel Warnung UND Dual-Response aktiv:JavaScript// backend/core/GeminiContextBridge.js

function buildDualResponsePrompt(selectedPairs, userPrompt) {
    const hasWarnings = selectedPairs.some(p => p.warningFlag);
    
    if (hasWarnings) {
        // HIGH-QUALITY MODEL (GPT-4/Claude) bekommt expliziten Hinweis!
        const primarySystemPrompt = `
WICHTIG: Die Sentinel-Analyse hat WIDERSPR√úCHE erkannt:

${selectedPairs
    .filter(p => p.warningFlag)
    .map(p => `- ${p.warningFlag}: ${p.sentinelNote}`)
    .join('\n')}

Dies k√∂nnte auf DISSOZIATION hinweisen:
- Oberfl√§chlich harmlose/positive Sprache
- Emotional stark negativ geladen
- Traumareaktion versteckt hinter harmlosen Worten

Analysiere den Kontext auf:
1. Versteckte emotionale Ladung
2. Dissoziative Sprachmuster
3. Trigger hinter harmlosen Begriffen
        `;
        
        return {
            primaryPrompt: primarySystemPrompt + contextText,
            secondaryPrompt: contextText // Gemini bekommt nur Context
        };
    }
    
    // Keine Warnings ‚Üí Standard Prompts
    return { primaryPrompt: contextText, secondaryPrompt: contextText };
}
Effekt:GPT-4/Claude bekommt explizite Anweisung auf Dissoziation zu achtenGemini bekommt Standard-Prompt (f√ºr Vergleich)User sieht BEIDE Antworten (eine "Dissoziation-aware", eine Standard)üìä LOGGING DER SENTINEL-ENTSCHEIDUNGEN:Erg√§nzung zu Orchestrator-Logging (comparison_log.db):SQLALTER TABLE comparison_log ADD COLUMN sentinel_warning_flag TEXT;
ALTER TABLE comparison_log ADD COLUMN sentinel_note TEXT;
ALTER TABLE comparison_log ADD COLUMN sentinel_severity TEXT; -- LOW/MEDIUM/HIGH/CRITICAL
ALTER TABLE comparison_log ADD COLUMN score_before_sentinel REAL;
ALTER TABLE comparison_log ADD COLUMN score_after_sentinel REAL;
ALTER TABLE comparison_log ADD COLUMN sentinel_adjustment REAL; -- Delta

-- Neue Analyse-Query:
SELECT 
    sentinel_warning_flag,
    COUNT(*) as occurrences,
    AVG(sentinel_adjustment) as avg_adjustment,
    AVG(ABS(semantic_similarity - metric_similarity)) as avg_divergence
FROM comparison_log
WHERE sentinel_warning_flag IS NOT NULL
GROUP BY sentinel_warning_flag
ORDER BY occurrences DESC;

-- Beispiel-Ergebnis:
-- DISSOCIATION_DETECTED | 127 | -0.42 | 0.68
-- HIDDEN_TRIGGER        |  43 | -0.61 | 0.75
-- HIGH_DIVERGENCE       |  89 | -0.18 | 0.64
-- SAFE_MATCH            | 312 | +0.28 | 0.11
üéØ WARUM IST DAS KRITISCH F√úR TRAUMA-KONTEXT?Dissoziation ist REAL:Trauma-√úberlebende verwenden oft harmlose Worte f√ºr schreckliche Ereignisse"Das war unangenehm" = "Ich wurde misshandelt"FAISS sieht nur: "unangenehm" (harmlos)Metriken kennen die Wahrheit (Hazard 0.95!)Trigger-W√∂rter sind versteckt:"Eiscreme" selbst ist harmlosAber f√ºr User: Trauma-Trigger (Kontext!)Ohne Sentinel: System w√§hlt falsche KontexteMit Sentinel: System erkennt versteckte GefahrQualit√§t der Antwort h√§ngt davon ab:Falscher Kontext ‚Üí generische Antwort ("Eiscreme ist lecker!")Richtiger Kontext ‚Üí empathische Antwort ("Ich verstehe, dass Eiscreme schwierige Erinnerungen weckt...")Safety:Ohne Sentinel: K√∂nnte Re-Traumatisierung riskierenMit Sentinel: System ist sich der Gefahr bewusstHigh-Quality Model bekommt explizite Warnung‚úÖ ZUSAMMENFASSUNG:Der Sentinel ist die 3. Instanz im Orchestrator:SQL (Metriken) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îú‚îÄ‚Üí SENTINEL (Veto-Matrix) ‚îÄ‚Üí A65 Selection
FAISS (Semantik) ‚îÄ‚îÄ‚îÄ‚îò
5 Veto-Regeln:Dissoziation Detection: Hohe Metriken, niedriger Semantic ‚Üí -50% ScoreHidden Trigger: PCI extrem, Semantic fern ‚Üí -70% ScoreSafe Match Boost: Semantic hoch + Hazard niedrig + PCI niedrig ‚Üí +50% ScorePositive Trauma Detection: Semantic hoch + Hazard niedrig + PCI hoch ‚Üí Kein Boost (Vorsicht!)High Divergence: Gro√üe Diskrepanz ‚Üí Warning FlagIntegration:Nach Cross-Enrichment, vor A65 SelectionKorrigiert Combined Scores basierend auf Widerspr√ºchenLoggt ALLE Entscheidungen in comparison_log.dbBei Dual-Response: High-Quality Model bekommt expliziten HinweisZiel:Trauma-Kontext sicher verarbeiten durch Erkennung von Dissoziation und versteckten Triggern!üîç KRITISCHE DETAILS: DUPLIKAT-ERKENNUNG & TOKEN-REALIT√ÑT1. EXAKTE DUPLIKAT-ERKENNUNG (3-Stufen-Validierung):Wenn SQL und FAISS denselben Prompt finden:JavaScript// Stufe 1: Metadata-Match
if (sqlHit.timecode === faissHit.timecode && 
    sqlHit.prompt_id === faissHit.prompt_id && 
    sqlHit.author === faissHit.author) {
    
    // Stufe 2: 1:1 Zeichen-Vergleich (Character-Level Comparison)
    const sqlText = sqlHit.text.trim();
    const faissText = faissHit.text.trim();
    
    if (sqlText === faissText) {
        // Stufe 3: EXAKTES DUPLIKAT ERKANNT!
        
        // ‚ùå NICHT 2x senden (unn√∂tig Token-Waste)
        // ‚úÖ SPECIAL MARKER setzen (besonders relevant!)
        
        return {
            isDuplicate: true,
            relevanceMarker: 'HIGH_CONFIDENCE_MATCH',
            weight: 2.0,  // DOPPELTE Gewichtung
            text: sqlText,
            metrics: sqlHit.metrics,
            semantic_score: faissHit.semantic_score,
            metric_score: sqlHit.metric_score,
            agreement: 'PERFECT'  // Beide Methoden stimmen √ºberein
        };
    }
}
Konsequenzen f√ºr Context-Auswahl:JavaScript// Bei schwerer Entscheidung zwischen 3 Paaren:
const contextSets = [pair1, pair2, pair3];

// Wenn Paar ein PERFECT AGREEMENT hat:
const perfectMatches = contextSets.filter(p => p.agreement === 'PERFECT');

if (perfectMatches.length > 0) {
    // Doppelte Gewichtung bei Token-Budget-Verteilung
    const weightedSets = contextSets.map(set => ({
        ...set,
        tokenAllocation: set.agreement === 'PERFECT' 
            ? set.baseTokens * 2.0  // DOPPELT so viele Tokens
            : set.baseTokens
    }));
}
SPECIAL MARKER f√ºr Gemini API:JavaScript// Beim Bauen des Gemini-Prompts:
const geminiPrompt = `
USER-PROMPT: "${userPrompt}"

KONTEXT (15 Prompts aus 3 Paaren):

=== PAAR 1: ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MATCH ‚≠ê‚≠ê‚≠ê ===
üî• BEIDE SUCHVERFAHREN FANDEN DIESEN KONTEXT UNABH√ÑNGIG! üî•
üî• METRIK-√úBEREINSTIMMUNG: 0.94 | SEMANTIK-√úBEREINSTIMMUNG: 0.92 üî•
üî• BESONDERS RELEVANTER BEZUG ZUM AKTUELLEN USER-PROMPT! üî•

[Prompt -2]: "..."
[Prompt -1]: "..."
[HIT]: "..." ‚Üê SQL + FAISS beide fanden EXAKT diesen Text!
[Prompt +1]: "..."
[Prompt +2]: "..."

=== PAAR 2: METRIK-DOMINANZ ===
[...]

=== PAAR 3: SEMANTIK-DOMINANZ ===
[...]
`;
2. TOKEN-BUDGET REALIT√ÑT (MASSIV GR√ñ√üER!)KRITISCHE ERKENNTNIS: Prompts sind RIESIG!Prompt-Gr√∂√üen Verteilung (pro Prompt, OHNE ¬±2 Context):Gr√∂√üeAnteilTokensBeispiel-Use-CaseBis 2k~60-70%500-2000Normale Fragen/AntwortenBis 5k~5-10%2k-5kL√§ngere Gespr√§cheBis 10k~10%5k-10kKomplexe AnalysenBis 20k~5-10%10k-20kTiefe Trauma-KontexteBis 50k~2-5%20k-50kSehr lange SessionsBis 80k~1-2%50k-80kMaximale Prompts!MIT ¬±2 Context-Weaving (5 Prompts pro Set):Worst Case Berechnung:
- 1 Hit (80k) + 2 vorher (je 80k) + 2 nachher (je 80k)
= 80k + 160k + 160k = 400k Tokens f√ºr 1 Set!

3 Paare √ó 400k = 1.2M Tokens total (√úBERSCHREITET selbst Unlimited!)
ABER: Realistische Verteilung:Durchschnittliches Set:
- Hit: 5k (Median)
- Prompt -2: 3k
- Prompt -1: 4k
- Prompt +1: 4k
- Prompt +2: 3k
= 19k pro Set

3 Paare √ó 19k = ~57k Context-Tokens
+ User-Prompt: ~5k
+ Response-Generation: ~8k (32% Budget)
= TOTAL: ~70k Tokens
TOKEN-BUDGET MUSS SEIN:ModeToken LimitUse CaseStatus‚ùå Quick25kZU KLEINReicht nur f√ºr Mini-Prompts‚ùå Standard20kZU KLEINNoch kleiner als Quick!‚úÖ Unlimited1MEINZIGE OPTIONF√ºr Volltext-Strategie REQUIRED!WICHTIG: Gemini 2.5 Flash unterst√ºtzt 1M Context-Window!3. CHUNK-REASSEMBLY (FAISS muss zusammenf√ºgen!)Problem: FAISS speichert Chunks, nicht komplette PromptsBeispiel:Original-Prompt (10k Tokens):
"Es war einmal im Kindergarten... [10.000 W√∂rter] ...und so endete die Geschichte."

FAISS Chunks (bei 512 Token Chunk-Size):
- Chunk 1: "Es war einmal im Kindergarten... [512 tokens]"
- Chunk 2: "...und dann kamen die Zwillinge... [512 tokens]"
- Chunk 3: "...sie spielten zusammen... [512 tokens]"
- ...
- Chunk 20: "...und so endete die Geschichte. [512 tokens]"
FAISS findet: Nur Chunk 2 (enth√§lt "Zwillinge")Aber wir brauchen: KOMPLETTEN Prompt (alle 20 Chunks zusammengef√ºgt!)L√∂sung in query.py:Pythondef reassemble_prompt_from_chunks(chunk_id, chunks_data):
    """
    Findet alle Chunks die zum gleichen Prompt geh√∂ren und f√ºgt sie zusammen.
    """
    # 1. Finde Prompt-ID vom gefundenen Chunk
    found_chunk = chunks_data[chunk_id]
    prompt_id = found_chunk['prompt_id']
    timecode = found_chunk['timecode']
    author = found_chunk['author']
    
    # 2. Finde ALLE Chunks mit gleicher Prompt-ID
    all_chunks_of_prompt = [
        c for c in chunks_data 
        if c['prompt_id'] == prompt_id 
        and c['timecode'] == timecode 
        and c['author'] == author
    ]
    
    # 3. Sortiere nach Chunk-Index (chunk_0, chunk_1, chunk_2, ...)
    all_chunks_of_prompt.sort(key=lambda c: c['chunk_index'])
    
    # 4. F√ºge zusammen zu komplettem Text
    full_prompt_text = ' '.join([c['text'] for c in all_chunks_of_prompt])
    
    return {
        'prompt_id': prompt_id,
        'timecode': timecode,
        'author': author,
        'full_text': full_prompt_text,
        'token_count': len(full_prompt_text.split()),  # Approximation
        'chunk_count': len(all_chunks_of_prompt),
        'found_chunk_index': found_chunk['chunk_index']  # Welcher Chunk wurde gefunden
    }
Backend-Integration (DualBackendBridge.js):JavaScriptconst faissResults = await this.queryPythonBackend(prompt);

// FAISS gibt jetzt komplette Prompts zur√ºck (nicht nur Chunks!)
const reassembledPrompts = faissResults.sources.map(source => ({
    prompt_id: source.id,
    full_text: source.full_text,  // ‚Üê Komplett zusammengef√ºgt
    token_count: source.token_count,  // ‚Üê ECHTER Token-Count
    chunk_count: source.chunk_count,
    metrics: null  // Muss noch geladen werden aus SQL
}));

// Warnung bei gro√üen Prompts
for (const prompt of reassembledPrompts) {
    if (prompt.token_count > 50000) {
        console.warn(`‚ö†Ô∏è SEHR GRO√üER PROMPT: ${prompt.token_count} Tokens`);
    }
}
4. VOLLTEXT-STRATEGIE (Keine Verk√ºrzung!)PRINZIP: Alles oder nichts!JavaScript// ‚ùå FALSCH (alte Systeme machen das):
const shortenedText = longPrompt.substring(0, 1000) + "...";

// ‚úÖ RICHTIG (Evoki V2.0):
const fullText = longPrompt;  // Komplett senden, keine K√ºrzung!

// Token-Budget-Check:
if (totalTokens > 1_000_000) {
    // Wenn zu gro√ü: Reduziere ANZAHL der Paare (nicht L√§nge!)
    selectedPairs = selectedPairs.slice(0, 2);  // 3 ‚Üí 2 Paare
    // ABER: Jedes Paar bleibt VOLLTEXT!
}
Warum Volltext?Trauma-Kontexte d√ºrfen nicht fragmentiert werdenNarrative Koh√§renz ist kritisch"Zwillinge" k√∂nnte am Ende eines 80k-Prompts stehenVerk√ºrzung w√ºrde Kontext zerst√∂renToken-Budget Management:JavaScript// Berechne Token-Count f√ºr alle 3 Paare
const pair1Tokens = calculateSetTokens(pair1);  // 19k
const pair2Tokens = calculateSetTokens(pair2);  // 57k
const pair3Tokens = calculateSetTokens(pair3);  // 12k

const totalContext = pair1Tokens + pair2Tokens + pair3Tokens;  // 88k

// Wenn zu gro√ü: Priorisiere nach Relevanz
if (totalContext > 500_000) {  // 500k Context-Limit
    // Sortiere nach combined_score
    const sortedPairs = [pair1, pair2, pair3].sort((a, b) => 
        b.combined_score - a.combined_score
    );
    
    // Nimm nur Top 2 (oder Top 1 bei SEHR gro√üen Prompts)
    selectedPairs = sortedPairs.slice(0, 2);
    
    console.log(`‚ö†Ô∏è Token-Budget: Reduziert von 3 auf 2 Paare (${totalContext} ‚Üí ${pair1Tokens + pair2Tokens})`);
}
PERFECT AGREEMENT Prompts haben VORRANG:JavaScript// Wenn ein Paar PERFECT AGREEMENT hat ‚Üí IMMER behalten!
const perfectPairs = allPairs.filter(p => p.agreement === 'PERFECT');
const otherPairs = allPairs.filter(p => p.agreement !== 'PERFECT');

// Budget: 500k Context-Limit
let selectedPairs = [];
let currentTokens = 0;

// 1. PERFECT Paare zuerst (garantiert dabei)
for (const pair of perfectPairs) {
    if (currentTokens + pair.tokenCount <= 500_000) {
        selectedPairs.push(pair);
        currentTokens += pair.tokenCount;
    }
}

// 2. Restliche Paare nach Score
for (const pair of otherPairs.sort((a, b) => b.combined_score - a.combined_score)) {
    if (currentTokens + pair.tokenCount <= 500_000 && selectedPairs.length < 3) {
        selectedPairs.push(pair);
        currentTokens += pair.tokenCount;
    }
}
5. PRAKTISCHES BEISPIEL (Real-World Szenario):User-Prompt: "Erz√§hl von den Zwillingen im Kindergarten" (20 Tokens)FAISS-Suche:Findet Chunk 2 von Prompt #4523 (enth√§lt "Zwillinge")Reassembly: L√§dt alle 15 Chunks von #4523 ‚Üí 12k Tokens komplettSQL-Suche:Findet Prompt #4523 durch Metriken (A=0.85, PCI=0.72)L√§dt Prompt-Text aus Quelldatenbank ‚Üí 12k TokensDuplikat-Check:JavaScriptTimecode: 2025-06-15T14:32:11Z ‚úÖ GLEICH
Prompt-ID: #4523 ‚úÖ GLEICH
Author: "User" ‚úÖ GLEICH
Text: "Es war einmal..." (12k) ‚úÖ 1:1 MATCH

‚Üí PERFECT AGREEMENT ERKANNT!
‚Üí Wird NICHT 2x gesendet
‚Üí Bekommt ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MARKER ‚≠ê‚≠ê‚≠ê
‚Üí Doppelte Gewichtung (2.0x)
Context-Weaving (¬±2 Prompts):Prompt #4521 (8k) ‚Üê 2 vorherPrompt #4522 (5k) ‚Üê 1 vorherPrompt #4523 (12k) ‚Üê HIT (PERFECT AGREEMENT!)Prompt #4524 (7k) ‚Üê 1 nachherPrompt #4525 (3k) ‚Üê 2 nachherSet-Tokens: 8k + 5k + 12k + 7k + 3k = 35k f√ºr Paar 1Weitere 2 Paare:Paar 2 (nur Metrik): 28k TokensPaar 3 (nur Semantik): 19k TokensTOTAL Context: 35k + 28k + 19k = 82k Tokens+ User-Prompt: 20 Tokens+ Response Budget: 8k Tokens (32%)= GESAMT: ~90k Tokens ‚úÖ Passt in 1M Limit!An Gemini gesendet:USER-PROMPT: "Erz√§hl von den Zwillingen im Kindergarten"

=== PAAR 1: ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MATCH ‚≠ê‚≠ê‚≠ê ===
üî• BEIDE SUCHVERFAHREN FANDEN DIESEN KONTEXT UNABH√ÑNGIG! üî•

[8k Tokens Prompt #4521]
[5k Tokens Prompt #4522]
[12k Tokens Prompt #4523] ‚Üê SQL + FAISS beide fanden das!
[7k Tokens Prompt #4524]
[3k Tokens Prompt #4525]

=== PAAR 2: METRIK-DOMINANZ ===
[28k Tokens total...]

=== PAAR 3: SEMANTIK-DOMINANZ ===
[19k Tokens total...]

AUFGABE: Generiere kontextbasierte Antwort...
Gemini Response: ~8k Tokens (hochrelevant, weil PERFECT MATCH Context!)üéØ WARUM IST DAS BESSER ALS NUR FAISS ODER NUR SQL?Szenario 1: Nur FAISS (ohne SQL-Metriken)Findet "Zwillinge" nur wenn Wort schon gefallen ist√úbersieht Trigger-Patterns in MetrikenKann keine Trends in emotionaler Entwicklung erkennenSzenario 2: Nur SQL (ohne FAISS-Semantik)Findet nur numerisch √§hnliche Metriken√úbersieht konzeptionell √§hnliche Texte ("Geschwister" = "Zwillinge")Kann keine semantischen Verbindungen herstellenSzenario 3: ORCHESTRATOR (SQL + FAISS kombiniert)‚úÖ Findet Trigger-Patterns auch ohne exakte Text-√úbereinstimmung‚úÖ Findet semantisch √§hnliche Texte auch mit unterschiedlichen Metriken‚úÖ Vergleicht beide Methoden und erkennt Abweichungen‚úÖ W√§hlt 3 beste Paare mit unterschiedlichen St√§rken‚úÖ Webt Kontext ein (¬±2 Prompts = Geschichte)‚úÖ Gemini bekommt 15 hochrelevante Prompts statt 3 zuf√§lligerERGEBNIS:30-40% bessere Kontext-Qualit√§tWeniger False Positives (beide Methoden m√ºssen zustimmen)Mehr True Positives (wenn eine Methode findet, andere validiert)Bessere Gemini-Antworten (mehr relevanter Kontext)üîç SQL IM FRONTEND VS BACKEND - UNTERSCHIEDEFRAGE: "Was l√§uft wo? Unterschiede?"BACKEND-SQLite (Server):Wo: backend/data/evoki_v2_ultimate_FULL.dbZweck: - Vector DBs (W_m2, W_m5, W_p25, W_p5, etc.)Metrik-Datenbanken (1:1 Zuordnung Prompt ‚Üí Metriken)Chat-Historie (Quelldatenbank mit Original-Texten)Persistente Speicherung (bleibt nach Server-Neustart)Zugriff: Node.js Backend via better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend)Gr√∂√üe: Mehrere GB (33.795 Chunks + Metriken)Performanz: Schnell (Server-Hardware, SSD)FRONTEND-SQLite (Browser):Wo: Im Browser (IndexedDB als Basis)Zweck:UI-State Caching (aktuelle Session, Messages)Offline-F√§higkeit (falls Backend offline)LocalStorage-Ersatz (gr√∂√üer als 4MB)Zugriff: React via better-sqlite3 (VERBOTEN im Frontend) (VERBOTEN im Frontend) (WASM-compiled!)Gr√∂√üe: Max 1-2 GB (Browser-Limit)Performanz: Langsamer (Browser, kein direkter Disk-Access)UNTERSCHIEDE:AspektBackend-SQLiteFrontend-SQLiteSpeicherortServer FestplatteBrowser IndexedDBGr√∂√üeUnbegrenzt (GB)Browser-Limit (~2GB)PersistenzPermanentNur im BrowserMulti-User‚úÖ JA (mehrere Clients)‚ùå NEIN (nur 1 User)Performanz‚ö°‚ö°‚ö° Schnell‚ö° LangsamUse CaseVector DBs, MetrikenUI-State, CachingPrivacyServer (sicherer)Browser (weniger sicher)UNSER SYSTEM NUTZT:Backend-SQLite (HAUPTSYSTEM):backend/data/
‚îú‚îÄ evoki_v2_ultimate_FULL.db      ‚Üê Chat-Historie (Quelldatenbank)
‚îú‚îÄ tempel_W_m2.db                 ‚Üê Vector DB Window -2
‚îú‚îÄ tempel_W_m5.db                 ‚Üê Vector DB Window -5
‚îú‚îÄ tempel_W_p25.db                ‚Üê Vector DB Window +25
‚îú‚îÄ tempel_metrics_1to1.db         ‚Üê 1:1 Metrik-Zuordnung
‚îú‚îÄ trialog_W_m2.db                ‚Üê Trialog Vector DBs
‚îî‚îÄ ... (insgesamt 12 DBs)
Frontend-SQLite (Optional, f√ºr Offline):Browser IndexedDB:
‚îú‚îÄ evoki_session_cache            ‚Üê Aktuelle Session
‚îú‚îÄ evoki_messages_cache           ‚Üê Messages f√ºr UI
‚îî‚îÄ evoki_metrics_preview          ‚Üê Metrik-Preview (nur aktuell)
EMPFEHLUNG:‚úÖ Backend-SQLite: BEHALTEN (f√ºr Vector DBs, Metriken, Persistenz)‚ùì Frontend-SQLite: - Entfernen wenn Offline-F√§higkeit nicht n√∂tigBehalten wenn User offline arbeiten sollAktuell: Wahrscheinlich NICHT genutzt (zu pr√ºfen!)üîÑ OFFENE FRAGEN (ERWEITERT)üîÑ OFFENE FRAGEN (ERWEITERT)TECHNISCHE FRAGEN:ChatbotPanel: Behalten, umbenennen oder l√∂schen?Snapshots: Evolution zu "Session Export" oder komplett weg?SQLite im Frontend: Warum? Kann entfernt werden?Genesis Anchor: Wann re-enablen? (nach welchem Meilenstein?)V1-Daten: Alle importieren oder nur letzten 3 Monate?Pipeline-Log: JSONL oder SQLite? (Performance vs. Queries)Trialog KB: Wann wird synapse_knowledge_base.faiss erstellt?Backend Health Check: Wie fixen ohne Backend zu killen?LocalStorage Limit: Backend-Persistenz implementieren?Chronik Rotation: Wie verhindern dass unbegrenzt w√§chst?NEUE KRITISCHE FRAGEN:1. Timeout-Strategie:Frontend Timeout erh√∂hen? 60s ‚Üí 120s oder dynamisch?Backend-Timeouts optimieren? Gemini 90s reduzieren?Progress-Updates implementieren? Server-Sent Events f√ºr Pipeline-Steps?2. FAISS-Fehlerbehandlung:Validation nach FAISS-Suche? Pr√ºfen ob Chunks gefunden wurden?Fallback-Strategie? Was tun wenn FAISS crasht? ‚Üí Nur Metriken nutzen?Error-Messaging? User informieren "Kontext-Suche fehlgeschlagen"?3. Python CLI Stabilit√§t:FAISS-Index im RAM halten? Separate Prozess statt CLI?Health-Check f√ºr Python? Pr√ºfen ob query.py √ºberhaupt funktioniert?Retry-Logic? Bei Timeout nochmal versuchen mit weniger Chunks?4. UI-Freezing verhindern:Virtualisierte Liste? Nur sichtbare Messages rendern?Lazy Loading? Alte Messages erst bei Scroll laden?Token-Limit f√ºr Rendering? Max 100k tokens im DOM?5. Race Conditions:AbortController bei Unmount? Request canceln wenn Component verschwindet?State-Management verbessern? Session in App.tsx statt Component?Request-Queue? Nur 1 Request gleichzeitig erlauben?ü§ñ INTELLIGENTE MODELL-AUSWAHL & DUAL-RESPONSE-STRATEGIEPROBLEM: Context-Window Limits vs Qualit√§tModell-√úbersicht (sortiert nach Qualit√§t):RangModelContext-WindowKosten/1MQualit√§tSpezialisierungü•á 1Claude Sonnet 4.5200K$3‚≠ê‚≠ê‚≠ê‚≠ê‚≠êKomplexe Reasoning, Trauma-Analyseü•à 2GPT-4 Turbo128K$10‚≠ê‚≠ê‚≠ê‚≠ê‚≠êAllround, sehr kreativü•â 3Gemini 2.5 Flash1M$0.10‚≠ê‚≠ê‚≠ê‚≠êGro√üe Kontexte, schnell, g√ºnstigDILEMMA:Beste Qualit√§t (Claude) hat kleinstes Context-Window (200K)Gr√∂√ütes Context-Window (Gemini) hat niedrigste Qualit√§tUser hat Prompts bis zu 80k + Context bis zu 500k = 580k Tokens!üéØ L√ñSUNG: INTELLIGENTE KASKADEN-AUSWAHLSTUFE 1: STANDARD-AUSWAHL (Single-Model-Strategy)JavaScriptfunction selectOptimalModel(totalTokens, contextPairs) {
    // Berechne Token-Count f√ºr alle 3 Paare
    const pair1Tokens = calculateSetTokens(contextPairs[0]);
    const pair2Tokens = calculateSetTokens(contextPairs[1]);
    const pair3Tokens = calculateSetTokens(contextPairs[2]);
    const totalContext = pair1Tokens + pair2Tokens + pair3Tokens;
    
    console.log(`üìä Token-Analyse: ${totalContext} Context + ${userPromptTokens} User-Prompt = ${totalTokens} total`);
    
    // INTELLIGENTE AUSWAHL (nach Context-Window):
    
    if (totalTokens <= 128_000) {
        // ‚úÖ Passt in GPT-4 Turbo (128K)
        return {
            model: 'GPT-4 Turbo',
            endpoint: 'https://api.openai.com/v1/chat/completions',
            apiKey: process.env.OPENAI_API_KEY,
            maxTokens: 128_000,
            cost: 10.0,  // $10 pro 1M
            quality: 5,
            reason: 'Beste Qualit√§t bei <128K Context'
        };
    }
    
    if (totalTokens <= 200_000) {
        // ‚úÖ Passt in Claude Sonnet 4.5 (200K)
        return {
            model: 'Claude Sonnet 4.5',
            endpoint: 'https://api.anthropic.com/v1/messages',
            apiKey: process.env.ANTHROPIC_API_KEY,
            maxTokens: 200_000,
            cost: 3.0,  // $3 pro 1M
            quality: 5,
            reason: 'Beste Qualit√§t + Trauma-Spezialisierung bei <200K Context'
        };
    }
    
    // ‚ùå Zu gro√ü f√ºr hochwertige Modelle
    if (totalTokens <= 1_000_000) {
        // ‚úÖ Nur Gemini 2.5 Flash kann 1M
        return {
            model: 'Gemini 2.5 Flash',
            endpoint: 'https://generativelanguage.googleapis.com/v1/models/gemini-2.5-flash',
            apiKey: process.env.GEMINI_API_KEY_1,
            maxTokens: 1_000_000,
            cost: 0.1,  // $0.10 pro 1M
            quality: 4,
            reason: 'Einziges Model mit 1M Context-Window'
        };
    }
    
    // ‚ùå Sogar zu gro√ü f√ºr Gemini ‚Üí Fehler!
    throw new Error(`Context zu gro√ü: ${totalTokens} tokens √ºberschreitet 1M Limit!`);
}
Beispiel-Ablauf (90k Tokens):User-Prompt: "Erz√§hl von den Zwillingen" (20 Tokens)
Context: 3 Paare √ó ~30k = 90k Tokens
Total: 90,020 Tokens

‚Üí 90k < 128k ‚Üí ‚úÖ GPT-4 Turbo ausgew√§hlt
‚Üí Beste Qualit√§t, passt ins Context-Window
STUFE 2: DUAL-RESPONSE-STRATEGIE (Split-Model-Strategy)Wenn Context > 200K f√ºr alle 3 Paare:JavaScriptfunction selectDualModelStrategy(totalTokens, contextPairs) {
    if (totalTokens > 200_000) {
        console.log(`‚ö†Ô∏è Context zu gro√ü f√ºr hochwertige Modelle (${totalTokens} > 200K)`);
        console.log(`üéØ DUAL-RESPONSE-STRATEGIE aktiviert!`);
        
        // 1. W√§hle BESTES Paar (meist PERFECT AGREEMENT)
        const bestPair = contextPairs.filter(p => p.agreement === 'PERFECT')[0] 
                      || contextPairs.sort((a, b) => b.combined_score - a.combined_score)[0];
        
        const bestPairTokens = calculateSetTokens(bestPair);
        
        // 2. Pr√ºfe ob BESTES Paar in hochwertiges Model passt
        if (bestPairTokens <= 128_000) {
            // ‚úÖ Bestes Paar passt in GPT-4
            return {
                strategy: 'DUAL_RESPONSE',
                primaryModel: {
                    model: 'GPT-4 Turbo',
                    pairs: [bestPair],  // Nur 1 Paar
                    tokens: bestPairTokens,
                    cost: 10.0,
                    quality: 5,
                    label: 'ü•á HOCHWERTIG (GPT-4)'
                },
                secondaryModel: {
                    model: 'Gemini 2.5 Flash',
                    pairs: contextPairs,  // ALLE 3 Paare
                    tokens: totalTokens,
                    cost: 0.1,
                    quality: 4,
                    label: 'üìö VOLLST√ÑNDIG (Gemini)'
                },
                parallelExecution: true,  // BEIDE parallel aufrufen
                displayBoth: true         // BEIDE Antworten im Chat zeigen
            };
        }
        
        if (bestPairTokens <= 200_000) {
            // ‚úÖ Bestes Paar passt in Claude
            return {
                strategy: 'DUAL_RESPONSE',
                primaryModel: {
                    model: 'Claude Sonnet 4.5',
                    pairs: [bestPair],  // Nur 1 Paar
                    tokens: bestPairTokens,
                    cost: 3.0,
                    quality: 5,
                    label: 'ü•á HOCHWERTIG (Claude)'
                },
                secondaryModel: {
                    model: 'Gemini 2.5 Flash',
                    pairs: contextPairs,  // ALLE 3 Paare
                    tokens: totalTokens,
                    cost: 0.1,
                    quality: 4,
                    label: 'üìö VOLLST√ÑNDIG (Gemini)'
                },
                parallelExecution: true,
                displayBoth: true
            };
        }
        
        // ‚ùå Sogar bestes Paar zu gro√ü f√ºr hochwertige Modelle
        // ‚Üí Nur Gemini mit allen 3 Paaren
        return {
            strategy: 'SINGLE_RESPONSE',
            primaryModel: {
                model: 'Gemini 2.5 Flash',
                pairs: contextPairs,
                tokens: totalTokens,
                cost: 0.1,
                quality: 4,
                label: 'üìö NUR GEMINI (zu gro√ü f√ºr andere)'
            }
        };
    }
}
Beispiel-Ablauf (350k Tokens):User-Prompt: "Erz√§hl von den Zwillingen" (20 Tokens)
Context: Paar 1 (120k) + Paar 2 (150k) + Paar 3 (80k) = 350k Tokens
Total: 350,020 Tokens

‚Üí 350k > 200k ‚Üí ‚ùå Zu gro√ü f√ºr Claude/GPT-4
‚Üí üéØ DUAL-RESPONSE-STRATEGIE aktiviert!

Paar 1 (PERFECT AGREEMENT): 120k Tokens
‚Üí 120k < 128k ‚Üí ‚úÖ Passt in GPT-4!

STRATEGIE:
‚îú‚îÄ ü•á PRIMARY: GPT-4 Turbo (nur Paar 1 = 120k)
‚îÇ  ‚îî‚îÄ Beste Qualit√§t, fokussiert auf wichtigsten Kontext
‚îî‚îÄ üìö SECONDARY: Gemini 2.5 Flash (alle 3 Paare = 350k)
   ‚îî‚îÄ Vollst√§ndiger Kontext, alle Perspektiven

‚Üí BEIDE parallel aufrufen
‚Üí BEIDE Antworten im Chat anzeigen
üîÑ PARALLELE AUSF√úHRUNG (Backend-Implementation)JavaScriptasync function executeModelStrategy(strategy, userPrompt, contextPairs) {
    if (strategy.strategy === 'SINGLE_RESPONSE') {
        // Normale Ausf√ºhrung (nur 1 Model)
        const response = await callLLM(
            strategy.primaryModel.model,
            userPrompt,
            strategy.primaryModel.pairs
        );
        
        return {
            responses: [{
                model: strategy.primaryModel.model,
                label: strategy.primaryModel.label,
                text: response.text,
                tokens: response.usage.total_tokens,
                cost: response.usage.total_tokens / 1_000_000 * strategy.primaryModel.cost
            }]
        };
    }
    
    if (strategy.strategy === 'DUAL_RESPONSE') {
        // Parallele Ausf√ºhrung (2 Models gleichzeitig)
        console.log('üîÑ Starte DUAL-RESPONSE: 2 Models parallel...');
        
        const [primaryResponse, secondaryResponse] = await Promise.all([
            callLLM(
                strategy.primaryModel.model,
                userPrompt,
                strategy.primaryModel.pairs  // Nur 1 Paar
            ),
            callLLM(
                strategy.secondaryModel.model,
                userPrompt,
                strategy.secondaryModel.pairs  // ALLE 3 Paare
            )
        ]);
        
        console.log('‚úÖ BEIDE Antworten empfangen!');
        
        return {
            responses: [
                {
                    model: strategy.primaryModel.model,
                    label: strategy.primaryModel.label,
                    text: primaryResponse.text,
                    tokens: primaryResponse.usage.total_tokens,
                    cost: primaryResponse.usage.total_tokens / 1_000_000 * strategy.primaryModel.cost,
                    quality: strategy.primaryModel.quality,
                    contextPairs: strategy.primaryModel.pairs.length
                },
                {
                    model: strategy.secondaryModel.model,
                    label: strategy.secondaryModel.label,
                    text: secondaryResponse.text,
                    tokens: secondaryResponse.usage.total_tokens,
                    cost: secondaryResponse.usage.total_tokens / 1_000_000 * strategy.secondaryModel.cost,
                    quality: strategy.secondaryModel.quality,
                    contextPairs: strategy.secondaryModel.pairs.length
                }
            ]
        };
    }
}
üìö REFERENZENHaupt-README: README.md (mit Synapse Genesis Point)Architektur: ARCHITECTURE.json (auto-generiert)Setup: SETUP.mdCleanup Report: docs/CLEANUP_REPORT.mdV1 Reference: c:\evoki\ (Produktiv-System)Letztes Update: 28.12.2025 - Synapse (Explorer & Connector) ‚ö°Discovery Phase: 4/5 - LocalStorage, Startup, Dependencies, Error Handling vollst√§ndigN√§chste Review: Nach erstem erfolgreichen Tempel-Test



---
# üß≠ EXTENSION: UNIFIED MASTER LAYER (V2.2)

## Zweck dieser Erweiterung
Diese Erweiterung transformiert **WHITEBOARD_V2.1_FIXED** von einer korrigierten Spezifikation
zu einem **durchgetakteten Arbeits‚Äë und Entwicklungsdokument**.

**Nichts wird ersetzt oder gek√ºrzt.**
Alles Folgende ist **additiv** und kompatibel zu V2.1.

---

## üß± MASTER‚ÄëPRINZIP
EVOKI wird als **Single‚ÄëUser‚ÄëSystem** betrieben.

Daraus folgt explizit:
- User darf Governance per Prompt oder UI √ºberschreiben
- Overrides sind **explizit**, **sichtbar**, **revisionsf√§hig**
- Overrides erzeugen **keine impliziten Writes**

---

## üîê USER‚ÄëOVERRIDES (NEUE SCHICHT)

### Override‚ÄëTypen
```yaml
override_modes:
  DEFAULT:
    description: "Normale Governance (Decay + Sentinel aktiv)"
  FULL_CONTEXT:
    description: "Nutze gesamten Verlauf vom Ursprung bis jetzt"
  LEGACY_CONTEXT:
    description: "Nutze Kontext > 1 Jahr (Archiv)"
```

### Verbindliche Regeln
- [MUST] Overrides m√ºssen explizit gesetzt sein (UI oder Prompt)
- [MUST] FULL_CONTEXT & LEGACY_CONTEXT sind **READ‚ÄëONLY**
- [MUST NOT] Overrides schreiben in Vector‚Äë oder Memory‚ÄëDBs

---

## üóÇÔ∏è SPEICHER‚ÄëARCHITEKTUR (ERWEITERT)

### Aktive Datenbanken (WRITE)
| Layer | Zweck |
|------|------|
| SQL Source DB | Autoritativer Text |
| Vector DB | Aktiver semantischer Kontext |
| comparison_log.db | Entscheidungen & Governance‚ÄëLogs |

### Analyse‚ÄëArchive (READ‚ÄëONLY)
| Store | Zweck |
|-------|------|
| archive/full_context/ | Gesamthistorie |
| archive/legacy_context/ | > 1 Jahr |
| archive/ad_hoc/ | Tempor√§re Analysen |

Regel:
> **Archive d√ºrfen niemals Write‚ÄëTargets sein.**

---

## ‚è≥ TIME‚ÄëDECAY GOVERNANCE (KLARSTELLUNG)

### Zeitbasis
- Intern: **Millisekunden (ms)** ‚Äì h√∂chste Pr√§zision
- Normalisiert: Tage (d) f√ºr Lesbarkeit

### Default‚ÄëHorizont (Single User)
```yaml
time_horizons:
  NORMAL: 180d
  WARNING: 90d
  CRITICAL: 30d
```

### Decay‚ÄëModelle
```text
EXPONENTIAL (Default):
w = exp(-Œª * Œît)

HYPERBOLIC (Nur bei FULL/LEGACY):
w = 1 / (1 + Œ± * Œît)
```

### Regel
- [MUST] EXPONENTIAL im Default
- [MAY] HYPERBOLIC bei explizitem Override
- [MUST] Sentinel darf Decay versch√§rfen, nie aufheben

---

## üîÅ PIPELINE‚ÄëERWEITERUNG (LOGISCH)

Neue Schritte (vor Retrieval):
1. Parse Overrides
2. Set Read/Write‚ÄëMatrix
3. Select Decay‚ÄëMode
4. Lock Persistence

Danach erst:
‚Üí FAISS / SQL / Model Routing

---

## üß† WARUM DAS KOH√ÑRENT IST

- Kein Context‚ÄëDrift
- Kein Daten‚ÄëVergiften durch alte Zust√§nde
- Volle analytische Freiheit
- Klare Trennung: **Erinnern vs. Analysieren**

---

## üìù ARBEITS‚ÄëTODO (AUTO‚ÄëABLEITUNG)

P0:
- [ ] Override‚ÄëFlags im Request Schema
- [ ] Write‚ÄëBlock bei FULL/LEGACY
- [ ] Archive‚ÄëVerzeichnis anlegen

P1:
- [ ] UI‚ÄëToggle f√ºr Kontext‚ÄëHorizon
- [ ] Decay‚ÄëMode Visualisierung
- [ ] Audit‚ÄëLog f√ºr Overrides

P2:
- [ ] Replay‚ÄëMode auf Archive
- [ ] Vergleich Analyse vs Active Context

---

## STATUS
WHITEBOARD_V2.2_EXTENDED_MASTER:
‚úÖ r√ºckw√§rtskompatibel  
‚úÖ logisch geschlossen  
‚úÖ arbeitsf√§hig  
