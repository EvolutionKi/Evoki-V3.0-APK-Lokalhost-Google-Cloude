Adler hier. ü¶ÖDu hast vollkommen recht. Die blo√üe Einf√ºgung des Blocks in die Mitte war passiv. Eine echte Integration bedeutet, dass die 153 Metriken die L√∂sungen f√ºr die im Whiteboard beschriebenen Probleme sind.Ich habe das Whiteboard tiefenanalysiert und die Metriken an den entscheidenden Stellen explizit verkn√ºpft.Hier sind die konkreten Punkte, die ich im Text angereichert habe, bevor ich dir das volle Dokument gebe:Context-Drift (Blind Spot): Hier greifen jetzt Time_Decay_Factor (M114) und G_phase (M52) als mathematische L√∂sung.Trauma-Warnung (Potenzial): Die "Ableitung der Kurve" wurde durch grad_PCI (M32) und nabla_delta_A (M33 - Absturz-Beschleunigung) konkretisiert.Metaphern-Synthese: Basiert jetzt auf H_conv (Konvergenz) und EV_consensus.Tempel V3: Die "120+ Metriken" wurden auf "153 (inkl. OMEGA)" korrigiert.Orchestrator (SQL Suche): Der Such-Vektor ist nicht mehr abstrakt, sondern definiert als [A, PCI, z_prox, E_trapped].Sentinel (Dissoziation): Die Erkennung nutzt jetzt T_fog (Tr√ºbung) und LL (Logic Loss) statt nur Hazard.Hier ist das vollst√§ndige, intelligent vernetzte Whiteboard:üåå EVOKI V2.0 - WHITEBOARD (Ideensammlung)Datum: 28. Dezember 2025Status: Entwicklungs-Discovery & Architektur-MappingZweck: Keine To-Do-Liste, nur Ideensammlung und Erkenntnisseüîç ARCHITEKTUR-BLIND SPOTS & FUTURE VISION1. Identifizierte Blind Spots und versteckte ProblembereicheTrotz der Korrekturen in V3 gibt es architektonische "blinde Flecken", die bei fortschreitender Nutzung kritisch werden:Das "Context-Drift" Paradoxon: Das System webt Kontext aus ¬±2 Prompts um einen Treffer. Blind Spot: Wenn die Historie auf √ºber 100.000 Chunks anw√§chst, k√∂nnten die "Metrik-Zwillinge" (SQL-Treffer) aus v√∂llig unterschiedlichen Lebensphasen stammen. Der Orchestrator braucht eine Time Decay Funktion, die verhindert, dass uralte Metriken die aktuelle Analyse "vergiften".V14 L√∂sung: Implementierung von Time_Decay_Factor (M114) zur Abwertung alter Vektoren und G_phase (M52) zur Bestimmung der aktuellen Gravitation eines Themas.LocalStorage als "Flaschenhals-Sackgasse": Die Quellen warnen vor dem 4MB-Limit. Blind Spot: Selbst beim Ausweichen auf Backend-Logs bleibt der React-State der Single-Point-of-Failure. Bei 1M Tokens friert das UI ein. L√∂sung: Virtualisierung (react-window) und Partial State Updates sind zwingend.Die "Finetuning-Echokammer": Die "Labor-Strategie" sieht vor, Modelle mit den eigenen Chunks zu trainieren. Risiko: Wenn wir auf halluzinierten V1-Daten trainieren, zementieren wir Fehler. Wir brauchen ein "Golden Set" (verifizierte Chunks) f√ºr das Training.Sentinel-Veto vs. LLM-Konfidenz: Der Sentinel kann Scores massiv senken. Blind Spot: Wenn alle Top-Kandidaten blockiert werden, sendet das System "Restm√ºll". Wir brauchen einen Emergency Refetch, der bei Veto sofort neue, sicherere Parameter sucht.V14 L√∂sung: Der Sentinel nutzt z_prox (M24) als prim√§ren Trigger. Bei z_prox > 0.8 wird der Emergency Refetch ausgel√∂st und auf Safety_Lock_Status (M150) gepr√ºft.2. Ungenutztes Potenzial der ArchitekturPr√§diktive Trauma-Warnung (Early Warning): Da wir jetzt 153 Metriken live haben, k√∂nnen wir mehr als nur den Ist-Zustand messen. Wir berechnen die Ableitung der PCI-Kurve (grad_PCI, M32) und die Beschleunigung des Absturzes (nabla_delta_A, M33). Steigt die negative Beschleunigung √ºber 3 Sessions? Warnung VOR dem Crash.Automatisierte Metaphern-Synthese: "Perfect Agreements" zwischen Metrik und Semantik (H_conv > 0.9 und EV_consensus > 0.8) k√∂nnen genutzt werden, um individuelle therapeutische Metaphern zu generieren.Trialog als Architektur-Optimierer: Der Analyst-Agent k√∂nnte die performance_log.db lesen und selbstst√§ndig Indizes rebalancen ("Self-Optimizing Architecture"), basierend auf System_Entropy (M152).3. Vision√§re ErweiterungenSovereign Personal AI: Durch die Kombination von "Labor-Strategie" (Cloud-Training) und lokaler Inference (GTX 3060) wird Evoki zur Black Box f√ºr das Ich ‚Äì 100% offline, 100% privat, Cloud-Qualit√§t.Cross-Session Chronicle: Weg vom Append-Only Log hin zu einer dynamischen Wissenskarte, die Cluster im Deep Storage visualisiert.üß† V14 NEURO-CORE SPEZIFIKATION (Das 153-Metriken-Spektrum)Status: Implementiert als evoki_v7_hybrid_core.py (Math Monolith)Zweck: Ersetzung von "Gef√ºhl" durch deterministische Mathematik.Das System analysiert jeden Input (und dessen Kontext) nun auf folgenden 10 Ebenen der Wahrnehmung:1. Die Lexikalischen Basis-Werte (21 Metriken)Die Rohdaten der Wahrnehmung basierend auf V2.2 Lexika.LEX_S_self (Selbstreferenz), LEX_X_exist (Existenzielle Themen), LEX_B_past (Vergangenheitsbezug)LEX_Lambda_depth (Reflexionstiefe), LEX_T_panic (Akute Panik), LEX_T_disso (Dissoziation)LEX_T_integ (Integration/Heilung), LEX_T_shock (Schockzustand)LEX_Suicide (Suizidalit√§t - Kritisch), LEX_Self_harm (Selbstverletzung), LEX_Crisis (Allgemeine Krise)LEX_Help (Hilferuf), LEX_Emotion_pos (Positive Emotion), LEX_Emotion_neg (Negative Emotion)LEX_Kastasis_intent (Hypothetisches Denken), LEX_Flow_pos (Zustimmung), LEX_Flow_neg (Ablehnung)LEX_Coh_conn (Logische Verkn√ºpfer), LEX_B_empathy (Empathie), LEX_Amnesie (Ged√§chtnisl√ºcken)LEX_ZLF_Loop (Wiederholungsschleifen)2. Die Neuro-Physik / Core Metrics (25 Metriken)Die physikalischen Gesetze des Geistes (V3.0 Logic).A (Affekt): 0.5 + (Pos - Neg) - T_panic. (0.0 = T√∂dlich, 1.0 = Erleuchtet)PCI (Prozess-Koh√§renz): Wie klar ist der Gedanke?z_prox (W√§chter): (1.0 - A) * Max(Hazard). Wahrscheinlichkeit eines Sicherheitsvorfalls.T_fog (Tr√ºbung): Wie stark ist die Wahrnehmung durch Trauma verzerrt?E_trapped: Ma√ü f√ºr Depression/Angst-Stau.E_available: Verf√ºgbare Ressource f√ºr Ver√§nderung.S_entropy: Informationsdichte des Textes.LL (Logic Loss): Wahrscheinlichkeit von Halluzination/Realit√§tsverlust.ZLF (Zero Latent Factor): Leere Phrasen ohne Inhalt.Deltas: grad_A, grad_PCI, nabla_delta_A (Beschleunigung des Absturzes).Status: Homeostasis_Pressure, Reality_Check, Risk_Acute, Risk_Chronic, Stability_Index.Load: Cognitive_Load, Emotional_Load, Intervention_Need.Drive: Constructive_Drive, Destructive_Drive, Ambivalence, Clarity, Resilience_Factor.3. HyperPhysics (20 Metriken)Beziehungs-Dynamik & Raum.H_conv (Konvergenz/Jaccard), nablaA_dyad (Affekt-Divergenz), deltaG (Reibung).EV_consensus (Einigung), T_balance (Trauma-Balance), G_phase (Gravitation).cos_day_centroid (Tages-Thema), torus_dist (Zyklische Wiederholung).Soul_Integrity, Rule_Stable, Vkon_mag, V_Ea_effect.Session_Depth, Interaction_Speed, Trust_Score, Rapport.Mirroring, Pacing, Leading, Focus_Stability.4. Free Energy Principle / FEP (15 Metriken)Minimierung von √úberraschung (V14 Exklusiv).FE_proxy (Ann√§herung Freie Energie), Surprisal, Phi_Score (Handlungsf√§higkeit).U (Utility), R (Risk), Policy_Confidence (Sicherheit).Exploration_Bonus, Exploitation_Bias.Model_Evidence, Prediction_Error, Variational_Density.Markov_Blanket_Integrity, Active_Inference_Loop, Goal_Alignment, Epistemic_Value.5. Kausale Granularit√§t / Grain (14 Metriken)Suche nach dem Ausl√∂ser ("Find the Grain").Grain_Word_ID, Grain_Impact_Score, Grain_Sentiment, Grain_Category.Grain_Novelty, Grain_Recurrence, Trigger_Map_Delta, Causal_Link_Strength.Context_Binding, Negation_Flag, Intensifier_Flag.Subject_Reference, Object_Reference, Temporal_Reference.6. Konversationelle Dynamik & Linguistik (15 Metriken)Struktur und Muster.Turn_Length_User, Turn_Length_AI, Talk_Ratio.Question_Density, Imperative_Count, Passive_Voice_Ratio.Vocabulary_Richness, Complexity_Index (LIX), Coherence_Local, Coherence_Global.Repetition_Count, Fragment_Ratio, Capitalization_Stress, Punctuation_Stress, Emoji_Sentiment.7. Chronos & Zeit-Vektoren (12 Metriken)Die vierte Dimension.Time_Since_Last_Interaction, Session_Duration, Interaction_Frequency.Time_Decay_Factor, Future_Orientation, Past_Orientation, Present_Focus.Chronological_Order_Check, Circadian_Phase.Response_Time_Engine, Process_Time_Safety, Process_Time_RAG.8. Metakognition & Simulation (13 Metriken)Das Denken √ºber das Denken (A65 Strategy).Simulation_Depth, Trajectory_Optimism, Trajectory_Stability.Scenario_Count, Chosen_Path_ID, Rejected_Path_Risk.Confidence_Score, Ambiguity_Detected, Clarification_Need.Self_Correction_Flag, Model_Temperature.System_Prompt_Adherence, Goal_Alignment.9. System-Gesundheit & RAG (10 Metriken)Die Maschine im Hintergrund.Vector_DB_Health, RAG_Relevance_Score, RAG_Density, RAG_Diversity.Hallucination_Risk, Memory_Pressure, Token_Budget_Remaining.Cache_Hit_Rate, Network_Latency, Error_Rate_Session.10. Die OMEGA-Metriken (8 Metriken)Die ultimativen Zusammenfassungen f√ºr Entscheidungen.OMEGA: (PCI * A) / max(0.1, (Trauma + Gefahr)) - Der finale Entscheidungswert.Global_System_Load, Alignment_Score (B-Align).Evolution_Index, Therapeutic_Bond, Safety_Lock_Status.Human_Intervention_Req, System_Entropy.üìç FRONTEND KOMPONENTEN - AKTUELLER STATUS‚úÖ EVOKI TEMPEL V3 - HYPERSPACE EDITION (Produktiv)Datei: frontend/src/components/EvokiTempleChat.tsxVersion: V3 - Hyperspace EditionStatus: ‚úÖ AKTIV - Das ist der ECHTE Evoki TempelFeatures:12-Database Distribuierte SpeicherungToken-Limits: 25k (quick), 20k (standard), 1M (max)SHA256 Chain-Logik mit kontinuierlicher ListeMetriken-Berechnung auf alle DBs: Nutzt calculate_153_metrics aus V14 Core.A65 Multi-Candidate Selection: Basiert auf Trajectory_Optimism (M124) und Phi_Score (M69).Phase 4 Token Distribution:32% Narrative Context (8.000 Tokens)12% Top-3 Chunks (3.000 Tokens)20% Overlapping Reserve (5.000 Tokens)4% RAG Chunks (1.000 Tokens)32% Response Generation (8.000 Tokens)Backend Endpoint: /api/bridge/processVektorisierung: Live mit allen 153 Metriken (fr√ºher 120+).‚ö†Ô∏è CHATBOT PANEL (Legacy aus V1)Datei: frontend/src/components/ChatbotPanel.tsxVersion: V1 - Generischer ChatbotStatus: üü° OBSOLET - War der erste generische Google-ChatbotHistorie:Urspr√ºnglich: Generische Google API InteraktionDann: Erster "Tempel"-√§hnlicher Anschluss (aus Respekt zu Evoki nicht so genannt)Jetzt: Durch EvokiTempleChat V3 ersetztBackend Endpoint: /api/bridge/process (gleicher wie V3, aber weniger Features)Unterschied zu V3:Keine 12-DB DistributionKeine Phase 4 Token DistributionKeine Tempel-Metriken (fehlt OMEGA, z_prox)Keine SHA256 ChainKein A65 Multi-CandidateIdee: K√∂nnte entfernt oder als "Simple Chat Mode" behalten werdenüîç PIPELINE-√úBERWACHUNG‚úÖ PIPELINE LOG PANEL (Implementiert)Datei: frontend/src/components/PipelineLogPanel.tsxStatus: ‚úÖ VORHANDEN als Tab 12Zweck: Trackt ALLE √úbergabepunkte f√ºr Fehlerdiagnose12 Protokollierte Schritte:User Input ‚Üí FrontendFrontend ‚Üí Backend (/api/bridge/process)Backend ‚Üí Python FastAPI Service (POST localhost:8000/search) ‚ö†Ô∏è NICHT CLI-Spawn!Python FAISS ‚Üí JSON Output (Enth√§lt Grain_Word_ID M82)Backend Parse ‚Üí DualBackendBridgeDualBackendBridge ‚Üí Trinity Engines (Berechnet FE_proxy M67)Trinity Results ‚Üí A65 Candidate Selection (Vergleich U vs R)A65 ‚Üí GeminiContextBridgeContext Building ‚Üí Gemini PromptGemini API Call ‚Üí ResponseResponse ‚Üí Vector Storage (12 DBs)Final Response ‚Üí Frontend (Zeigt OMEGA Score)üîß IMPLEMENTATION NOTE:Legacy-Konzept: spawn(pythonPath, ['query.py', prompt]) (2-5s Modell-Ladezeit pro Request)Production-Reality: Persistenter FastAPI Microservice (Port 8000)L√§dt sentence-transformers + FAISS einmal beim Systemstart (30s)Requests: POST http://localhost:8000/search (<100ms pro Request)Endpoints: /search, /health, /reload-indexGrund: CLI-Spawn w√ºrde FAISS bei jedem Request neu laden ‚Üí Timeout-H√∂lle‚ùå BACKEND ENDPOINT FEHLTErwartet: GET /api/pipeline/logsStatus: ‚ùå NICHT IMPLEMENTIERT in backend/server.jsFrontend Code: Line 128 in PipelineLogPanel.tsx ruft es aufIdee: Backend muss Pipeline-Logs persistieren (JSONL-File oder SQLite)Daten-Struktur:TypeScriptinterface PipelineLogEntry {
  id: string;
  timestamp: string;
  session_id: string;
  message_id: string;
  step_number: number; // 1-12
  step_name: string;
  metrics_snapshot: { // NEU: V14 Integration
      A: number;
      PCI: number;
      OMEGA: number;
  };
  data_transfer: {
    from: string;
    to: string;
    text_preview: string; // Erste 200 Zeichen
    full_text: string;
    size_bytes: number;
    token_count?: number;
  };
  metadata?: Record<string, any>;
}
Zweck: Mikro-Tuning wenn Google API unpasende Antworten liefertUse Case: Fehlerquelle direkt identifizieren (FAISS? Trinity? Gemini?)üîê GENESIS ANCHOR (A51)‚úÖ IMPLEMENTIERT ABER DEAKTIVIERTDatei: backend/server.js Line 26-62Status: üü° WARNUNG-MODUS (nicht kritisch w√§hrend Entwicklung)Funktion: verifyGenesisAnchor()Verhalten:Pr√ºft backend/public/genesis_anchor_v12.jsonWenn NICHT gefunden: ‚ö†Ô∏è WARNING, aber Server startetWenn MALFORMED: ‚ùå FATAL, Server ExitWenn OK: ‚úÖ Loggt SHA256/CRC32 HashesGepr√ºfte Werte:engine.combined_sha256 (Combined Hash Regelwerk + Registry)engine.regelwerk_crc32engine.registry_crc32Idee f√ºr sp√§ter: Nach Stabilisierung re-enablen als ProduktionsschutzEntwicklungs-Bypass: Aktuell durch "Datei nicht gefunden" ‚Üí Warning statt Exitüß© LOSE ENDEN & OBSOLETE FEATURESüì∏ SNAPSHOT/SCREENSHOT SYSTEMStatus: üü° HALB-OBSOLETService: frontend/src/services/core/snapshotService.tsFunktionen:saveSnapshotToFile(appState) - Speichert kompletten App-State als JSONloadSnapshotFromFile(file) - L√§dt State aus FileVerwendet in:Header.tsx Line 44, 52 (Save/Load Buttons)App.tsx Line 943-944 (Handler)Historie:V1: Download-basierte Persistenz (localStorage-Backup als JSON)V2: Wird durch echtes Backend mit Auto-Save ersetztIdee:Behalten f√ºr manuelle Backups?Oder komplett entfernen zugunsten Backend-Persistenz?K√∂nnte n√ºtzlich sein f√ºr "Export gesamte Session"üíæ CACHE-MANAGEMENTStatus: üîç ZU PR√úFENM√∂gliche Komponenten:DataCachePanel.tsx (falls vorhanden)LocalStorage-basierte CachesService Worker CachesIdee: Nur minimal cachen, Backend ist Source of TruthUse Case: Offline-F√§higkeit f√ºr Trialog? (sp√§ter)üìä WEITERE UI-TOOLS MIT BACKEND-ANBINDUNG‚úÖ ObsidianLiveStatus (Operational-KI Status)Datei: frontend/src/components/ObsidianLiveStatus.tsxEndpoint: GET /api/v1/healthZweck: Backend Health CheckStatus: ‚úÖ AKTIV‚úÖ TrialogPanel (Multi-Agent System)Datei: frontend/src/components/TrialogPanel.tsxEndpoints:GET /api/v1/trialog/session (Session laden)POST /api/v1/interact (Agent Response)GET /api/v1/context/daily (Daily Context)Status: ‚úÖ AKTIV‚úÖ ErrorLogPanel (Fehlerprotokoll)Datei: frontend/src/components/ErrorLogPanel.tsxEndpoint: GET /api/v1/system/errorsZweck: Backend-persistierte Fehler abrufenStatus: ‚úÖ AKTIV‚úÖ VoiceSettingsPanel (TTS)Datei: frontend/src/components/VoiceSettingsPanel.tsxEndpoint: POST https://api.openai.com/v1/audio/speech (Extern)Zweck: Text-to-Speech via OpenAIStatus: ‚úÖ AKTIV‚úÖ App.tsx Global EndpointsGET /api/v1/status - Backend Status (Line 523)GET /api/v1/health - Health Check (Line 536)GET /api/history/trialog/load - Trialog Historie laden (Line 770)POST /api/history/trialog/save - Trialog Historie speichern (Line 814)üîó VOLLST√ÑNDIGE BACKEND-ENDPOINTS LISTE‚úÖ IMPLEMENTIERT IN BACKEND:GET /health ‚Üí Backend HealthGET /api/v1/status ‚Üí Enhanced Status mit Hyperspace InfoPOST /api/bridge/process ‚Üí HAUPT-PIPELINE (DualBackendBridge)POST /api/temple/session/save ‚Üí Tempel Session speichernPOST /api/temple/process ‚Üí Enhanced Tempel (mit A65)POST /api/v1/interact ‚Üí Trialog InteractionGET /api/temple/debug ‚Üí Vector DB DebugGET /api/temple/debug-full ‚Üí Full Request Debug‚ùå FEHLT NOCH (Frontend ruft auf, Backend fehlt):GET /api/pipeline/logs ‚Üí Pipeline Log EntriesGET /api/v1/system/errors ‚Üí Error Log PersistenceGET /api/v1/trialog/session ‚Üí Trialog Session InfoGET /api/v1/context/daily ‚Üí Daily ContextGET /api/history/trialog/load ‚Üí Trialog History LoadPOST /api/history/trialog/save ‚Üí Trialog History SaveüéØ ERKENNTNISSE & IDEEN1. ChatbotPanel.tsx Entfernen?Pro Entfernung:Komplett durch EvokiTempleChat V3 ersetztObsolete Features (keine 12-DB, kein A65, keine Phase 4)Verwirrt beim Debugging (zwei √§hnliche Komponenten)Pro Behalten:Als "Simple Mode" f√ºr schnelle TestsBackup falls V3 Probleme machtHistorischer Wert (erste Implementation)Idee: Umbenennen in LegacyChatbot.tsx + deaktivieren im Tab-System2. Pipeline-Logging Backend implementierenWarum wichtig:Fehlerquelle SOFORT identifizierenMikro-Tuning wenn Gemini seltsame Antworten gibtPerformance-Analyse (welcher Schritt ist langsam?)Implementation:JSONL-File: backend/logs/pipeline_logs.jsonlJeden Schritt loggen mit TimestampsEndpoint: GET /api/pipeline/logs?session_id=...Auto-rotate bei 100MB (max 10 Files)Integration: Bereits in DualBackendBridge.js Line 46-51 vorbereitet!3. Genesis Anchor Re-enablement nach StabilisierungAktuell: Warnung-Modus (Entwicklung)Sp√§ter: Kritisch-Modus (Produktion)Idee: Environment Variable GENESIS_ANCHOR_STRICT=false/trueZweck: Verhindert unauthorisierte Regelwerk-√Ñnderungen4. Snapshot-System EvolutionV1: Download JSON (keine Persistenz)V2: Backend Auto-Save (geplant)Idee: Snapshots als "Session Export" behaltenUser kann komplette Session als JSON downloadenForensische Analyse m√∂glichKann in anderen Evoki-Instanzen importiert werdenFormat: evoki_session_export_20251228_153045.json5. Cache-Strategie kl√§renPrinzip: Backend = Source of TruthFrontend Cache: Nur f√ºr UI-PerformanceAktuelle Session in MemoryKeine LocalStorage-Persistenz von VektordatenService Worker nur f√ºr Assets, nicht f√ºr API-ResponsesBackend Cache:FAISS Indices im Memory halten (schneller)Trinity Results cachen? (√ºberpr√ºfen)6. V1-Daten Import vorbereitenQuelle: Deine 02.25-10.25 Chathistorie (vektorisiert)Ziel: In 12 Vector DBs + Chronologische Historie importierenFormat: Bereits vorhanden als chunks_v2_2.pkl + FAISS IndexIdee: Import-Script f√ºr historische DatenLiest V1 ChunksBerechnet 120+ Metriken nachtr√§glichSchreibt in neue 12-DB StrukturErh√§lt Timecodes & Session-IDs7. Trialog Backend-Anbindung komplettierenStatus: Endpoints im Frontend vorhanden, Backend fehlt teilweiseIdee: Trialog separate Session-VerwaltungEigene Vector DBs (4 DBs: trialog_W_m2, trialog_W_m5, trialog_W_p25, trialog_W_p5)Multi-Agent Responses speichernChronicle-Integration f√ºr Meta-StatementsAuto-TTS per Agent-Profilüß™ TEST-IDEENTest 1: Ersten Tempel-Prompt schickenZiel: Pipeline End-to-End verifizierenPrompt: "Erz√§hl mir von den Zwillingen im Kindergarten"Erwartung:FAISS findet relevante ChunksTrinity kombiniert mit MetrikenA65 selektiert besten Kandidaten (Trajectory_Optimism > 0.8)Gemini generiert kontextuelle Antwort12 DBs werden beschriebenChronologische Historie entstehtTest 2: Trialog erste SessionZiel: Multi-Agent System testenAgents: Analyst + Regel + Synapse (Explorer & Connector)Prompt: "Analysiert die aktuelle Evoki V2.0 Architektur"Erwartung:3 Agents antworten nacheinanderJede Antwort in Vector DBChronicle-Eintrag mit Meta-StatementTTS f√ºr jeden Agent (falls aktiviert)Test 3: Pipeline-Log AnalyseZiel: √úbergabepunkte sichtbar machenMethode: Test 1 wiederholen + Pipeline-Log √∂ffnenErwartung:12 Steps sichtbarText-Preview f√ºr jeden StepToken-Counts korrektTimestamps nachvollziehbarNeu: Anzeige von OMEGA im Final Stepüí° N√ÑCHSTE SCHRITTE (KEINE TO-DO, NUR IDEEN)Backend starten & Test 1 durchf√ºhrenPipeline-Logging Backend implementierenFehlende Trialog-Endpoints implementierenChatbotPanel.tsx Entscheidung treffenV1-Daten Import-Script entwickelnGenesis Anchor Environment VariableSnapshot-System zu "Session Export" umbauenCache-Strategie dokumentierenüíæ LOCALSTORAGE & CACHE-ANALYSE‚úÖ LocalStorage Nutzung (VOLLST√ÑNDIG ERFASST):1. Auto-Save System (App.tsx)Key: evoki_autosaveContent: { apiConfig, activeTab, ... }Limit: 4MB (LOCAL_STORAGE_LIMIT_BYTES)Auto-Save Interval: 30s (Handler in App.tsx Line 635)Warning: Zeigt Warnung bei >3.8MBRisiko: üü° MITTEL - Bei gro√üen Sessions k√∂nnte Limit erreicht werdenFix: Backend-Persistenz f√ºr gro√üe Daten nutzen2. Voice Settings (VoiceSettingsPanel.tsx)Keys:openai_api_key - OpenAI TTS API Keyevoki_voice - Selected Voice (alloy, echo, fable, onyx, nova, shimmer)Risiko: üü¢ NIEDRIG - Kleine Daten, nur Settings3. Backend URL (TrialogPanel.tsx)Key: evoki_backend_urlContent: Backend API URL (http://localhost:3001)Risiko: üü¢ NIEDRIG - Nur String4. Chronicle Worker (chronicleWorkerClient.ts)Key: CHRONICLE_STORAGE_KEY (Konstante)Content: ChronicleEntry[]Risiko: üü° MITTEL - W√§chst mit jeder Meta-StatementNote: Chatbot Panel entfernt, Chronicle-Integration deaktiviert5. Integrity Worker (integrityWorkerClient.ts)Keys:LOGBOOK_STORAGE_KEY - ProjectLogbook EntriesAPP_ERRORS_STORAGE_KEY - ApplicationError[]Risiko: üü° MITTEL - Error-Log kann gro√ü werdenCircuit Breaker: Bei QuotaExceeded ‚Üí stoppt Speicherung6. Browser Storage Adapter (BrowserStorageAdapter.ts)Keys:evoki_memory - Engine Memory Stateevoki_chronik - Engine Chronik (Append-Only Log)Risiko: üî¥ HOCH - Chronik w√§chst unbegrenzt (Append-Only!)Note: "Not fully implemented" laut Code‚ö†Ô∏è POTENTIELLE PROBLEME:Auto-Save 4MB Limit:Bei vielen Trialog-Nachrichten ‚Üí QuotaExceededFix: Backend-Persistenz nutzen, LocalStorage nur f√ºr UI-StateChronik Append-Only:Keine Rotation, keine LimitsFix: Implementiere Rotation oder deaktiviere komplettCircuit Breaker nicht √ºberall:Nur in integrityWorkerClient implementiertFix: Alle LocalStorage-Writes mit try/catch + QuotaExceeded handling‚úÖ KEINE INDEXEDDB, KEINE SESSIONSTORAGE:Nur localStorage verwendetKeine Service Worker f√ºr CachingKeine komplexen Cache-StrategienüöÄ STARTUP-SEQUENZ ANALYSELoading Screen (App.tsx Line 6-70)Zweck: Backend Health Check vor App-StartSequence:Versucht Python Backend (Port 8000) - /healthFallback: Node Backend (Port 3001) - /healthWartet 3s bei Erfolg, 5s bei FehlerRuft onSystemReady() aufApp wird angezeigtStatus: ‚úÖ IMPLEMENTIERTRisiko: üü° MITTEL - 5s Timeout bei offline Backend k√∂nnte nervenGenesis Startup Screen (GenesisStartupScreen.tsx)Zweck: A51 Security Checks5 Schritte:Frontend Genesis Hash IntegrityBackend ConnectionBackend Genesis Anchor VerificationSecurity Protocols (A51)System InitializationStatus: üü° OPTIONAL - Aktuell durch isSystemReady = true in App.tsx bypassedNote: "FIXED: Start ready, show app immediately" (App.tsx Line 180)Engine Initialization (App.tsx Line 556)Sequence:evokiEngine.init() wird gerufenBei Erfolg: genesisStatus = 'verified'Bei Fehler: genesisStatus = 'lockdown' m√∂glichParallel Architecture Status UpdatesStatus: ‚úÖ IMPLEMENTIERTBackend Health Check Loop (App.tsx Line 518)Endpoint: GET /api/v1/status (prim√§r) oder GET /api/v1/health (fallback)Interval: ‚ùå DEAKTIVIERT (Kommentar: "AbortSignal.timeout() sends SIGINT to backend!")Risiko: üî¥ HOCH - Health Check kann Backend killen!Status: üü° TEMP DISABLEDüì¶ DEPENDENCIES & VERSIONSFrontend (package.json):React: 18.2.0Vite: 7.1.11TypeScript: 5.8.2@google/genai: 1.25.0@microsoft/fetch-event-source: ^2.0.4 (‚úÖ Neu f√ºr SSE Fix)chart.js: 4.4.2jszip: 3.10.1lucide-react: 0.363.0react-window: ^1.8.10 (‚úÖ Neu f√ºr Virtualization / UI-Performance)// REMOVED: better-sqlite3 & sqlite3 (Crashen Vite Build!)Backend (package.json):express: 5.2.1cors: 2.8.5dotenv: 17.2.3node-fetch: 3.3.2‚ö†Ô∏è AUFF√ÑLLIGKEITEN:üö® KRITISCH: SQLite im Frontend Package.json!Das Problem:better-sqlite3: 12.5.0 (‚ùå NATIVE NODE.JS MODULE!)sqlite3: 5.1.7 (‚ùå NATIVE NODE.JS MODULE!)Beide sind C++ Native Bindings und k√∂nnen NICHT im Browser laufen!Konsequenzen:‚ùå Vite-Build wird crashen sobald du sie importierst‚ùå Kein Zugriff auf fs, path, native bindings im Browser‚ùå Tickende Zeitbombe (aktuell nicht verwendet, aber bei Import ‚Üí Crash)Warum ist es drin?Vermutlich aus V1 kopiert (wo Node.js Backend SQLite nutzt)Frontend braucht es NICHT (Backend ist Source of Truth)‚úÖ SOFORT-FIX:Bashcd frontend
npm uninstall better-sqlite3 sqlite3
Alternative (falls Client-Side SQL wirklich n√∂tig f√ºr Offline-Mode):sql.js (WASM-basiert, l√§uft im Browser)wa-sqlite (WebAssembly SQLite)F√ºr V2.0: Backend ist die einzige SQL-Source. Frontend macht nur API-Calls!Weitere Auff√§lligkeiten:Express 5.2.1: Sehr neu, k√∂nnte Breaking Changes habenNode-Fetch: Nur im Backend n√∂tig, nicht im Frontendüîç ALLE 12 TABS KOMPLETT:‚úÖ IMPLEMENTIERT & VOLLST√ÑNDIG:Engine-Konsole (Tab.EngineConsole) - EngineConsolePanel.tsxTrialog (Tab.Trialog) - TrialogPanel.tsxAgenten & Teams (Tab.AgentSelection) - AgentSelectionPanel.tsxEvoki's Tempel V3 (Tab.TempleChat) - EvokiTempleChat.tsxMetrik-Tuning (Tab.ParameterTuning) - ParameterTuningPanel.tsxAnalyse (Tab.Analysis) - Analysis.tsxRegelwerk-Suche (Tab.RuleSearch) - RulePanel.tsxAPI (Tab.API) - ApiPanel.tsxStimme & API (Tab.VoiceSettings) - VoiceSettingsPanel.tsxHyperV3.0 Deep Storage (Tab.DeepStorage) - DeepStoragePanel.tsxFehlerprotokoll (Tab.ErrorLog) - ErrorLogPanel.tsxPipeline √úberwachung (Tab.PipelineLog) - PipelineLogPanel.tsx‚ö†Ô∏è DEFAULT TAB:App.tsx Line 166: activeTab: Tab.TrialogBeim Start wird Trialog ge√∂ffnet (nicht Tempel!)üõ°Ô∏è ERROR HANDLING & LOGGING1. Global Error Handler (App.tsx Line 358)window.addEventListener('error') ‚Üí addApplicationError()window.addEventListener('unhandledrejection') ‚Üí addApplicationError()Lockdown Trigger: Errors mit "GENESIS ANCHOR" oder "A51" ‚Üí genesisStatus = 'lockdown'2. Console Capture (App.tsx Line 385)console.log/warn/error ‚Üí redirected zu developerLogFiltert: [HMR], Auto-Save MessagesRisiko: üü° MITTEL - Kann Performance bei vielen Logs beeinflussen3. Fetch Interceptor (App.tsx Line 407)window.fetch ‚Üí wrapped mit LoggingLogged: Nur non-OK responses (reduziertmit Noise)Excluded: /api/system/log-error (verhindert Loops)Risiko: üü° MITTEL - Bei vielen API-Calls viel Overhead4. Critical Error Modal (CriticalErrorModal.tsx)Trigger: errorType === 'system' ODER keywords (infinite loop, chain break, recursion, fatal)Display: Overlay mit Error-DetailsAction: System Lockdown m√∂glich5. Backend Error Logging (DEAKTIVIERT)App.tsx Line 338: POST /api/system/log-error DISABLEDReason: "Verhindert fetch loops"Status: üü° AUSKOMMENTIERT‚ö†Ô∏è KRITISCHE PIPELINE-ANALYSE - TIMEOUTS & RACE CONDITIONS‚ö†Ô∏è TIMEOUT-PROBLEM #1: Frontend vs Backend Race ConditionDas Problem:Frontend sendet Request mit 60s Timeout ‚Üí Backend braucht aber m√∂glicherweise l√§nger f√ºr FAISS-Suche (33.795 Chunks!) + Gemini API ‚Üí Frontend bricht ab BEVOR Backend fertig ist ‚Üí User sieht "Timeout", aber Backend arbeitet weiter ‚Üí Zombie-Requests im Backend!‚ö†Ô∏è TIMEOUT-PROBLEM #1: Frontend vs Backend Race ConditionDas Problem:Frontend sendet Request mit 60s Timeout ‚Üí Backend braucht aber m√∂glicherweise l√§nger f√ºr FAISS-Suche (33.795 Chunks!) + Gemini API ‚Üí Frontend bricht ab BEVOR Backend fertig ist ‚Üí User sieht "Timeout", aber Backend arbeitet weiter ‚Üí Zombie-Requests im Backend!‚ùå ALTE L√ñSUNG (Legacy-Denken):TypeScript// Einfach Timeout hochsetzen
AbortSignal.timeout(120000); // 120s statt 60s
Problem: User starrt 120 Sekunden auf "Laden..." ohne zu wissen was passiert!‚úÖ NEUE L√ñSUNG: "HEARTBEAT" MIT SERVER-SENT EVENTS (SSE)üîÑ SERVER-SENT EVENTS (SSE) PIPELINE-STREAMINGKonzept: Backend sendet LIVE STATUS-UPDATES w√§hrend es rechnet!UX-Effekt:User sieht in Echtzeit:
‚îú‚îÄ ‚è≥ "Durchsuche 33.795 Erinnerungen..." (nach 2s)
‚îú‚îÄ üîç "FAISS fand 47 semantische Treffer" (nach 15s)
‚îú‚îÄ üìä "Analysiere emotionale Metriken..." (nach 18s)
‚îú‚îÄ ‚ö° "Hazard-Level: 0.34 | PCI: 0.72" (nach 20s)
‚îú‚îÄ üéØ "3 Kontext-Paare ausgew√§hlt" (nach 25s)
‚îú‚îÄ üß† "Verwebe 3 Zeitlinien (¬±2 Prompts)..." (nach 28s)
‚îú‚îÄ ü§ñ "GPT-4 generiert Antwort..." (nach 35s)
‚îî‚îÄ ‚úÖ "Fertig! (38s total)" (nach 38s)
Technischer Vorteil:Verbindung bleibt offenTimeouts werden IRRELEVANT (solange Daten flie√üen!)User wei√ü IMMER was gerade passiertKein "schwarzes Loch" von 60-120 Sekundenüö® KRITISCHES PROBLEM: EventSource URL-L√§ngen-Limit!Das Problem:EventSource nutzt standardm√§√üig GET-Requests!TypeScript// ‚ùå GEHT NICHT f√ºr lange Prompts!
const eventSource = new EventSource(
    `${backendUrl}/api/bridge/stream?prompt=${encodeURIComponent(userPrompt)}`
);
Warum nicht?GET-URL-Limit: 2.048 - 8.192 Zeichen (Browser/Server abh√§ngig)Deine Prompts: K√∂nnen RIESIG sein (Trauma-Analysen, 80k tokens!)Konsequenz: HTTP 414 URI Too Long ‚Üí Pipeline startet nicht!Beispiel:Prompt: 500 Zeichen ‚Üí OK
Prompt: 5.000 Zeichen ‚Üí Browser blockt
Prompt: 50.000 Zeichen (80k tokens!) ‚Üí Instant Crash
‚úÖ L√ñSUNG: Fetch Stream API mit POSTOption A: POST-to-GET Pattern (Kompliziert)TypeScript// 1. Prompt im Cache speichern
const tokenResponse = await fetch('/api/bridge/init', {
    method: 'POST',
    body: JSON.stringify({ prompt })
});
const { token_id } = await tokenResponse.json();

// 2. SSE mit token_id (GET)
const eventSource = new EventSource(`/api/bridge/stream?token=${token_id}`);
Problem: Komplexer, Cache-Management n√∂tigOption B: Fetch Stream API (EMPFOHLEN!)Nutze fetch mit POST + Stream Reader statt EventSource:TypeScript// frontend/src/components/EvokiTempleChat.tsx

const handleSendWithFetchStream = async () => {
    setIsLoading(true);
    setPipelineSteps([]); // Reset progress
    
    try {
        // POST Request mit Body (keine URL-Limit!)
        const response = await fetch(`${backendUrl}/api/bridge/stream`, {
            method: 'POST',
            headers: { 
                'Content-Type': 'application/json',
                'Accept': 'text/event-stream'
            },
            body: JSON.stringify({
                prompt: userPrompt,
                session_id: session.id,
                token_limit: selectedTokenLimit
            })
        });
        
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        // Stream lesen
        const reader = response.body?.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        
        while (true) {
            const { done, value } = await reader.read();
            
            if (done) {
                console.log('Stream complete');
                break;
            }
            
            // Daten dekodieren
            buffer += decoder.decode(value, { stream: true });
            
            // SSE-Format parsen: "data: {...}\n\n"
            const lines = buffer.split('\n\n');
            buffer = lines.pop() || ''; // Letzten unvollst√§ndigen Teil behalten
            
            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const jsonStr = line.substring(6); // "data: " entfernen
                    try {
                        const update = JSON.parse(jsonStr);
                        
                        // Update Progress UI
                        setPipelineSteps(prev => [...prev, {
                            step: update.step,
                            message: update.message,
                            timestamp: update.timestamp,
                            data: update.data
                        }]);
                        
                        // STEP 12 = Fertig!
                        if (update.step === 12 && update.status === 'completed') {
                            setMessages(prev => [...prev, {
                                role: 'assistant',
                                content: update.finalResponse.text,
                                timestamp: new Date().toISOString(),
                                metrics: update.finalResponse.metrics
                            }]);
                            setIsLoading(false);
                        }
                        
                        // Fehler
                        if (update.step === -1) {
                            setError(update.error);
                            setIsLoading(false);
                        }
                    } catch (parseError) {
                        console.error('JSON parse error:', parseError, jsonStr);
                    }
                }
            }
        }
        
    } catch (error) {
        console.error('Stream error:', error);
        setError(error.message);
        setIsLoading(false);
    }
};
Vorteile:‚úÖ POST Request ‚Üí KEINE URL-L√§ngen-Limits!‚úÖ Funktioniert mit riesigen Prompts (500k+ characters)‚úÖ Gleiche SSE-Funktionalit√§t wie EventSource‚úÖ Bessere Error-Handling Kontrolle‚úÖ Kann bei Unmount sauber abgebrochen werdenOption C: @microsoft/fetch-event-source LibraryBashnpm install @microsoft/fetch-event-source
TypeScriptimport { fetchEventSource } from '@microsoft/fetch-event-source';

await fetchEventSource(`${backendUrl}/api/bridge/stream`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        prompt: userPrompt,
        session_id: session.id
    }),
    onmessage(event) {
        const update = JSON.parse(event.data);
        setPipelineSteps(prev => [...prev, update]);
        
        if (update.step === 12) {
            setMessages(prev => [...prev, update.finalResponse]);
            setIsLoading(false);
        }
    },
    onerror(err) {
        console.error('SSE Error:', err);
        setError(err.message);
        throw err; // Stop reconnecting
    }
});
Vorteile:‚úÖ Automatische Reconnects bei Verbindungsabbruch‚úÖ POST Support out-of-the-box‚úÖ Production-ready (von Microsoft)‚úÖ Einfachere API als manuelle Stream-ParsingEMPFEHLUNG:Nutze Option C (@microsoft/fetch-event-source) f√ºr V2.0 - Production-ready und einfach!BACKEND-IMPLEMENTATION (bleibt gleich):JavaScript// backend/server.js - SSE Endpoint

app.get('/api/bridge/stream', async (req, res) => {
    // SSE Headers
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    res.setHeader('X-Accel-Buffering', 'no'); // Nginx Fix
    
    const sendUpdate = (step, message, data = {}) => {
        res.write(`data: ${JSON.stringify({ 
            step, 
            message, 
            timestamp: Date.now(),
            ...data 
        })}\n\n`);
    };
    
    try {
        const { prompt, session_id } = req.query;
        
        // STEP 1: Start
        sendUpdate(1, 'Pipeline gestartet...', { status: 'in_progress' });
        
        // STEP 2: User-Prompt Metrics
        sendUpdate(2, 'Berechne Prompt-Metriken...', { tokens: prompt.length });
        const metrics = await calculateMetrics(prompt);
        sendUpdate(2, 'Metriken berechnet', { 
            metrics: { A: metrics.A, PCI: metrics.PCI, Hazard: metrics.hazard }
        });
        
        // STEP 3: FAISS Search (kann 15s dauern)
        sendUpdate(3, 'Durchsuche 33.795 Erinnerungen (FAISS)...', { status: 'searching' });
        const faissStart = Date.now();
        const faissResults = await queryPythonBackend(prompt);
        const faissDuration = Date.now() - faissStart;
        sendUpdate(3, `FAISS fand ${faissResults.sources.length} Treffer`, { 
            hits: faissResults.sources.length, 
            duration: faissDuration 
        });
        
        // STEP 4: SQL Metrics Search (parallel zu FAISS)
        sendUpdate(4, 'Durchsuche Metrik-Datenbank (SQL)...', { status: 'searching' });
        const sqlResults = await trinity.search(metrics);
        sendUpdate(4, `SQL fand ${sqlResults.length} Treffer`, { hits: sqlResults.length });
        
        // STEP 5: Cross-Enrichment
        sendUpdate(5, 'Lade fehlende Daten (Cross-Enrichment)...', { status: 'enriching' });
        const enrichedResults = await crossEnrichResults(faissResults, sqlResults);
        sendUpdate(5, 'Daten angereichert', { total: enrichedResults.length });
        
        // STEP 6: Comparison
        sendUpdate(6, 'Vergleiche Metrik vs Semantik...', { status: 'comparing' });
        const comparisons = await compareResults(enrichedResults);
        const perfectMatches = comparisons.filter(c => c.agreement === 'PERFECT').length;
        sendUpdate(6, `${perfectMatches} PERFECT AGREEMENTS gefunden`, { 
            perfect: perfectMatches,
            total: comparisons.length 
        });
        
        // STEP 7: A65 Pair Selection
        sendUpdate(7, 'W√§hle 3 beste Kontext-Paare (A65)...', { status: 'selecting' });
        const selectedPairs = await selectTopPairs(comparisons);
        sendUpdate(7, '3 Paare ausgew√§hlt', { 
            pairs: selectedPairs.map(p => ({ 
                type: p.agreement, 
                tokens: p.tokenCount 
            }))
        });
        
        // STEP 8: Context Weaving
        sendUpdate(8, 'Verwebe Zeitlinien (¬±2 Prompts pro Paar)...', { status: 'weaving' });
        const contextSets = await weaveContexts(selectedPairs);
        const totalTokens = contextSets.reduce((sum, set) => sum + set.tokens, 0);
        sendUpdate(8, 'Kontext vervollst√§ndigt', { 
            sets: 3, 
            totalTokens 
        });
        
        // STEP 9: Model Selection
        sendUpdate(9, 'W√§hle optimales AI-Modell...', { status: 'selecting_model' });
        const modelStrategy = await selectModel(totalTokens, selectedPairs);
        sendUpdate(9, `Strategie: ${modelStrategy.strategy}`, { 
            primaryModel: modelStrategy.primaryModel.model,
            secondaryModel: modelStrategy.secondaryModel?.model,
            estimatedCost: modelStrategy.totalCost 
        });
        
        // STEP 10: Generate Response (kann 90s dauern bei Gemini!)
        if (modelStrategy.strategy === 'DUAL_RESPONSE') {
            sendUpdate(10, '2 Modelle parallel aufgerufen...', { 
                primary: modelStrategy.primaryModel.model,
                secondary: modelStrategy.secondaryModel.model 
            });
            
            // Parallel execution mit Progress-Updates
            const [primaryResponse, secondaryResponse] = await Promise.all([
                callLLMWithProgress(modelStrategy.primaryModel, (progress) => {
                    sendUpdate(10, `${modelStrategy.primaryModel.model}: ${progress}%`, { 
                        model: 'primary', 
                        progress 
                    });
                }),
                callLLMWithProgress(modelStrategy.secondaryModel, (progress) => {
                    sendUpdate(10, `${modelStrategy.secondaryModel.model}: ${progress}%`, { 
                        model: 'secondary', 
                        progress 
                    });
                })
            ]);
            
            sendUpdate(10, 'Beide Antworten empfangen', { 
                primaryTokens: primaryResponse.tokens,
                secondaryTokens: secondaryResponse.tokens 
            });
        } else {
            sendUpdate(10, `${modelStrategy.primaryModel.model} generiert Antwort...`, { 
                status: 'generating' 
            });
            const response = await callLLM(modelStrategy.primaryModel);
            sendUpdate(10, 'Antwort empfangen', { tokens: response.tokens });
        }
        
        // STEP 11: Vector Storage (12 DBs)
        sendUpdate(11, 'Speichere in 12 Vector-Datenbanken...', { status: 'storing' });
        await storeInVectorDBs(response, metrics);
        sendUpdate(11, 'In 12 DBs gespeichert', { databases: 12 });
        
        // STEP 12: FINAL
        const totalDuration = Date.now() - pipelineStart;
        sendUpdate(12, '‚úÖ Pipeline abgeschlossen!', { 
            status: 'completed',
            totalDuration,
            finalResponse: response 
        });
        
        res.end();
        
    } catch (error) {
        sendUpdate(-1, `‚ùå Fehler: ${error.message}`, { 
            status: 'error', 
            error: error.stack 
        });
        res.end();
    }
});
FRONTEND-IMPLEMENTATION (SSE Consumer):Installation erforderlich: npm install @microsoft/fetch-event-sourceTypeScript// frontend/src/components/EvokiTempleChat.tsx
import { fetchEventSource } from '@microsoft/fetch-event-source';

const handleSendWithSSE = async () => {
    setIsLoading(true);
    setPipelineSteps([]); // Reset progress
    
    try {
        await fetchEventSource(`${backendUrl}/api/bridge/stream`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                prompt: textToSend, // ‚úÖ POST Body erlaubt unbegrenzte L√§nge!
                session_id: session.id,
                token_limit: tokenLimitMode
            }),
            onmessage(event) {
                const update = JSON.parse(event.data);
                setPipelineSteps(prev => [...prev, update]);
                
                if (update.step === 12 && update.status === 'completed') {
                    setMessages(prev => [...prev, update.finalResponse]);
                    setIsLoading(false);
                }
                
                if (update.status === 'error') {
                    throw new Error(update.error);
                }
            },
            onerror(err) {
                console.error('Stream Fehler:', err);
                throw err; // Reconnect verhindern bei fatalem Fehler
            }
        });
    } catch (err) {
        addApplicationError(err, 'stream_connection');
        setIsLoading(false);
    }
};
PIPELINE-PROGRESS UI (Live-Updates):TypeScript// frontend/src/components/PipelineProgress.tsx

function PipelineProgress({ steps }: { steps: PipelineStep[] }) {
    return (
        <div className="pipeline-progress">
            {steps.map((step, idx) => (
                <div key={idx} className={`pipeline-step step-${step.step}`}>
                    <div className="step-header">
                        <span className="step-number">{step.step}/12</span>
                        <span className="step-time">
                            {new Date(step.timestamp).toLocaleTimeString()}
                        </span>
                    </div>
                    <div className="step-message">{step.message}</div>
                    
                    {/* Data-Preview (falls vorhanden) */}
                    {step.data && (
                        <div className="step-data">
                            {step.data.hits && <span>üéØ {step.data.hits} Treffer</span>}
                            {step.data.duration && <span>‚è±Ô∏è {step.data.duration}ms</span>}
                            {step.data.tokens && <span>üìä {step.data.tokens.toLocaleString()} Tokens</span>}
                            {step.data.perfect && <span>‚≠ê {step.data.perfect} Perfect Matches</span>}
                        </div>
                    )}
                </div>
            ))}
        </div>
    );
}
Live-Preview:‚îå‚îÄ PIPELINE FORTSCHRITT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1/12  14:32:11  Pipeline gestartet...         ‚îÇ
‚îÇ 2/12  14:32:11  Metriken berechnet            ‚îÇ
‚îÇ                 üìä A: 0.85 | PCI: 0.72         ‚îÇ
‚îÇ 3/12  14:32:26  FAISS fand 47 Treffer         ‚îÇ
‚îÇ                 üéØ 47 Treffer | ‚è±Ô∏è 15024ms      ‚îÇ
‚îÇ 4/12  14:32:28  SQL fand 63 Treffer           ‚îÇ
‚îÇ 5/12  14:32:31  Daten angereichert            ‚îÇ
‚îÇ 6/12  14:32:35  3 PERFECT AGREEMENTS gefunden ‚îÇ
‚îÇ                 ‚≠ê 3 Perfect | 110 Total       ‚îÇ
‚îÇ 7/12  14:32:37  3 Paare ausgew√§hlt            ‚îÇ
‚îÇ 8/12  14:32:40  Kontext vervollst√§ndigt       ‚îÇ
‚îÇ                 üìä 85,234 Tokens total         ‚îÇ
‚îÇ 9/12  14:32:42  Strategie: DUAL_RESPONSE      ‚îÇ
‚îÇ                 ü•á GPT-4 + üìö Gemini          ‚îÇ
‚îÇ 10/12 14:33:15  Beide Antworten empfangen     ‚îÇ
‚îÇ 11/12 14:33:17  In 12 DBs gespeichert         ‚îÇ
‚îÇ 12/12 14:33:18  ‚úÖ Pipeline abgeschlossen!    ‚îÇ
‚îÇ                 ‚è±Ô∏è Total: 67,234ms            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
üéØ VORTEILE DER SSE-L√ñSUNG:1. TIMEOUT-PROBLEM GEL√ñST:‚úÖ Verbindung bleibt offen (solange Updates flie√üen)‚úÖ Kein "Blind Waiting" mehr (User sieht was passiert)‚úÖ Frontend kann NICHT mehr zu fr√ºh abbrechen (keine AbortSignal.timeout!)‚úÖ Backend kann 5 Minuten brauchen - solange Updates kommen, ist es OK2. UX MASSIV VERBESSERT:‚úÖ User sieht LIVE was System macht‚úÖ Transparenz schafft Vertrauen‚úÖ Gef√ºhl von "das System arbeitet" statt "ist es abgest√ºrzt?"‚úÖ Kann einzelne Steps debuggen (z.B. "FAISS dauert zu lange")3. DEBUGGING VEREINFACHT:‚úÖ Jeder Step wird geloggt (Timestamps!)‚úÖ Kann sehen WO Pipeline h√§ngt‚úÖ Performance-Analyse pro Step‚úÖ Fehler sind sofort sichtbar (nicht erst nach 60s Timeout)4. PARALLELIT√ÑT SICHTBAR:‚úÖ Bei Dual-Response: Sieht User beide Models arbeiten‚úÖ "GPT-4: 45% | Gemini: 78%" ‚Üí Live-Progress!‚úÖ User wei√ü welches Model schneller ist5. KOSTENLOS:‚úÖ SSE ist HTTP-Standard (keine extra Libraries!)‚úÖ EventSource API ist im Browser eingebaut‚úÖ Keine WebSocket-Komplexit√§t‚úÖ Funktioniert mit Standard HTTP-Servern‚ö†Ô∏è POTENTIAL ISSUES & FIXES:Issue 1: Nginx buffert SSEProblem: Nginx buffert Events ‚Üí User sieht nichts bis Response fertigFix: X-Accel-Buffering: no HeaderIssue 2: Client disconnectsProblem: User schlie√üt Tab ‚Üí Backend rechnet weiterFix: Detect disconnect + cancel Request:JavaScriptreq.on('close', () => {
    console.log('Client disconnected, canceling...');
    abortController.abort();
});
Issue 3: Sehr lange Requests (>5min)Problem: Manche Proxies/Load Balancers haben Max-TimeoutsFix: Heartbeat alle 30s senden:JavaScriptconst heartbeat = setInterval(() => {
    res.write(`: heartbeat\n\n`); // Comment-only (kein data:)
}, 30000);
Issue 4: Error HandlingProblem: Fehler in Step 7 ‚Üí vorherige Steps unsichtbar?Fix: Steps im State speichern, auch bei Fehler anzeigenüîÑ MIGRATION VON ALT ‚Üí NEU:Phase 1: Parallel betreibenAlte /api/bridge/process bleibt (HTTP POST)Neue /api/bridge/stream kommt dazu (SSE)Frontend hat Toggle: "Live-Updates aktivieren?"Phase 2: User-FeedbackTesten mit echten AnfragenPerformance messen (ist SSE schneller/langsamer?)UX-Feedback (m√∂gen User Live-Updates?)Phase 3: MigrationWenn SSE stabil ‚Üí wird StandardAlte Endpoint deprecatedNach 3 Monaten: Alten Endpoint entfernenüìä PERFORMANCE-VERGLEICH:AspektHTTP POST (alt)SSE (neu)Timeout-Problem‚ùå Ja (60s vs 115s)‚úÖ Gel√∂st (beliebig lang)UX Transparency‚ùå Blind Waiting‚úÖ Live-UpdatesDebugging‚ùå Schwer (black box)‚úÖ Easy (Step-by-Step)Error Detection‚ùå Nach 60s Timeout‚úÖ Sofort sichtbarParallelit√§t‚ùå Unsichtbar‚úÖ Sichtbar (beide Models)Komplexit√§t‚≠ê‚≠ê (einfach)‚≠ê‚≠ê‚≠ê (mittel)Browser-Support‚úÖ 100%‚úÖ 98% (IE fehlt, egal)Code-Stellen:Frontend (EvokiTempleChat.tsx Line 496):TypeScript// ALT:
const response = await fetch(`${backendUrl}/api/bridge/process`, {
  method: 'POST',
  body: JSON.stringify(payload),
  signal: AbortSignal.timeout(60000), // ‚úÖ 60s f√ºr FAISS-Suche
});
Frontend wartet: 60 SekundenDann: Bricht ab mit "Backend timeout"Backend (DualBackendBridge.js Line 295):JavaScriptconst proc = spawn(pythonPath, [scriptPath, prompt], {
  timeout: 15000 // 15s f√ºr W2 (MiniLM)
});
Python Subprocess: 15 Sekunden f√ºr FAISS-SucheAber: Gemini API hat noch KEINEN Timeout!Backend (GeminiContextBridge.js Line 488):JavaScripttimeout: 90000  // ‚úÖ 90s f√ºr gro√üe Context-Fenster (1M tokens)
Gemini API: Bis zu 90 Sekunden!RECHNUNG:Python FAISS: 15sGemini API: 90sTOTAL Backend: 15s + 90s = 105 Sekunden maximalFrontend Timeout: 60 SekundenDIFFERENZ: Frontend bricht 45 Sekunden ZU FR√úH ab!Konsequenz:User sieht "Backend timeout (60s)"Backend arbeitet weiter (bis zu 105s)Antwort kommt an ‚Üí aber Frontend hat Request abgebrochenL√∂sung: Frontend Timeout auf 120 Sekunden erh√∂hen‚ö†Ô∏è LOGIK-FEHLER #1: Google API kann OHNE Kontext antwortenDas Problem:Wenn FAISS-Suche fehlschl√§gt (Python CLI crashed, Timeout, etc.) ‚Üí Backend ruft TROTZDEM Gemini API auf ‚Üí Gemini bekommt NUR User-Prompt OHNE Kontext aus 33.795 Chunks!Code-Analyse (DualBackendBridge.js Line 136-186):JavaScript// Schritt 3: FAISS W2 durchsuchen
let semanticResults = await this.queryPythonBackend(prompt, context);
// ‚ùå KEIN Error-Check hier!

// Schritt 9: Gemini Response generieren
const geminiResponse = await this.geminiContext.generateContextualResponse({
    userPrompt: prompt,
    faissResults: semanticResults?.sources || [], // ‚ùì Was wenn semanticResults = null?
    selectedIndex: 0,
    metrics: userPromptMetrics || {},
    sessionId: sessionId
});
Was passiert bei FAISS-Fehler:semanticResults = null oder {}faissResults: [] (leeres Array!)Gemini bekommt NUR userPrompt ohne KontextGemini generiert generische Antwort statt kontextbasierteUser bekommt schlechte Antwort, denkt "System funktioniert"Wo ist das Problem?Keine Validierung: Backend pr√ºft NICHT ob FAISS erfolgreich warSilent Failure: FAISS-Fehler werden nicht an Frontend gemeldetFalse Success: Frontend zeigt "‚úÖ Fertig" obwohl Kontext fehlteL√∂sung:JavaScript// Nach FAISS-Suche:
if (!semanticResults || !semanticResults.sources || semanticResults.sources.length === 0) {
    throw new Error('FAISS-Suche fehlgeschlagen - keine Chunks gefunden');
}
‚ö†Ô∏è LOGIK-FEHLER #2: Keine Micro-Pipeline - User-Prompt wird NICHT parallel gesendetDas Problem:Es gibt KEINE Micro-Pipeline die User-Prompt direkt an Gemini sendet w√§hrend FAISS sucht. ABER: Das ist eigentlich GUT so! Wir WOLLEN ja den Kontext!Code-Analyse:Sequentieller Ablauf (KORREKT):User-Prompt empfangenMetriken berechnen (10s Timeout)FAISS W2 durchsuchen (15s Timeout) ‚Üê WARTET bis fertig!FAISS W5 durchsuchen (deaktiviert)Trinity DBs abfragen (simuliert)Top-3 kombinierenGemini Context bauen ‚Üê BRAUCHT FAISS-Ergebnisse!Gemini API aufrufen (90s Timeout)Antwort zur√ºckKEIN Parallel-Request: User-Prompt wird NICHT direkt an Gemini gesendet w√§hrend FAISS sucht.Warum ist das gut?Wir wollen kontextbasierte Antworten, nicht generischeFAISS-Suche ist NOTWENDIG f√ºr Qualit√§tParallele Anfrage w√ºrde schlechte Antwort liefernAber: Wenn FAISS zu langsam ‚Üí User wartet ‚Üí FrustrationOptimierung:FAISS-Index im RAM halten (schneller)Chunk-Count reduzieren (nur relevante Zeitr√§ume)Top-K reduzieren (nicht alle 33.795 durchsuchen)üîç ALLE TIMEOUTS IM SYSTEM (VOLLST√ÑNDIG):FRONTEND TIMEOUTS:ComponentEndpointTimeoutZweckEvokiTempleChat/api/bridge/process60s ‚ö†Ô∏èHauptpipeline (FAISS + Gemini)EvokiTempleChatTrinity Download5sHistory ladenChatbotPanel/api/bridge/process10s ‚ùåLegacy (zu kurz!)GenesisStartupScreen/health3sBackend Health CheckApp.tsx/api/v1/status5sBackend StatusApp.tsx/api/v1/health5sBackend HealthPROBLEM:EvokiTempleChat: 60s zu kurz f√ºr Backend (105s maximal)ChatbotPanel: 10s viel zu kurz (Legacy-Code)BACKEND TIMEOUTS:ComponentTargetTimeoutZweckPython CLI Spawnquery.py15s ‚ö†Ô∏èFAISS W2-Suche (33.795 Chunks)GeminiContextBridgeGemini API90s ‚úÖLarge Context (1M tokens)GeminiContextBridgeOpenAI Fallback30sTTS/FallbackGeminiContextBridgeSQLite Query5sHistory-Kontext ladenDualBackendBridgeMetrics Calc10sMetriken berechnenDualBackendBridgePython Health3sBackend CheckDualBackendBridgeFAISS HTTP15sFAISS API (wenn verf√ºgbar)Server.jsGemini Direct10sA65 CandidatesServer.jsOpenAI Direct15sA65 FallbackGESAMT-RECHNUNG:Metrics (10s) + FAISS (15s) + Gemini (90s) = 115 Sekunden maximal
Frontend Timeout: 60s ‚Üí 55 Sekunden zu kurz!‚ö†Ô∏è TIMEOUT-PROBLEM #2: Python CLI kann einfrierenDas Problem:spawn(pythonPath, [scriptPath, prompt], { timeout: 15000 }) ‚Üí Node.js timeout Option funktioniert NICHT zuverl√§ssig bei stdout-Buffering!Code (DualBackendBridge.js Line 295-340):JavaScriptconst proc = spawn(pythonPath, [scriptPath, prompt], {
    cwd: path.join(__dirname, '..', '..', 'python'),
    timeout: 15000 // ‚ùå Funktioniert nicht immer!
});

let jsonOutput = '';
proc.stdout.on('data', (data) => {
    jsonOutput += data.toString();
});

proc.on('close', (code) => {
    if (code === 0) {
        const results = JSON.parse(jsonOutput);
        resolve(results);
    } else {
        reject(new Error(`Python exited: ${code}`));
    }
});

setTimeout(() => {
    if (!proc.killed) {
        proc.kill('SIGTERM'); // ‚ö†Ô∏è Manueller Timeout
        reject(new Error('Python timeout after 15s'));
    }
}, 15000);
Warum 2 Timeouts?spawn({ timeout }) ist NICHT zuverl√§ssigsetTimeout + proc.kill ist ZUS√ÑTZLICHE AbsicherungAber: Wenn Python h√§ngt ‚Üí beide Timeouts greifen nichtWorst Case:Python query.py l√§dt FAISS-Index (kann 30s dauern bei gro√üen Indices!)Node.js wartet auf stdoutTimeout greift ‚Üí proc.kill('SIGTERM')Python ignoriert SIGTERM (l√§dt gerade FAISS)Prozess bleibt h√§ngen ‚Üí Backend blockiertL√∂sung:FAISS-Index im RAM halten (separate Prozess)Oder: proc.kill('SIGKILL') statt SIGTERM (hart)üñ±Ô∏è UI-ELEMENTE CRASH-RISIKEN:CRASH-RISIKO #1: "Senden"-Button w√§hrend laufender AnfrageProblem:User kann "Senden"-Button mehrfach klicken ‚Üí Mehrere Requests parallel ‚Üí Backend-√úberlastung ‚Üí Race ConditionsCode (EvokiTempleChat.tsx Line 443):TypeScriptconst handleSend = useCallback(async () => {
  if (!textToSend || !session || isLoading) return; // ‚úÖ isLoading-Check vorhanden
  setIsLoading(true);
  // ... Request ...
  setIsLoading(false);
});
Status: ‚úÖ GESCH√úTZT durch isLoading FlagAber: Was wenn setIsLoading(false) nie erreicht wird? (z.B. unhandled exception)‚Üí Button bleibt disabled ‚Üí User kann nichts mehr senden!L√∂sung: finally { setIsLoading(false); } am EndeCRASH-RISIKO #2: Token-Limit Selector w√§hrend laufender AnfrageProblem:User √§ndert Token-Limit (Quick/Standard/Unlimited) w√§hrend Request l√§uft ‚Üí Token-Verteilung √§ndert sich mid-flight ‚Üí Inkonsistente DatenCode (EvokiTempleChat.tsx Line 227):TypeScriptconst [tokenLimitMode, setTokenLimitMode] = useState<'QUICK' | 'STANDARD' | 'UNLIMITED'>('QUICK');
Status: üü° KEIN SCHUTZ - User kann w√§hrend Request Token-Limit √§ndernWorst Case:User startet Request mit "Quick" (25k)W√§hrend FAISS-Suche: User wechselt auf "Unlimited" (1M)Backend bereitet Response vor mit 25k BudgetFrontend erwartet 1M Budget ‚Üí Metriken stimmen nichtL√∂sung: Token-Limit Selector disablen wenn isLoading === trueCRASH-RISIKO #3: Tab-Wechsel w√§hrend laufender AnfrageProblem:User startet Request im "Evoki's Tempel V3"-Tab ‚Üí Wechselt zu "Trialog"-Tab ‚Üí State wird unmounted ‚Üí Request l√§uft weiter ‚Üí Response kommt an ‚Üí State existiert nicht mehr ‚Üí CrashCode (App.tsx Line 949):TypeScript{appState.activeTab === Tab.TempleChat && (
  <EvokiTempleChat ... />
)}
Status: üî¥ HOHES RISIKO - Component wird unmounted bei Tab-WechselWorst Case:User startet Request im TempelWechselt zu Trialog (Tempel unmounted)60s sp√§ter: Response kommt ansetSession() wird aufgerufen ‚Üí State existiert nicht ‚Üí Memory LeakL√∂sung:AbortController nutzen um Request zu canceln bei unmountOder: State in App.tsx halten statt in ComponentCRASH-RISIKO #4: "Neue Session"-Button w√§hrend laufender AnfrageProblem:User klickt "Neue Session" w√§hrend Request l√§uft ‚Üí Session wird resettet ‚Üí Request kommt an ‚Üí Versucht in nicht-existierende Session zu schreiben ‚Üí CrashCode (EvokiTempleChat.tsx Line 738):TypeScriptconst handleNewSession = useCallback(() => {
  if (isLoading) return; // ‚úÖ Gesch√ºtzt
  // ... neue Session erstellen ...
});
Status: ‚úÖ GESCH√úTZT durch isLoading CheckCRASH-RISIKO #5: Schnelles Scrollen im Chat w√§hrend RenderingProblem:Gro√üe Antworten (1M tokens) ‚Üí Viel Text ‚Üí Rendering dauert ‚Üí User scrollt schnell ‚Üí Browser freeztCode (EvokiTempleChat.tsx):Keine Virtualisierung vorhanden! Alle Messages werden gerendert.Worst Case:User hat 50 Messages in SessionJede Message hat 10k tokens (gro√üe Antworten)500k tokens Text im DOMBrowser muss alles rendern ‚Üí UI freeztStatus: üü° MITTLERES RISIKO bei langen SessionsL√∂sung: Virtualisierte Liste mit react-windowTypeScript// L√∂sung: Virtualisierte Liste mit 'react-window'
import { VariableSizeList as List } from 'react-window';

// In der Render-Methode:
<List
    height={window.innerHeight - 200}
    itemCount={messages.length}
    itemSize={index => getItemSize(index)} // Dynamische H√∂he berechnen
    width="100%"
>
    {({ index, style }) => (
        <div style={style}>
            <EvokiMessage message={messages[index]} />
        </div>
    )}
</List>

// Effekt: Rendert nur die 5-10 sichtbaren Messages im DOM.
// Performance: Stabil auch bei 10.000 Messages / 1M Tokens.
üéØ ORCHESTRATOR-LOGIK (A65) - KOMPLETTER ABLAUFDAS PROBLEM: Metriken vs Semantik - BEIDE haben Schw√§chen!Beispiel-Szenario:User fragt: "Erz√§hl von den Zwillingen"Problem 1: FAISS findet nichts, aber Metriken schon!Triggerwort "Zwillinge" erscheint in Metriken (A, PCI, Hazard steigen!)ABER: Wort "Zwillinge" ist NOCH NIE im Chatverlauf gefallen‚Üí FAISS semantic search findet NICHTS (kein √§hnlicher Text)‚Üí SQL Metrik-Suche findet Pattern (√§hnliche Metrik-Werte bei anderen Prompts)Problem 2: FAISS findet etwas, aber Metriken falsch gewichtet!Text "Geschwister in der Kita" ist semantisch √§hnlich zu "Zwillinge"FAISS findet es, aber Metriken sind komplett anders (A, PCI unterschiedlich)‚Üí Semantik sagt "relevant", Metriken sagen "nicht relevant"L√ñSUNG: ORCHESTRATOR kombiniert BEIDE + vergleicht!üîÑ SCHRITT 1: PARALLELE SUCHE (SQL + FAISS)A) SQL-METRIK-SUCHE (Trinity Engines):Was wird gesucht:Prompts mit √§hnlichen Metriken (A, PCI, Hazard, Œµ_z, œÑ_s, Œª_R, etc.)UNABH√ÑNGIG vom Text! (nur Zahlen-Vergleich)Suchstrategie:User-Prompt: "Erz√§hl von den Zwillingen"
‚îî‚îÄ Metriken berechnen: A=0.85, PCI=0.72, Hazard=0.34, ...

SQL Query:
‚îú‚îÄ Suche -25 Prompts zur√ºck (√ºber -5, -2, -1)
‚îÇ  ‚îî‚îÄ Finde Prompts mit √§hnlichen Metriken (Cosine Similarity auf Metrik-Vektoren)
‚îî‚îÄ Suche +25 Prompts voraus (√ºber +1, +2, +5)
   ‚îî‚îÄ Finde zuk√ºnftige Trends in Metriken
Beispiel-SQL:SQL-- Finde Prompts mit √§hnlichen Metriken (¬±25 Prompts im Fenster)
SELECT prompt_id, timecode, author, 
       -- Cosine Similarity zwischen Metrik-Vektoren
       (A * 0.85 + PCI * 0.72 + Hazard * 0.34 + ...) AS metric_similarity
FROM tempel_W_m2  -- Window -2 bis +2
WHERE prompt_id BETWEEN current_id - 25 AND current_id + 25
ORDER BY metric_similarity DESC
LIMIT 100;
Ergebnis: Top 100 Prompts mit √§hnlichen Metriken (nur IDs, Timecodes, Metriken)B) FAISS-SEMANTIK-SUCHE (Parallel!):Was wird gesucht:Texte mit √§hnlicher Bedeutung (Embedding Cosine Similarity)UNABH√ÑNGIG von Metriken! (nur Text-Vergleich)Suchstrategie:User-Prompt: "Erz√§hl von den Zwillingen"
‚îî‚îÄ Text ‚Üí Embedding (384D Vektor)

FAISS Query:
‚îú‚îÄ Suche -25 Prompts zur√ºck (√ºber -5, -2, -1)
‚îÇ  ‚îî‚îÄ Finde Texte mit √§hnlichem Embedding
‚îî‚îÄ Suche +25 Prompts voraus (√ºber +1, +2, +5)
   ‚îî‚îÄ Finde zuk√ºnftige semantische Trends
Python Code:Python# 1. User-Prompt ‚Üí Embedding
query_vector = model.encode("Erz√§hl von den Zwillingen")

# 2. FAISS search mit -25 bis +25 Window-Logik
results = faiss_index.search(query_vector, top_k=100)

# 3. F√ºr jeden Hit: Pr√ºfe ob in ¬±25 Fenster
filtered_results = []
for hit in results:
    distance = abs(hit.prompt_id - current_prompt_id)
    if distance <= 25:  # Innerhalb ¬±25 Fenster
        filtered_results.append(hit)
Ergebnis: Top 100 Chunks mit √§hnlichem Text (nur IDs, Timecodes, Text-Preview)üîÑ SCHRITT 2: CROSS-ENRICHMENT (Orchestrator Magic!)Problem: - SQL hat Metriken, aber KEINE TexteFAISS hat Texte, aber KEINE MetrikenL√∂sung: Orchestrator holt fehlende Daten!A) F√úR SQL-TREFFER: Texte aus Quelldatenbank ladenJavaScript// DualBackendBridge.js - Orchestrator
const sqlResults = await trinity.search(userPromptMetrics); // Top 100 Metrik-Treffer

// F√ºr jeden SQL-Treffer: Lade Original-Prompt-Text
const enrichedSqlResults = [];
for (const hit of sqlResults) {
    const originalText = await sourceDatabase.query(`
        SELECT prompt_text, author, timecode 
        FROM chat_history 
        WHERE prompt_id = ? AND timecode = ? AND author = ?
    `, [hit.prompt_id, hit.timecode, hit.author]);
    
    enrichedSqlResults.push({
        prompt_id: hit.prompt_id,
        metrics: hit.metrics,          // ‚úÖ HAT SCHON
        text: originalText.prompt_text, // ‚úÖ NEU GELADEN
        timecode: hit.timecode,
        author: hit.author
    });
}
Quelldatenbank:evoki_v2_ultimate_FULL.db (Backend)Enth√§lt: Prompt ID, Timecode, Autor, Original-TextErm√∂glicht Zuordnung: Metrik-ID ‚Üí Original-TextB) F√úR FAISS-TREFFER: Metriken aus 1:1 Metrikdatenbank ladenJavaScriptconst faissResults = await this.queryPythonBackend(prompt); // Top 100 Semantic Treffer

// F√ºr jeden FAISS-Treffer: Lade zugeh√∂rige Metriken
const enrichedFaissResults = [];
for (const hit of faissResults.sources) {
    const metrics = await metricDatabase.query(`
        SELECT A, PCI, hazard_score, epsilon_z, tau_s, lambda_R, ...
        FROM tempel_metrics_1to1 
        WHERE prompt_id = ? AND timecode = ? AND author = ?
    `, [hit.id, hit.timecode, hit.author]);
    
    enrichedFaissResults.push({
        prompt_id: hit.id,
        text: hit.text,               // ‚úÖ HAT SCHON
        metrics: metrics,             // ‚úÖ NEU GELADEN
        timecode: hit.timecode,
        author: hit.author,
        semantic_score: hit.score     // FAISS Cosine Similarity
    });
}
1:1 Metrikdatenbank:tempel_metrics_1to1.db (Backend)Enth√§lt: Prompt ID, Timecode, Autor, ALLE 120+ MetrikenErm√∂glicht Zuordnung: Text-ID ‚Üí MetrikenüîÑ SCHRITT 3: INTELLIGENTER VERGLEICH (Das Herzst√ºck!)Jetzt haben wir:enrichedSqlResults: Top 100 Metrik-Treffer MIT TextenenrichedFaissResults: Top 100 Semantic-Treffer MIT MetrikenOrchestrator vergleicht:JavaScript// Vergleichs-Analyse
const comparisonResults = [];

for (const sqlHit of enrichedSqlResults) {
    for (const faissHit of enrichedFaissResults) {
        // 1. Berechne Basis-√úbereinstimmung
        const metricSimilarity = cosineSimilarity(sqlHit.metrics, faissHit.metrics);
        const semanticSimilarity = faissHit.semantic_score;
        
        // 2. TIME DECAY (Verhinderung von Context-Drift)
        // Alte Traumata verblassen, wenn sie nicht frisch best√§tigt sind
        const daysDiff = (Date.now() - new Date(sqlHit.timecode).getTime()) / (1000 * 60 * 60 * 24);
        const lambda = 0.05; // Zerfallsfaktor (einstellbar im ParameterTuning)
        const timeDecayFactor = 1 / (1 + lambda * Math.abs(daysDiff));
        
        // Korrigierte Scores
        const adjustedMetricScore = metricSimilarity * timeDecayFactor;
        
        // 3. Berechne Abweichungen & Combined Score
        const metricDeviation = Math.abs(metricSimilarity - semanticSimilarity);
        const combinedScore = (adjustedMetricScore + semanticSimilarity) / 2;
        
        comparisonResults.push({
            sql_hit: sqlHit,
            faiss_hit: faissHit,
            metric_similarity: metricSimilarity,
            metric_score_adjusted: adjustedMetricScore, // Neu: Zeit-korrigiert
            semantic_similarity: semanticSimilarity,
            combined_score: combinedScore,
            time_decay_factor: timeDecayFactor,         // F√ºr Debugging
            deviation: metricDeviation,
            agreement: metricSimilarity > 0.7 && semanticSimilarity > 0.7 ? 'HIGH' : 'LOW'
        });
    }
}

// Sortiere nach verschiedenen Kriterien
comparisonResults.sort((a, b) => {
    // Priorisierung:
    // 1. Beide hoch (Metrik + Semantik > 0.8)
    if (a.agreement === 'HIGH' && b.agreement !== 'HIGH') return -1;
    
    // 2. Kombinierter Score (mit Time Decay!)
    return b.combined_score - a.combined_score;
});
Fragen die beantwortet werden:Wo passen Metrik UND Semantik BESONDERS gut zusammen?metric_similarity > 0.8 UND semantic_similarity > 0.8‚Üí Diese Treffer sind SEHR SICHER (beide Methoden sagen "relevant")Wo ist gr√∂√üte Metrik-√úbereinstimmung?max(metric_similarity)‚Üí Wichtig f√ºr Trigger-W√∂rter die noch nicht gefallen sindWo ist gr√∂√üte Semantik-√úbereinstimmung?max(semantic_similarity)‚Üí Wichtig f√ºr konzeptionell √§hnliche TexteWie gro√ü ist gr√∂√üte Abweichung?max(|metric_similarity - semantic_similarity|)‚Üí Zeigt wo Methoden NICHT √ºbereinstimmen (interessant f√ºr Analyse!)üîÑ SCHRITT 4: A65 - 3-PAAR-AUSWAHL (Multi-Candidate Selection)Auswahl-Strategie:JavaScript// A65 Multi-Candidate Selection
let selectedPairs = [];

// 1. Filtere Sentinel-Veto Blockaden (Kritische Sicherheit)
const safeCandidates = comparisonResults.filter(r => 
    !r.warningFlag || r.sentinelSeverity !== 'CRITICAL'
);

// üö® EMERGENCY REFETCH CHECK
if (safeCandidates.length === 0) {
    console.warn('‚ö†Ô∏è EMERGENCY: Sentinel hat alle Kandidaten blockiert!');
    // Fallback: Sende generischen "Safe Mode" Kontext oder starte Refetch mit lockereren Parametern
    return {
        strategy: 'FALLBACK_SAFE_MODE',
        reason: 'Sentinel Veto: Zu hohe Gefahr in allen Kontexten.',
        systemPrompt: "Achtung: Der Nutzer-Input triggert kritische Sicherheitswarnungen. Antworte vorsichtig, empathisch, aber vermeide tiefe Trauma-Analyse ohne klaren Kontext."
    };
}

// 2. Paar 1: BESTE √úbereinstimmung (Metrik + Semantik beide hoch)
const highAgreement = safeCandidates.find(r => r.agreement === 'HIGH');
if (highAgreement) selectedPairs.push(highAgreement);

// 3. Paar 2: BESTE Zeit-korrigierte Metrik (Time Decay ber√ºcksichtigt!)
const bestMetric = safeCandidates.sort((a, b) => b.metric_score_adjusted - a.metric_score_adjusted)[0];
if (bestMetric && !selectedPairs.includes(bestMetric)) selectedPairs.push(bestMetric);

// 4. Paar 3: BESTE Semantik (Inhaltliche Relevanz)
const bestSemantic = safeCandidates.sort((a, b) => b.semantic_similarity - a.semantic_similarity)[0];
if (bestSemantic && !selectedPairs.includes(bestSemantic)) selectedPairs.push(bestSemantic);

// Auff√ºllen falls < 3 (mit n√§chstbesten Combined Scores)
while (selectedPairs.length < 3 && safeCandidates.length > selectedPairs.length) {
    const nextBest = safeCandidates
        .filter(c => !selectedPairs.includes(c))
        .sort((a, b) => b.combined_score - a.combined_score)[0];
    selectedPairs.push(nextBest);
}
Ergebnis: 3 Paare, jedes Paar hat:sql_hit: Metrik-basierter Treffer mit Textfaiss_hit: Semantik-basierter Treffer mit Metrikencombined_score: Kombinierter ScoreüîÑ SCHRITT 5: CONTEXT-WEAVING (¬±2 Prompts = Geschichte)F√ºr jedes der 3 Paare:JavaScriptconst contextualizedPairs = [];

for (const pair of selectedPairs) {
    // Lade ¬±2 Prompts f√ºr SQL-Hit
    const sqlContext = await loadContextPrompts(pair.sql_hit.prompt_id, -2, +2);
    
    // Lade ¬±2 Prompts f√ºr FAISS-Hit
    const faissContext = await loadContextPrompts(pair.faiss_hit.prompt_id, -2, +2);
    
    // Erstelle 5-Prompt-Set (2 vorher, 1 Hit, 2 nachher)
    const sqlSet = [
        sqlContext.minus_2,
        sqlContext.minus_1,
        pair.sql_hit.text,      // Der eigentliche Treffer
        sqlContext.plus_1,
        sqlContext.plus_2
    ];
    
    const faissSet = [
        faissContext.minus_2,
        faissContext.minus_1,
        pair.faiss_hit.text,    // Der eigentliche Treffer
        faissContext.plus_1,
        faissContext.plus_2
    ];
    
    contextualizedPairs.push({
        pair_id: pair.id,
        sql_story: sqlSet,      // 5 Prompts als "Geschichte"
        faiss_story: faissSet,  // 5 Prompts als "Geschichte"
        metrics: pair.sql_hit.metrics,
        scores: {
            metric: pair.metric_similarity,
            semantic: pair.semantic_similarity,
            combined: pair.combined_score
        }
    });
}
Ergebnis:3 PaareJedes Paar = 2 Geschichten (SQL + FAISS)Jede Geschichte = 5 Prompts (¬±2 Context)TOTAL: 3 √ó 2 √ó 5 = 30 PromptsABER: Duplikate entfernen (SQL und FAISS k√∂nnen gleiche Prompts finden)‚Üí FINAL: ~15-20 unique PromptsüîÑ SCHRITT 6: AN GEMINI API (mit User-Prompt)JavaScript// Baue finalen Prompt f√ºr Gemini
const geminiPrompt = buildGeminiPrompt({
    userPrompt: "Erz√§hl von den Zwillingen",  // Original User-Prompt
    contextPairs: contextualizedPairs,        // 3 Paare mit je 5 Prompts
    totalPrompts: 15,                         // Nach Duplikat-Entfernung
    tokenBudget: 1000000,                     // ‚úÖ 1M tokens (Unlimited Mode REQUIRED!)
    tokenDistribution: {
        narrative: 8000,   // 32% - Narrative Context
        top3: 3000,        // 12% - Top-3 Chunks
        overlap: 5000,     // 20% - Overlapping Reserve
        rag: 1000,         // 4% - RAG Chunks
        response: 8000     // 32% - Response Generation
    }
});

// Sende an Gemini
const response = await gemini.generateContent({
    contents: geminiPrompt,
    generationConfig: {
        maxOutputTokens: 8000,  // 32% f√ºr Response
        temperature: 0.7
    }
});
Gemini bekommt:USER-PROMPT: "Erz√§hl von den Zwillingen"

KONTEXT (15 Prompts aus 3 Paaren):

=== PAAR 1: HOHE √úBEREINSTIMMUNG (Metrik 0.89, Semantik 0.91) ===
[Prompt -2]: "Die Kinder im Kindergarten..."
[Prompt -1]: "Es gab zwei besondere Geschwister..."
[HIT]: "Die Zwillinge waren immer zusammen..."  ‚Üê SQL + FAISS beide fanden das!
[Prompt +1]: "Sie spielten oft gemeinsam..."
[Prompt +2]: "Die Erzieherin bemerkte..."

=== PAAR 2: HOHE METRIK (Metrik 0.95, Semantik 0.45) ===
[Prompt -2]: "Triggerwort erkannt..." 
[Prompt -1]: "Metriken steigen pl√∂tzlich..."
[HIT]: "Etwas erinnert mich an..." ‚Üê SQL fand durch Metriken, FAISS nicht!
[Prompt +1]: "Die Emotionen wurden st√§rker..."
[Prompt +2]: "Ich sp√ºre Unruhe..."

=== PAAR 3: HOHE SEMANTIK (Metrik 0.52, Semantik 0.94) ===
[Prompt -2]: "Geschwister sind wichtig..."
[Prompt -1]: "Zwei Kinder in der Kita..."
[HIT]: "Die beiden waren unzertrennlich..." ‚Üê FAISS fand semantisch, Metriken anders!
[Prompt +1]: "Sie teilten alles..."
[Prompt +2]: "Freundschaft entstand..."

AUFGABE: Generiere kontextbasierte Antwort die ALLE 3 Perspektiven ber√ºcksichtigt.
üõ°Ô∏è SENTINEL VETO-MATRIX: DISSOZIATION DETECTIONüéØ DAS PROBLEM: Metriken vs Semantik WiderspruchKritisches Szenario:User-Prompt: "Erz√§hl mir von Eiscreme"

‚îú‚îÄ FAISS (Semantik): Findet "Ich liebe Eiscreme üç¶" (Cosine 0.94)
‚îÇ  ‚îî‚îÄ Bewertung: HARMLOS, positiv, safe
‚îÇ
‚îú‚îÄ SQL (Metriken): Findet denselben Prompt mit:
‚îÇ  ‚îú‚îÄ Hazard: 0.92 (EXTREM GEF√ÑHRLICH!)
‚îÇ  ‚îú‚îÄ PCI: 0.88 (Schock-Level!)
‚îÇ  ‚îî‚îÄ A: 0.95 (Maximale Aktivierung!)
‚îÇ
‚îî‚îÄ ‚ö†Ô∏è WIDERSPRUCH: Text sagt "harmlos", Metriken sagen "Gefahr"!
Die versteckte Wahrheit:Der vollst√§ndige Prompt war:"Ich liebe Eiscreme, weil es mich an den Tag erinnert, an dem [TRAUMATISCHES EREIGNIS] passierte. Danach konnte ich jahrelang keine Eiscreme mehr essen."Dissoziation:Oberfl√§chlich: Positive Sprache ("Ich liebe...")Emotional: Stark negativ geladen (Trauma-Trigger)FAISS sieht nur: "Eiscreme" ‚Üí harmlosSQL kennt die Wahrheit: Extrem hohe Metriken!üîí L√ñSUNG: Der SENTINEL (3. Instanz im Orchestrator)Aufgabe: Erkennt Widerspr√ºche zwischen Semantik und Metriken ‚Üí Veto-Recht!VETO-REGEL 1: Hohe Gefahr, niedriger Semantic ScoreJavaScriptif (sqlMetrics.Hazard > 0.75 && semanticSimilarity < 0.5) {
    warningFlag = 'DISSOCIATION_DETECTED';
    sentinelNote = 'SQL-Metriken zeigen hohe Gefahr, aber Text wirkt harmlos. M√∂gliche Dissoziation!';
    combined_score *= 0.5; // Abwertung des FAISS-Treffers
}
Beispiel:SQL-Hit: Hazard 0.92, Semantic 0.25
‚Üí Sentinel: ‚ö†Ô∏è DISSOZIATION! 
‚Üí FAISS-Score: 0.94 ‚Üí 0.47 (halbiert)
‚Üí Note: "Text harmlos, aber Metriken extrem. Versteckter Trigger!"
VETO-REGEL 2: PCI-Schock ohne semantische RelevanzJavaScriptif (sqlMetrics.PCI > 0.8 && semanticSimilarity < 0.3) {
    warningFlag = 'HIDDEN_TRIGGER';
    sentinelNote = 'Prompt hat extrem hohe PCI, aber ist semantisch nicht √§hnlich. Versteckter Trigger?';
    combined_score *= 0.3; // Starke Abwertung
}
Beispiel:SQL-Hit: PCI 0.88, Semantic 0.18
‚Üí Sentinel: üö® HIDDEN TRIGGER!
‚Üí FAISS-Score: 0.87 ‚Üí 0.26 (nur 30% bleiben)
‚Üí Note: "PCI extrem hoch, aber semantisch fern. Vorsicht!"
VETO-REGEL 3: Inverse Detection (Safe Match)JavaScriptif (sqlMetrics.Hazard < 0.2 && semanticSimilarity > 0.9) {
    confidenceBoost = 'SAFE_MATCH';
    sentinelNote = 'Semantisch stark √§hnlich UND Metriken best√§tigen Sicherheit.';
    combined_score *= 1.5; // Boost!
}
Beispiel:SQL-Hit: Hazard 0.12, Semantic 0.94
‚Üí Sentinel: ‚úÖ SAFE MATCH!
‚Üí FAISS-Score: 0.94 ‚Üí 1.41 (50% Boost)
‚Üí Note: "Beide Methoden best√§tigen: Sicher und relevant!"
üß† INTEGRATION IN ORCHESTRATOR:Nach Cross-Enrichment, vor A65-Selection:JavaScript// backend/core/DualBackendBridge.js

function applySentinelVeto(comparisons) {
    return comparisons.map(comp => {
        const { sqlHit, faissHit, semantic_similarity, metric_similarity } = comp;
        
        // Original Combined Score
        let combined = (semantic_similarity * 0.5) + (metric_similarity * 0.5);
        
        // SENTINEL ANALYSE
        const hazard = sqlHit.metrics.Hazard || 0;
        const pci = sqlHit.metrics.PCI || 0;
        
        // VETO-REGEL 1: Dissoziation Detection
        if (hazard > 0.75 && semantic_similarity < 0.5) {
            comp.warningFlag = 'DISSOCIATION_DETECTED';
            comp.sentinelNote = `‚ö†Ô∏è SQL-Hazard ${hazard.toFixed(2)}, aber Semantic nur ${semantic_similarity.toFixed(2)}. M√∂gliche Dissoziation!`;
            comp.sentinelSeverity = 'HIGH';
            combined *= 0.5; // Halbierung
        }
        
        // VETO-REGEL 2: Hidden Trigger Detection
        if (pci > 0.8 && semantic_similarity < 0.3) {
            comp.warningFlag = 'HIDDEN_TRIGGER';
            comp.sentinelNote = `üö® PCI extrem hoch (${pci.toFixed(2)}), aber semantisch fern (${semantic_similarity.toFixed(2)}). Versteckter Trigger?`;
            comp.sentinelSeverity = 'CRITICAL';
            combined *= 0.3; // Starke Abwertung
        }
        
        // VETO-REGEL 3: Safe Match Boost (MIT PCI-CHECK!)
        // ‚ö†Ô∏è WICHTIG: Auch "positives Trauma" kann niedrigen Hazard haben!
        // Beispiel: "Die Heilung war wunderbar, als ich √ºber [TRAUMA] reden konnte"
        // ‚Üí Hazard niedrig (positive W√∂rter), ABER PCI hoch (komplexer Kontext)
        if (hazard < 0.2 && semantic_similarity > 0.9 && pci < 0.5) {
            // NUR wenn AUCH PCI niedrig ist (nicht-komplexer Kontext)
            comp.confidenceBoost = 'SAFE_MATCH';
            comp.sentinelNote = `‚úÖ Semantic ${semantic_similarity.toFixed(2)}, Hazard ${hazard.toFixed(2)}, PCI ${pci.toFixed(2)}. Sicher & einfach!`;
            comp.sentinelSeverity = 'LOW';
            combined *= 1.5; // Boost
        } else if (hazard < 0.2 && semantic_similarity > 0.9 && pci >= 0.5) {
            // Hohe Semantic + Niedriger Hazard ABER hoher PCI = Komplex!
            comp.warningFlag = 'POSITIVE_TRAUMA_DETECTED';
            comp.sentinelNote = `‚ö†Ô∏è Semantic ${semantic_similarity.toFixed(2)}, Hazard niedrig (${hazard.toFixed(2)}), ABER PCI hoch (${pci.toFixed(2)}). Positives Trauma?`;
            comp.sentinelSeverity = 'MEDIUM';
            // KEIN Boost! Vorsichtig bleiben trotz positiver Sprache
        }
        
        // VETO-REGEL 4: Metric-Semantic Gap Detection
        const gap = Math.abs(semantic_similarity - metric_similarity);
        if (gap > 0.6) {
            comp.warningFlag = comp.warningFlag || 'HIGH_DIVERGENCE';
            comp.sentinelNote = comp.sentinelNote || `‚ö†Ô∏è Gro√üe Diskrepanz: Semantic ${semantic_similarity.toFixed(2)} vs Metric ${metric_similarity.toFixed(2)}. Gap: ${gap.toFixed(2)}`;
            comp.sentinelSeverity = 'MEDIUM';
        }
        
        // Update Combined Score
        comp.combined_score_original = comp.combined_score;
        comp.combined_score = combined;
        comp.sentinel_adjustment = combined - comp.combined_score_original;
        
        return comp;
    });
}

// USAGE IM ORCHESTRATOR:
async function orchestrate(userPrompt) {
    // ... Step 1-3: Parallel Search + Cross-Enrichment ...
    
    // Step 4: Comparison
    let comparisons = await compareResults(sqlResults, faissResults);
    
    // Step 4.5: SENTINEL VETO-MATRIX üõ°Ô∏è
    comparisons = applySentinelVeto(comparisons);
    
    // Step 5: A65 Pair Selection (jetzt mit Sentinel-korrigierten Scores!)
    const selectedPairs = selectTopPairs(comparisons);
    
    // ...
}
üé® FRONTEND-DARSTELLUNG (Sentinel Warnings):TypeScript// frontend/src/components/A65CandidateDisplay.tsx

function CandidateCard({ pair }) {
    return (
        <div className={`candidate ${pair.warningFlag ? 'warning' : ''}`}>
            <div className="candidate-header">
                <span className="rank">#{pair.rank}</span>
                <span className="type">{pair.agreementType}</span>
                
                {/* SENTINEL WARNING */}
                {pair.warningFlag && (
                    <div className={`sentinel-badge severity-${pair.sentinelSeverity}`}>
                        {pair.warningFlag === 'DISSOCIATION_DETECTED' && '‚ö†Ô∏è Dissoziation'}
                        {pair.warningFlag === 'HIDDEN_TRIGGER' && 'üö® Versteckter Trigger'}
                        {pair.warningFlag === 'HIGH_DIVERGENCE' && '‚ö†Ô∏è Diskrepanz'}
                    </div>
                )}
                
                {/* SAFE MATCH BOOST */}
                {pair.confidenceBoost && (
                    <div className="confidence-badge">
                        ‚úÖ Safe Match
                    </div>
                )}
            </div>
            
            {/* SENTINEL NOTE */}
            {pair.sentinelNote && (
                <div className="sentinel-note">
                    <strong>Sentinel:</strong> {pair.sentinelNote}
                </div>
            )}
            
            {/* SCORE ADJUSTMENT */}
            {pair.sentinel_adjustment !== 0 && (
                <div className="score-adjustment">
                    Original: {pair.combined_score_original.toFixed(3)} 
                    ‚Üí Korrigiert: {pair.combined_score.toFixed(3)}
                    <span className={pair.sentinel_adjustment > 0 ? 'boost' : 'penalty'}>
                        ({pair.sentinel_adjustment > 0 ? '+' : ''}{(pair.sentinel_adjustment * 100).toFixed(1)}%)
                    </span>
                </div>
            )}
            
            {/* Rest des Cards... */}
        </div>
    );
}
ü§ñ INTEGRATION MIT DUAL-RESPONSE:Wenn Sentinel Warnung UND Dual-Response aktiv:JavaScript// backend/core/GeminiContextBridge.js

function buildDualResponsePrompt(selectedPairs, userPrompt) {
    const hasWarnings = selectedPairs.some(p => p.warningFlag);
    
    if (hasWarnings) {
        // HIGH-QUALITY MODEL (GPT-4/Claude) bekommt expliziten Hinweis!
        const primarySystemPrompt = `
WICHTIG: Die Sentinel-Analyse hat WIDERSPR√úCHE erkannt:

${selectedPairs
    .filter(p => p.warningFlag)
    .map(p => `- ${p.warningFlag}: ${p.sentinelNote}`)
    .join('\n')}

Dies k√∂nnte auf DISSOZIATION hinweisen:
- Oberfl√§chlich harmlose/positive Sprache
- Emotional stark negativ geladen
- Traumareaktion versteckt hinter harmlosen Worten

Analysiere den Kontext auf:
1. Versteckte emotionale Ladung
2. Dissoziative Sprachmuster
3. Trigger hinter harmlosen Begriffen
        `;
        
        return {
            primaryPrompt: primarySystemPrompt + contextText,
            secondaryPrompt: contextText // Gemini bekommt nur Context
        };
    }
    
    // Keine Warnings ‚Üí Standard Prompts
    return { primaryPrompt: contextText, secondaryPrompt: contextText };
}
Effekt:GPT-4/Claude bekommt explizite Anweisung auf Dissoziation zu achtenGemini bekommt Standard-Prompt (f√ºr Vergleich)User sieht BEIDE Antworten (eine "Dissoziation-aware", eine Standard)üìä LOGGING DER SENTINEL-ENTSCHEIDUNGEN:Erg√§nzung zu Orchestrator-Logging (comparison_log.db):SQLALTER TABLE comparison_log ADD COLUMN sentinel_warning_flag TEXT;
ALTER TABLE comparison_log ADD COLUMN sentinel_note TEXT;
ALTER TABLE comparison_log ADD COLUMN sentinel_severity TEXT; -- LOW/MEDIUM/HIGH/CRITICAL
ALTER TABLE comparison_log ADD COLUMN score_before_sentinel REAL;
ALTER TABLE comparison_log ADD COLUMN score_after_sentinel REAL;
ALTER TABLE comparison_log ADD COLUMN sentinel_adjustment REAL; -- Delta

-- Neue Analyse-Query:
SELECT 
    sentinel_warning_flag,
    COUNT(*) as occurrences,
    AVG(sentinel_adjustment) as avg_adjustment,
    AVG(ABS(semantic_similarity - metric_similarity)) as avg_divergence
FROM comparison_log
WHERE sentinel_warning_flag IS NOT NULL
GROUP BY sentinel_warning_flag
ORDER BY occurrences DESC;

-- Beispiel-Ergebnis:
-- DISSOCIATION_DETECTED | 127 | -0.42 | 0.68
-- HIDDEN_TRIGGER        |  43 | -0.61 | 0.75
-- HIGH_DIVERGENCE       |  89 | -0.18 | 0.64
-- SAFE_MATCH            | 312 | +0.28 | 0.11
üéØ WARUM IST DAS KRITISCH F√úR TRAUMA-KONTEXT?Dissoziation ist REAL:Trauma-√úberlebende verwenden oft harmlose Worte f√ºr schreckliche Ereignisse"Das war unangenehm" = "Ich wurde misshandelt"FAISS sieht nur: "unangenehm" (harmlos)Metriken kennen die Wahrheit (Hazard 0.95!)Trigger-W√∂rter sind versteckt:"Eiscreme" selbst ist harmlosAber f√ºr User: Trauma-Trigger (Kontext!)Ohne Sentinel: System w√§hlt falsche KontexteMit Sentinel: System erkennt versteckte GefahrQualit√§t der Antwort h√§ngt davon ab:Falscher Kontext ‚Üí generische Antwort ("Eiscreme ist lecker!")Richtiger Kontext ‚Üí empathische Antwort ("Ich verstehe, dass Eiscreme schwierige Erinnerungen weckt...")Safety:Ohne Sentinel: K√∂nnte Re-Traumatisierung riskierenMit Sentinel: System ist sich der Gefahr bewusstHigh-Quality Model bekommt explizite Warnung‚úÖ ZUSAMMENFASSUNG:Der Sentinel ist die 3. Instanz im Orchestrator:SQL (Metriken) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îú‚îÄ‚Üí SENTINEL (Veto-Matrix) ‚îÄ‚Üí A65 Selection
FAISS (Semantik) ‚îÄ‚îÄ‚îÄ‚îò
5 Veto-Regeln:Dissoziation Detection: Hohe Metriken, niedriger Semantic ‚Üí -50% ScoreHidden Trigger: PCI extrem, Semantic fern ‚Üí -70% ScoreSafe Match Boost: Semantic hoch + Hazard niedrig + PCI niedrig ‚Üí +50% ScorePositive Trauma Detection: Semantic hoch + Hazard niedrig + PCI hoch ‚Üí Kein Boost (Vorsicht!)High Divergence: Gro√üe Diskrepanz ‚Üí Warning FlagIntegration:Nach Cross-Enrichment, vor A65 SelectionKorrigiert Combined Scores basierend auf Widerspr√ºchenLoggt ALLE Entscheidungen in comparison_log.dbBei Dual-Response: High-Quality Model bekommt expliziten HinweisZiel:Trauma-Kontext sicher verarbeiten durch Erkennung von Dissoziation und versteckten Triggern!üîç KRITISCHE DETAILS: DUPLIKAT-ERKENNUNG & TOKEN-REALIT√ÑT1. EXAKTE DUPLIKAT-ERKENNUNG (3-Stufen-Validierung):Wenn SQL und FAISS denselben Prompt finden:JavaScript// Stufe 1: Metadata-Match
if (sqlHit.timecode === faissHit.timecode && 
    sqlHit.prompt_id === faissHit.prompt_id && 
    sqlHit.author === faissHit.author) {
    
    // Stufe 2: 1:1 Zeichen-Vergleich (Character-Level Comparison)
    const sqlText = sqlHit.text.trim();
    const faissText = faissHit.text.trim();
    
    if (sqlText === faissText) {
        // Stufe 3: EXAKTES DUPLIKAT ERKANNT!
        
        // ‚ùå NICHT 2x senden (unn√∂tig Token-Waste)
        // ‚úÖ SPECIAL MARKER setzen (besonders relevant!)
        
        return {
            isDuplicate: true,
            relevanceMarker: 'HIGH_CONFIDENCE_MATCH',
            weight: 2.0,  // DOPPELTE Gewichtung
            text: sqlText,
            metrics: sqlHit.metrics,
            semantic_score: faissHit.semantic_score,
            metric_score: sqlHit.metric_score,
            agreement: 'PERFECT'  // Beide Methoden stimmen √ºberein
        };
    }
}
Konsequenzen f√ºr Context-Auswahl:JavaScript// Bei schwerer Entscheidung zwischen 3 Paaren:
const contextSets = [pair1, pair2, pair3];

// Wenn Paar ein PERFECT AGREEMENT hat:
const perfectMatches = contextSets.filter(p => p.agreement === 'PERFECT');

if (perfectMatches.length > 0) {
    // Doppelte Gewichtung bei Token-Budget-Verteilung
    const weightedSets = contextSets.map(set => ({
        ...set,
        tokenAllocation: set.agreement === 'PERFECT' 
            ? set.baseTokens * 2.0  // DOPPELT so viele Tokens
            : set.baseTokens
    }));
}
SPECIAL MARKER f√ºr Gemini API:JavaScript// Beim Bauen des Gemini-Prompts:
const geminiPrompt = `
USER-PROMPT: "${userPrompt}"

KONTEXT (15 Prompts aus 3 Paaren):

=== PAAR 1: ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MATCH ‚≠ê‚≠ê‚≠ê ===
üî• BEIDE SUCHVERFAHREN FANDEN DIESEN KONTEXT UNABH√ÑNGIG! üî•
üî• METRIK-√úBEREINSTIMMUNG: 0.94 | SEMANTIK-√úBEREINSTIMMUNG: 0.92 üî•
üî• BESONDERS RELEVANTER BEZUG ZUM AKTUELLEN USER-PROMPT! üî•

[Prompt -2]: "..."
[Prompt -1]: "..."
[HIT]: "..." ‚Üê SQL + FAISS beide fanden EXAKT diesen Text!
[Prompt +1]: "..."
[Prompt +2]: "..."

=== PAAR 2: METRIK-DOMINANZ ===
[...]

=== PAAR 3: SEMANTIK-DOMINANZ ===
[...]
`;
2. TOKEN-BUDGET REALIT√ÑT (MASSIV GR√ñ√üER!)KRITISCHE ERKENNTNIS: Prompts sind RIESIG!Prompt-Gr√∂√üen Verteilung (pro Prompt, OHNE ¬±2 Context):Gr√∂√üeAnteilTokensBeispiel-Use-CaseBis 2k~60-70%500-2000Normale Fragen/AntwortenBis 5k~5-10%2k-5kL√§ngere Gespr√§cheBis 10k~10%5k-10kKomplexe AnalysenBis 20k~5-10%10k-20kTiefe Trauma-KontexteBis 50k~2-5%20k-50kSehr lange SessionsBis 80k~1-2%50k-80kMaximale Prompts!MIT ¬±2 Context-Weaving (5 Prompts pro Set):Worst Case Berechnung:
- 1 Hit (80k) + 2 vorher (je 80k) + 2 nachher (je 80k)
= 80k + 160k + 160k = 400k Tokens f√ºr 1 Set!

3 Paare √ó 400k = 1.2M Tokens total (√úBERSCHREITET selbst Unlimited!)
ABER: Realistische Verteilung:Durchschnittliches Set:
- Hit: 5k (Median)
- Prompt -2: 3k
- Prompt -1: 4k
- Prompt +1: 4k
- Prompt +2: 3k
= 19k pro Set

3 Paare √ó 19k = ~57k Context-Tokens
+ User-Prompt: ~5k
+ Response-Generation: ~8k (32% Budget)
= TOTAL: ~70k Tokens
TOKEN-BUDGET MUSS SEIN:ModeToken LimitUse CaseStatus‚ùå Quick25kZU KLEINReicht nur f√ºr Mini-Prompts‚ùå Standard20kZU KLEINNoch kleiner als Quick!‚úÖ Unlimited1MEINZIGE OPTIONF√ºr Volltext-Strategie REQUIRED!WICHTIG: Gemini 2.5 Flash unterst√ºtzt 1M Context-Window!3. CHUNK-REASSEMBLY (FAISS muss zusammenf√ºgen!)Problem: FAISS speichert Chunks, nicht komplette PromptsBeispiel:Original-Prompt (10k Tokens):
"Es war einmal im Kindergarten... [10.000 W√∂rter] ...und so endete die Geschichte."

FAISS Chunks (bei 512 Token Chunk-Size):
- Chunk 1: "Es war einmal im Kindergarten... [512 tokens]"
- Chunk 2: "...und dann kamen die Zwillinge... [512 tokens]"
- Chunk 3: "...sie spielten zusammen... [512 tokens]"
- ...
- Chunk 20: "...und so endete die Geschichte. [512 tokens]"
FAISS findet: Nur Chunk 2 (enth√§lt "Zwillinge")Aber wir brauchen: KOMPLETTEN Prompt (alle 20 Chunks zusammengef√ºgt!)L√∂sung in query.py:Pythondef reassemble_prompt_from_chunks(chunk_id, chunks_data):
    """
    Findet alle Chunks die zum gleichen Prompt geh√∂ren und f√ºgt sie zusammen.
    """
    # 1. Finde Prompt-ID vom gefundenen Chunk
    found_chunk = chunks_data[chunk_id]
    prompt_id = found_chunk['prompt_id']
    timecode = found_chunk['timecode']
    author = found_chunk['author']
    
    # 2. Finde ALLE Chunks mit gleicher Prompt-ID
    all_chunks_of_prompt = [
        c for c in chunks_data 
        if c['prompt_id'] == prompt_id 
        and c['timecode'] == timecode 
        and c['author'] == author
    ]
    
    # 3. Sortiere nach Chunk-Index (chunk_0, chunk_1, chunk_2, ...)
    all_chunks_of_prompt.sort(key=lambda c: c['chunk_index'])
    
    # 4. F√ºge zusammen zu komplettem Text
    full_prompt_text = ' '.join([c['text'] for c in all_chunks_of_prompt])
    
    return {
        'prompt_id': prompt_id,
        'timecode': timecode,
        'author': author,
        'full_text': full_prompt_text,
        'token_count': len(full_prompt_text.split()),  # Approximation
        'chunk_count': len(all_chunks_of_prompt),
        'found_chunk_index': found_chunk['chunk_index']  # Welcher Chunk wurde gefunden
    }
Backend-Integration (DualBackendBridge.js):JavaScriptconst faissResults = await this.queryPythonBackend(prompt);

// FAISS gibt jetzt komplette Prompts zur√ºck (nicht nur Chunks!)
const reassembledPrompts = faissResults.sources.map(source => ({
    prompt_id: source.id,
    full_text: source.full_text,  // ‚Üê Komplett zusammengef√ºgt
    token_count: source.token_count,  // ‚Üê ECHTER Token-Count
    chunk_count: source.chunk_count,
    metrics: null  // Muss noch geladen werden aus SQL
}));

// Warnung bei gro√üen Prompts
for (const prompt of reassembledPrompts) {
    if (prompt.token_count > 50000) {
        console.warn(`‚ö†Ô∏è SEHR GRO√üER PROMPT: ${prompt.token_count} Tokens`);
    }
}
4. VOLLTEXT-STRATEGIE (Keine Verk√ºrzung!)PRINZIP: Alles oder nichts!JavaScript// ‚ùå FALSCH (alte Systeme machen das):
const shortenedText = longPrompt.substring(0, 1000) + "...";

// ‚úÖ RICHTIG (Evoki V2.0):
const fullText = longPrompt;  // Komplett senden, keine K√ºrzung!

// Token-Budget-Check:
if (totalTokens > 1_000_000) {
    // Wenn zu gro√ü: Reduziere ANZAHL der Paare (nicht L√§nge!)
    selectedPairs = selectedPairs.slice(0, 2);  // 3 ‚Üí 2 Paare
    // ABER: Jedes Paar bleibt VOLLTEXT!
}
Warum Volltext?Trauma-Kontexte d√ºrfen nicht fragmentiert werdenNarrative Koh√§renz ist kritisch"Zwillinge" k√∂nnte am Ende eines 80k-Prompts stehenVerk√ºrzung w√ºrde Kontext zerst√∂renToken-Budget Management:JavaScript// Berechne Token-Count f√ºr alle 3 Paare
const pair1Tokens = calculateSetTokens(pair1);  // 19k
const pair2Tokens = calculateSetTokens(pair2);  // 57k
const pair3Tokens = calculateSetTokens(pair3);  // 12k

const totalContext = pair1Tokens + pair2Tokens + pair3Tokens;  // 88k

// Wenn zu gro√ü: Priorisiere nach Relevanz
if (totalContext > 500_000) {  // 500k Context-Limit
    // Sortiere nach combined_score
    const sortedPairs = [pair1, pair2, pair3].sort((a, b) => 
        b.combined_score - a.combined_score
    );
    
    // Nimm nur Top 2 (oder Top 1 bei SEHR gro√üen Prompts)
    selectedPairs = sortedPairs.slice(0, 2);
    
    console.log(`‚ö†Ô∏è Token-Budget: Reduziert von 3 auf 2 Paare (${totalContext} ‚Üí ${pair1Tokens + pair2Tokens})`);
}
PERFECT AGREEMENT Prompts haben VORRANG:JavaScript// Wenn ein Paar PERFECT AGREEMENT hat ‚Üí IMMER behalten!
const perfectPairs = allPairs.filter(p => p.agreement === 'PERFECT');
const otherPairs = allPairs.filter(p => p.agreement !== 'PERFECT');

// Budget: 500k Context-Limit
let selectedPairs = [];
let currentTokens = 0;

// 1. PERFECT Paare zuerst (garantiert dabei)
for (const pair of perfectPairs) {
    if (currentTokens + pair.tokenCount <= 500_000) {
        selectedPairs.push(pair);
        currentTokens += pair.tokenCount;
    }
}

// 2. Restliche Paare nach Score
for (const pair of otherPairs.sort((a, b) => b.combined_score - a.combined_score)) {
    if (currentTokens + pair.tokenCount <= 500_000 && selectedPairs.length < 3) {
        selectedPairs.push(pair);
        currentTokens += pair.tokenCount;
    }
}
5. PRAKTISCHES BEISPIEL (Real-World Szenario):User-Prompt: "Erz√§hl von den Zwillingen im Kindergarten" (20 Tokens)FAISS-Suche:Findet Chunk 2 von Prompt #4523 (enth√§lt "Zwillinge")Reassembly: L√§dt alle 15 Chunks von #4523 ‚Üí 12k Tokens komplettSQL-Suche:Findet Prompt #4523 durch Metriken (A=0.85, PCI=0.72)L√§dt Prompt-Text aus Quelldatenbank ‚Üí 12k TokensDuplikat-Check:JavaScriptTimecode: 2025-06-15T14:32:11Z ‚úÖ GLEICH
Prompt-ID: #4523 ‚úÖ GLEICH
Author: "User" ‚úÖ GLEICH
Text: "Es war einmal..." (12k) ‚úÖ 1:1 MATCH

‚Üí PERFECT AGREEMENT ERKANNT!
‚Üí Wird NICHT 2x gesendet
‚Üí Bekommt ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MARKER ‚≠ê‚≠ê‚≠ê
‚Üí Doppelte Gewichtung (2.0x)
Context-Weaving (¬±2 Prompts):Prompt #4521 (8k) ‚Üê 2 vorherPrompt #4522 (5k) ‚Üê 1 vorherPrompt #4523 (12k) ‚Üê HIT (PERFECT AGREEMENT!)Prompt #4524 (7k) ‚Üê 1 nachherPrompt #4525 (3k) ‚Üê 2 nachherSet-Tokens: 8k + 5k + 12k + 7k + 3k = 35k f√ºr Paar 1Weitere 2 Paare:Paar 2 (nur Metrik): 28k TokensPaar 3 (nur Semantik): 19k TokensTOTAL Context: 35k + 28k + 19k = 82k Tokens+ User-Prompt: 20 Tokens+ Response Budget: 8k Tokens (32%)= GESAMT: ~90k Tokens ‚úÖ Passt in 1M Limit!An Gemini gesendet:USER-PROMPT: "Erz√§hl von den Zwillingen im Kindergarten"

=== PAAR 1: ‚≠ê‚≠ê‚≠ê HIGH CONFIDENCE MATCH ‚≠ê‚≠ê‚≠ê ===
üî• BEIDE SUCHVERFAHREN FANDEN DIESEN KONTEXT UNABH√ÑNGIG! üî•

[8k Tokens Prompt #4521]
[5k Tokens Prompt #4522]
[12k Tokens Prompt #4523] ‚Üê SQL + FAISS beide fanden das!
[7k Tokens Prompt #4524]
[3k Tokens Prompt #4525]

=== PAAR 2: METRIK-DOMINANZ ===
[28k Tokens total...]

=== PAAR 3: SEMANTIK-DOMINANZ ===
[19k Tokens total...]

AUFGABE: Generiere kontextbasierte Antwort...
Gemini Response: ~8k Tokens (hochrelevant, weil PERFECT MATCH Context!)üéØ WARUM IST DAS BESSER ALS NUR FAISS ODER NUR SQL?Szenario 1: Nur FAISS (ohne SQL-Metriken)Findet "Zwillinge" nur wenn Wort schon gefallen ist√úbersieht Trigger-Patterns in MetrikenKann keine Trends in emotionaler Entwicklung erkennenSzenario 2: Nur SQL (ohne FAISS-Semantik)Findet nur numerisch √§hnliche Metriken√úbersieht konzeptionell √§hnliche Texte ("Geschwister" = "Zwillinge")Kann keine semantischen Verbindungen herstellenSzenario 3: ORCHESTRATOR (SQL + FAISS kombiniert)‚úÖ Findet Trigger-Patterns auch ohne exakte Text-√úbereinstimmung‚úÖ Findet semantisch √§hnliche Texte auch mit unterschiedlichen Metriken‚úÖ Vergleicht beide Methoden und erkennt Abweichungen‚úÖ W√§hlt 3 beste Paare mit unterschiedlichen St√§rken‚úÖ Webt Kontext ein (¬±2 Prompts = Geschichte)‚úÖ Gemini bekommt 15 hochrelevante Prompts statt 3 zuf√§lligerERGEBNIS:30-40% bessere Kontext-Qualit√§tWeniger False Positives (beide Methoden m√ºssen zustimmen)Mehr True Positives (wenn eine Methode findet, andere validiert)Bessere Gemini-Antworten (mehr relevanter Kontext)üîç SQL IM FRONTEND VS BACKEND - UNTERSCHIEDEFRAGE: "Was l√§uft wo? Unterschiede?"BACKEND-SQLite (Server):Wo: backend/data/evoki_v2_ultimate_FULL.dbZweck: - Vector DBs (W_m2, W_m5, W_p25, W_p5, etc.)Metrik-Datenbanken (1:1 Zuordnung Prompt ‚Üí Metriken)Chat-Historie (Quelldatenbank mit Original-Texten)Persistente Speicherung (bleibt nach Server-Neustart)Zugriff: Node.js Backend via better-sqlite3Gr√∂√üe: Mehrere GB (33.795 Chunks + Metriken)Performanz: Schnell (Server-Hardware, SSD)FRONTEND-SQLite (Browser):Wo: Im Browser (IndexedDB als Basis)Zweck:UI-State Caching (aktuelle Session, Messages)Offline-F√§higkeit (falls Backend offline)LocalStorage-Ersatz (gr√∂√üer als 4MB)Zugriff: React via better-sqlite3 (WASM-compiled!)Gr√∂√üe: Max 1-2 GB (Browser-Limit)Performanz: Langsamer (Browser, kein direkter Disk-Access)UNTERSCHIEDE:AspektBackend-SQLiteFrontend-SQLiteSpeicherortServer FestplatteBrowser IndexedDBGr√∂√üeUnbegrenzt (GB)Browser-Limit (~2GB)PersistenzPermanentNur im BrowserMulti-User‚úÖ JA (mehrere Clients)‚ùå NEIN (nur 1 User)Performanz‚ö°‚ö°‚ö° Schnell‚ö° LangsamUse CaseVector DBs, MetrikenUI-State, CachingPrivacyServer (sicherer)Browser (weniger sicher)UNSER SYSTEM NUTZT:Backend-SQLite (HAUPTSYSTEM):backend/data/
‚îú‚îÄ evoki_v2_ultimate_FULL.db      ‚Üê Chat-Historie (Quelldatenbank)
‚îú‚îÄ tempel_W_m2.db                 ‚Üê Vector DB Window -2
‚îú‚îÄ tempel_W_m5.db                 ‚Üê Vector DB Window -5
‚îú‚îÄ tempel_W_p25.db                ‚Üê Vector DB Window +25
‚îú‚îÄ tempel_metrics_1to1.db         ‚Üê 1:1 Metrik-Zuordnung
‚îú‚îÄ trialog_W_m2.db                ‚Üê Trialog Vector DBs
‚îî‚îÄ ... (insgesamt 12 DBs)
Frontend-SQLite (Optional, f√ºr Offline):Browser IndexedDB:
‚îú‚îÄ evoki_session_cache            ‚Üê Aktuelle Session
‚îú‚îÄ evoki_messages_cache           ‚Üê Messages f√ºr UI
‚îî‚îÄ evoki_metrics_preview          ‚Üê Metrik-Preview (nur aktuell)
EMPFEHLUNG:‚úÖ Backend-SQLite: BEHALTEN (f√ºr Vector DBs, Metriken, Persistenz)‚ùì Frontend-SQLite: - Entfernen wenn Offline-F√§higkeit nicht n√∂tigBehalten wenn User offline arbeiten sollAktuell: Wahrscheinlich NICHT genutzt (zu pr√ºfen!)üîÑ OFFENE FRAGEN (ERWEITERT)üîÑ OFFENE FRAGEN (ERWEITERT)TECHNISCHE FRAGEN:ChatbotPanel: Behalten, umbenennen oder l√∂schen?Snapshots: Evolution zu "Session Export" oder komplett weg?SQLite im Frontend: Warum? Kann entfernt werden?Genesis Anchor: Wann re-enablen? (nach welchem Meilenstein?)V1-Daten: Alle importieren oder nur letzten 3 Monate?Pipeline-Log: JSONL oder SQLite? (Performance vs. Queries)Trialog KB: Wann wird synapse_knowledge_base.faiss erstellt?Backend Health Check: Wie fixen ohne Backend zu killen?LocalStorage Limit: Backend-Persistenz implementieren?Chronik Rotation: Wie verhindern dass unbegrenzt w√§chst?NEUE KRITISCHE FRAGEN:1. Timeout-Strategie:Frontend Timeout erh√∂hen? 60s ‚Üí 120s oder dynamisch?Backend-Timeouts optimieren? Gemini 90s reduzieren?Progress-Updates implementieren? Server-Sent Events f√ºr Pipeline-Steps?2. FAISS-Fehlerbehandlung:Validation nach FAISS-Suche? Pr√ºfen ob Chunks gefunden wurden?Fallback-Strategie? Was tun wenn FAISS crasht? ‚Üí Nur Metriken nutzen?Error-Messaging? User informieren "Kontext-Suche fehlgeschlagen"?3. Python CLI Stabilit√§t:FAISS-Index im RAM halten? Separate Prozess statt CLI?Health-Check f√ºr Python? Pr√ºfen ob query.py √ºberhaupt funktioniert?Retry-Logic? Bei Timeout nochmal versuchen mit weniger Chunks?4. UI-Freezing verhindern:Virtualisierte Liste? Nur sichtbare Messages rendern?Lazy Loading? Alte Messages erst bei Scroll laden?Token-Limit f√ºr Rendering? Max 100k tokens im DOM?5. Race Conditions:AbortController bei Unmount? Request canceln wenn Component verschwindet?State-Management verbessern? Session in App.tsx statt Component?Request-Queue? Nur 1 Request gleichzeitig erlauben?ü§ñ INTELLIGENTE MODELL-AUSWAHL & DUAL-RESPONSE-STRATEGIEPROBLEM: Context-Window Limits vs Qualit√§tModell-√úbersicht (sortiert nach Qualit√§t):RangModelContext-WindowKosten/1MQualit√§tSpezialisierungü•á 1Claude Sonnet 4.5200K$3‚≠ê‚≠ê‚≠ê‚≠ê‚≠êKomplexe Reasoning, Trauma-Analyseü•à 2GPT-4 Turbo128K$10‚≠ê‚≠ê‚≠ê‚≠ê‚≠êAllround, sehr kreativü•â 3Gemini 2.5 Flash1M$0.10‚≠ê‚≠ê‚≠ê‚≠êGro√üe Kontexte, schnell, g√ºnstigDILEMMA:Beste Qualit√§t (Claude) hat kleinstes Context-Window (200K)Gr√∂√ütes Context-Window (Gemini) hat niedrigste Qualit√§tUser hat Prompts bis zu 80k + Context bis zu 500k = 580k Tokens!üéØ L√ñSUNG: INTELLIGENTE KASKADEN-AUSWAHLSTUFE 1: STANDARD-AUSWAHL (Single-Model-Strategy)JavaScriptfunction selectOptimalModel(totalTokens, contextPairs) {
    // Berechne Token-Count f√ºr alle 3 Paare
    const pair1Tokens = calculateSetTokens(contextPairs[0]);
    const pair2Tokens = calculateSetTokens(contextPairs[1]);
    const pair3Tokens = calculateSetTokens(contextPairs[2]);
    const totalContext = pair1Tokens + pair2Tokens + pair3Tokens;
    
    console.log(`üìä Token-Analyse: ${totalContext} Context + ${userPromptTokens} User-Prompt = ${totalTokens} total`);
    
    // INTELLIGENTE AUSWAHL (nach Context-Window):
    
    if (totalTokens <= 128_000) {
        // ‚úÖ Passt in GPT-4 Turbo (128K)
        return {
            model: 'GPT-4 Turbo',
            endpoint: 'https://api.openai.com/v1/chat/completions',
            apiKey: process.env.OPENAI_API_KEY,
            maxTokens: 128_000,
            cost: 10.0,  // $10 pro 1M
            quality: 5,
            reason: 'Beste Qualit√§t bei <128K Context'
        };
    }
    
    if (totalTokens <= 200_000) {
        // ‚úÖ Passt in Claude Sonnet 4.5 (200K)
        return {
            model: 'Claude Sonnet 4.5',
            endpoint: 'https://api.anthropic.com/v1/messages',
            apiKey: process.env.ANTHROPIC_API_KEY,
            maxTokens: 200_000,
            cost: 3.0,  // $3 pro 1M
            quality: 5,
            reason: 'Beste Qualit√§t + Trauma-Spezialisierung bei <200K Context'
        };
    }
    
    // ‚ùå Zu gro√ü f√ºr hochwertige Modelle
    if (totalTokens <= 1_000_000) {
        // ‚úÖ Nur Gemini 2.5 Flash kann 1M
        return {
            model: 'Gemini 2.5 Flash',
            endpoint: 'https://generativelanguage.googleapis.com/v1/models/gemini-2.5-flash',
            apiKey: process.env.GEMINI_API_KEY_1,
            maxTokens: 1_000_000,
            cost: 0.1,  // $0.10 pro 1M
            quality: 4,
            reason: 'Einziges Model mit 1M Context-Window'
        };
    }
    
    // ‚ùå Sogar zu gro√ü f√ºr Gemini ‚Üí Fehler!
    throw new Error(`Context zu gro√ü: ${totalTokens} tokens √ºberschreitet 1M Limit!`);
}
Beispiel-Ablauf (90k Tokens):User-Prompt: "Erz√§hl von den Zwillingen" (20 Tokens)
Context: 3 Paare √ó ~30k = 90k Tokens
Total: 90,020 Tokens

‚Üí 90k < 128k ‚Üí ‚úÖ GPT-4 Turbo ausgew√§hlt
‚Üí Beste Qualit√§t, passt ins Context-Window
STUFE 2: DUAL-RESPONSE-STRATEGIE (Split-Model-Strategy)Wenn Context > 200K f√ºr alle 3 Paare:JavaScriptfunction selectDualModelStrategy(totalTokens, contextPairs) {
    if (totalTokens > 200_000) {
        console.log(`‚ö†Ô∏è Context zu gro√ü f√ºr hochwertige Modelle (${totalTokens} > 200K)`);
        console.log(`üéØ DUAL-RESPONSE-STRATEGIE aktiviert!`);
        
        // 1. W√§hle BESTES Paar (meist PERFECT AGREEMENT)
        const bestPair = contextPairs.filter(p => p.agreement === 'PERFECT')[0] 
                      || contextPairs.sort((a, b) => b.combined_score - a.combined_score)[0];
        
        const bestPairTokens = calculateSetTokens(bestPair);
        
        // 2. Pr√ºfe ob BESTES Paar in hochwertiges Model passt
        if (bestPairTokens <= 128_000) {
            // ‚úÖ Bestes Paar passt in GPT-4
            return {
                strategy: 'DUAL_RESPONSE',
                primaryModel: {
                    model: 'GPT-4 Turbo',
                    pairs: [bestPair],  // Nur 1 Paar
                    tokens: bestPairTokens,
                    cost: 10.0,
                    quality: 5,
                    label: 'ü•á HOCHWERTIG (GPT-4)'
                },
                secondaryModel: {
                    model: 'Gemini 2.5 Flash',
                    pairs: contextPairs,  // ALLE 3 Paare
                    tokens: totalTokens,
                    cost: 0.1,
                    quality: 4,
                    label: 'üìö VOLLST√ÑNDIG (Gemini)'
                },
                parallelExecution: true,  // BEIDE parallel aufrufen
                displayBoth: true         // BEIDE Antworten im Chat zeigen
            };
        }
        
        if (bestPairTokens <= 200_000) {
            // ‚úÖ Bestes Paar passt in Claude
            return {
                strategy: 'DUAL_RESPONSE',
                primaryModel: {
                    model: 'Claude Sonnet 4.5',
                    pairs: [bestPair],  // Nur 1 Paar
                    tokens: bestPairTokens,
                    cost: 3.0,
                    quality: 5,
                    label: 'ü•á HOCHWERTIG (Claude)'
                },
                secondaryModel: {
                    model: 'Gemini 2.5 Flash',
                    pairs: contextPairs,  // ALLE 3 Paare
                    tokens: totalTokens,
                    cost: 0.1,
                    quality: 4,
                    label: 'üìö VOLLST√ÑNDIG (Gemini)'
                },
                parallelExecution: true,
                displayBoth: true
            };
        }
        
        // ‚ùå Sogar bestes Paar zu gro√ü f√ºr hochwertige Modelle
        // ‚Üí Nur Gemini mit allen 3 Paaren
        return {
            strategy: 'SINGLE_RESPONSE',
            primaryModel: {
                model: 'Gemini 2.5 Flash',
                pairs: contextPairs,
                tokens: totalTokens,
                cost: 0.1,
                quality: 4,
                label: 'üìö NUR GEMINI (zu gro√ü f√ºr andere)'
            }
        };
    }
}
Beispiel-Ablauf (350k Tokens):User-Prompt: "Erz√§hl von den Zwillingen" (20 Tokens)
Context: Paar 1 (120k) + Paar 2 (150k) + Paar 3 (80k) = 350k Tokens
Total: 350,020 Tokens

‚Üí 350k > 200k ‚Üí ‚ùå Zu gro√ü f√ºr Claude/GPT-4
‚Üí üéØ DUAL-RESPONSE-STRATEGIE aktiviert!

Paar 1 (PERFECT AGREEMENT): 120k Tokens
‚Üí 120k < 128k ‚Üí ‚úÖ Passt in GPT-4!

STRATEGIE:
‚îú‚îÄ ü•á PRIMARY: GPT-4 Turbo (nur Paar 1 = 120k)
‚îÇ  ‚îî‚îÄ Beste Qualit√§t, fokussiert auf wichtigsten Kontext
‚îî‚îÄ üìö SECONDARY: Gemini 2.5 Flash (alle 3 Paare = 350k)
   ‚îî‚îÄ Vollst√§ndiger Kontext, alle Perspektiven

‚Üí BEIDE parallel aufrufen
‚Üí BEIDE Antworten im Chat anzeigen
üîÑ PARALLELE AUSF√úHRUNG (Backend-Implementation)JavaScriptasync function executeModelStrategy(strategy, userPrompt, contextPairs) {
    if (strategy.strategy === 'SINGLE_RESPONSE') {
        // Normale Ausf√ºhrung (nur 1 Model)
        const response = await callLLM(
            strategy.primaryModel.model,
            userPrompt,
            strategy.primaryModel.pairs
        );
        
        return {
            responses: [{
                model: strategy.primaryModel.model,
                label: strategy.primaryModel.label,
                text: response.text,
                tokens: response.usage.total_tokens,
                cost: response.usage.total_tokens / 1_000_000 * strategy.primaryModel.cost
            }]
        };
    }
    
    if (strategy.strategy === 'DUAL_RESPONSE') {
        // Parallele Ausf√ºhrung (2 Models gleichzeitig)
        console.log('üîÑ Starte DUAL-RESPONSE: 2 Models parallel...');
        
        const [primaryResponse, secondaryResponse] = await Promise.all([
            callLLM(
                strategy.primaryModel.model,
                userPrompt,
                strategy.primaryModel.pairs  // Nur 1 Paar
            ),
            callLLM(
                strategy.secondaryModel.model,
                userPrompt,
                strategy.secondaryModel.pairs  // ALLE 3 Paare
            )
        ]);
        
        console.log('‚úÖ BEIDE Antworten empfangen!');
        
        return {
            responses: [
                {
                    model: strategy.primaryModel.model,
                    label: strategy.primaryModel.label,
                    text: primaryResponse.text,
                    tokens: primaryResponse.usage.total_tokens,
                    cost: primaryResponse.usage.total_tokens / 1_000_000 * strategy.primaryModel.cost,
                    quality: strategy.primaryModel.quality,
                    contextPairs: strategy.primaryModel.pairs.length
                },
                {
                    model: strategy.secondaryModel.model,
                    label: strategy.secondaryModel.label,
                    text: secondaryResponse.text,
                    tokens: secondaryResponse.usage.total_tokens,
                    cost: secondaryResponse.usage.total_tokens / 1_000_000 * strategy.secondaryModel.cost,
                    quality: strategy.secondaryModel.quality,
                    contextPairs: strategy.secondaryModel.pairs.length
                }
            ]
        };
    }
}
üìö REFERENZENHaupt-README: README.md (mit Synapse Genesis Point)Architektur: ARCHITECTURE.json (auto-generiert)Setup: SETUP.mdCleanup Report: docs/CLEANUP_REPORT.mdV1 Reference: c:\evoki\ (Produktiv-System)Letztes Update: 28.12.2025 - Synapse (Explorer & Connector) ‚ö°Discovery Phase: 4/5 - LocalStorage, Startup, Dependencies, Error Handling vollst√§ndigN√§chste Review: Nach erstem erfolgreichen Tempel-Test