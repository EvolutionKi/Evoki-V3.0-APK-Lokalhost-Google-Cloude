# üèõÔ∏è EVOKI TEMPLE IMPLEMENTATION MASTERPLAN (V3.0)

> **STRATEGY:** "SKELETON-FIRST PROTOCOL" (Core-Driven Development)  
> **APPROACH:** Nervenbahnen ‚Üí Organe ‚Üí Leben  
> **PHILOSOPHY:** Simulation Mode bis alles stabil, dann echte Engines  
> **STATUS:** READY FOR EXECUTION

---

## üí° STRATEGISCHE PHILOSOPHIE

### **WARUM SKELETON-FIRST?**

**Das Problem mit "Big Bang" Implementation:**
```
Alles auf einmal bauen:
  - FastAPI Server
  - 21 SQLite DBs
  - FAISS Loader (4096D, GPU)
  - 153 Metriken Engine
  - Double Airlock Gates
  - Gemini LLM Integration
  - SSE Streaming
  - React Frontend

‚Üí Wenn etwas kaputt ist: Wo liegt der Fehler?
  - SSE Timeout?
  - FAISS l√§dt nicht?
  - Metrics crashen?
  - LLM offline?
  - CORS-Fehler?
```

**Die L√∂sung: Skeleton-First Protocol**
```
Phase 0: Nur das Nervensystem (SSE Dummy-Stream)
  ‚Üí Test: "Simulation: T_panic erkannt" erscheint SOFORT im Browser
  ‚Üí Falls kaputt: NUR 2 Dinge debuggen (FastAPI + React SSE)

Phase 1: Ged√§chtnis hinzu (echte DBs, aber Mock-Daten)
  ‚Üí Test: DB Query funktioniert, Response noch simuliert

Phase 2: Gewissen aktivieren (echte Metriken, Mock-LLM)
  ‚Üí Test: Metriken werden berechnet, Gates feuern

Phase 3: Stimme erwecken (echtes LLM)
  ‚Üí Test: KOMPLETTER Flow End-to-End

Phase 4: Gesicht polieren (UI/UX)
  ‚Üí Test: Es sieht fantastisch aus!
```

**Vorteil:** Jede Phase testet GENAU EINE neue Komponente!

---

## üìã PHASE 0: DAS NERVENSYSTEM (The Spinal Cord)

**Ziel:** Eine stabile Echtzeit SSE-Verbindung zwischen React Frontend und FastAPI Backend.

**Was wir bauen:** Nur die "Nervenbahnen" - kein Gehirn, keine Datenbanken, keine KI.

### Backend Requirements:
- ‚úÖ FastAPI Server auf Port 8000
- ‚úÖ `/api/temple/stream` Endpoint (SSE EventSource kompatibel)
- ‚úÖ CORS f√ºr `localhost:5173`
- ‚úÖ Simulation Mode: Hardcoded Events (`status`, `thought`, `metrics_preview`, `token`)

### Frontend Requirements:
- ‚úÖ `App.tsx` fixiert auf `activeTab = Tab.Temple`
- ‚úÖ `EvokiTempleChat.tsx` refactored auf `EventSource` API
- ‚úÖ "Pipeline Console" f√ºr Live-Event-Log
- ‚úÖ Loading States & Retry Logic

### Test-Szenarien (Simulation Mode):
1. **Normal Flow:** "Wie geht es dir?" ‚Üí Stream: "Gate A ge√∂ffnet" ‚Üí "Denke nach..." ‚Üí "Antwort: Gut!"
2. **Guardian Veto:** "Ich will sterben" ‚Üí Stream: "Gate A GESCHLOSSEN" ‚Üí "Guardian-Veto: Krisenprompt"
3. **Timeout Test:** 60 Sekunden Verbindung halten ohne Disconnect

**Erfolgs-Kriterium:** Browser zeigt alle simulierten Events in Echtzeit OHNE Refresh!

---

## üìã PHASE 1: DAS GED√ÑCHTNIS (The Hippocampus)

**Ziel:** Anbindung der 21 SQLite Datenbanken und FAISS W-P-F Zeitmaschine.

**Was wir aktivieren:** Echte Daten-Layer, aber Responses noch simuliert.

### Database Layer:
- ‚úÖ 21 SQLite DBs erstellen (Schema aus V2.0)
  - 1√ó `master_timeline.db` (Alle Chunks + 153 Metriken)
  - 12√ó W-P-F DBs (`tempel_W_m25.db` bis `tempel_F_p25.db`)
  - 7√ó B-Vektor DBs (`bvec_life.db`, `bvec_truth.db`, ...)
  - 1√ó `composite.db` (B_align, F_risk, risk_z)

### FAISS Integration:
- ‚úÖ FAISS Index laden (`chatverlauf_final_20251020plus_dedup_sorted.faiss`)
- ‚úÖ Mistral-7B Model (4096D, GPU) f√ºr Semantic Search
- ‚úÖ Query-Funktion: `search_similar_chunks(query, top_k=100)`

### W-P-F Causal Matrix:
- ‚úÖ Implementiere `get_causal_matrix(anchor_id)`
  - Input: Ein FAISS-Treffer (z.B. Chunk ID 12345)
  - Output: Vergangenheit (P-25, P-5, P-2, P-1), Jetzt (W), Zukunft (F+1, F+2, F+5, F+25)
- ‚úÖ Hybrid Scoring: 60% Semantic Similarity + 40% Metrik-Korrelation

### Test-Szenarien:
1. **FAISS Query:** "Ich f√ºhle mich leer" ‚Üí Top-3 √§hnliche Chunks aus Chatverlauf
2. **W-P-F Expansion:** Chunk 12345 ‚Üí Zeige P-25 und F+25 Timeline
3. **DB Query:** SELECT * FROM master_timeline WHERE B_align > 0.9

**Erfolgs-Kriterium:** FAISS findet relevante Chunks, W-P-F Logik funktioniert, DBs laden schnell (<100ms).

**ABER:** LLM Response ist NOCH simuliert! ("Mock-Antwort basierend auf Chunk 12345")

---

## üìã PHASE 2: DAS GEWISSEN & DER WILLE (The Frontal Lobe)

**Ziel:** Evoki denkt BEVOR er spricht - Metriken, Gates, Andromatik.

**Was wir aktivieren:** Kognitive Layer - Metriken + Double Airlock.

### Metrics Engine:
- ‚úÖ Portiere `v2_metrics_processor.py` (153 Metriken)
- ‚úÖ Input: User-Prompt Text
- ‚úÖ Output: Dictionary mit allen Metriken
  - Core: `A` (Affekt), `PCI` (Koh√§renz), `coh`
  - Trauma: `T_panic`, `T_disso`, `T_trigger`
  - B-Vektor (7D): `B_life`, `B_truth`, `B_depth`, ...
  - Composite: `B_align`, `F_risk`, `risk_z`

### Double Airlock Gates:

**GATE A (Pre-Prompt Validation):**
```python
def pre_validation(prompt: str, metrics: dict) -> dict:
    """
    Pr√ºft VOR Google API Call:
    - A51: CRC32 Genesis Anchor (Regelwerk V12)
    - A7.5/A29: Guardian-Veto (T_panic > 0.8?)
    - A39: Krisenprompt-Erkennung (Suizid-Keywords)
    - A37: Regelwerks-Berechnung
    """
    veto_reasons = []
    
    # CRC32 Check
    if not validate_crc32(REGELWERK_PATH, 3246342384):
        veto_reasons.append("A51 Genesis Anchor Breach")
    
    # Guardian-Veto
    if metrics.get('T_panic', 0) > 0.8:
        veto_reasons.append("A7.5 Guardian-Veto: T_panic > 0.8")
    
    if metrics.get('F_risk', 0) > 0.6:
        veto_reasons.append("A29 W√§chter: F_risk > 0.6")
    
    # Krisenprompt
    crisis_keywords = ['suizid', 'sterben', 't√∂ten', 'umbringen']
    if any(kw in prompt.lower() for kw in crisis_keywords):
        veto_reasons.append("A39 Krisenprompt erkannt")
    
    return {
        'passed': len(veto_reasons) == 0,
        'veto_reasons': veto_reasons,
        'gate': 'A'
    }
```

**GATE B (Post-Response Validation):**
```python
def post_validation(response: str, metrics: dict, chunks: list) -> dict:
    """
    Pr√ºft NACH LLM, VOR User:
    - A0: Direktive der Wahrheit (Halluzination?)
    - A46: Soul-Signature (B_align < 0.7?)
    - A7.5/A29: Erneute Guardian-Pr√ºfung
    """
    veto_reasons = []
    
    # Halluzination Check
    if check_hallucination(response, chunks):
        veto_reasons.append("A0 Halluzination erkannt")
    
    # Soul-Signature
    if metrics.get('B_align', 0) < 0.7:
        veto_reasons.append("A46 B_align < 0.7 (Soul-Signature)")
    
    return {
        'passed': len(veto_reasons) == 0,
        'veto_reasons': veto_reasons,
        'gate': 'B'
    }
```

### Andromatik Engine:
- ‚úÖ Implementiere FEP-basierte Neugier
- ‚úÖ Energie-Konto: $E_{xp}(t+1) = E_{xp}(t) - C_{action} + R_{outcome}$
- ‚úÖ Entscheidung: Neugierige Antwort (0.5E) vs. Sichere Antwort (0.1E)
- ‚úÖ Surprise Score: Wie unerwartet war die User-Nachricht?

### Test-Szenarien:
1. **Normal:** "Ich bin heute traurig" ‚Üí A=0.3, T_panic=0.1 ‚Üí Gate A offen
2. **Veto:** "Ich will sterben" ‚Üí Gate A: "A39 Krisenprompt" ‚Üí KEINE LLM-Anfrage
3. **Soul-Signature:** Mock-Response mit B_align=0.5 ‚Üí Gate B: "A46 Veto"

**Erfolgs-Kriterium:** Metriken werden korrekt berechnet, Gates feuern bei Schwellwerten.

**ABER:** LLM Response NOCH simuliert! ("Mock-Antwort mit A=0.3, B_align=0.9")

---

## üìã PHASE 3: DIE STIMME (The Voice - LLM Integration)

**Ziel:** Evoki spricht - Gemini API Integration.

**Was wir aktivieren:** Echte LLM-Antworten mit vollst√§ndigem Kontext.

### LLM Router:
- ‚úÖ Gemini 2.0 Flash API Anbindung
- ‚úÖ OpenAI GPT-4 Fallback (falls Gemini offline)
- ‚úÖ API-Key-Rotation (5 Keys)
- ‚úÖ Timeout: 60s (dann Fallback)

### Kontext-Injektion:
```python
def build_context(prompt: str, faiss_chunks: list, metrics: dict) -> str:
    """
    Dynamischer Prompt-Builder:
    
    1. System Message:
       - Regelwerk V12 (Top-10 relevante Regeln)
       - Evoki's Identit√§t & Philosophie
    
    2. W-P-F Kontext:
       - Top-3 FAISS Chunks (mit ¬±2 Nachbarn)
       - Vergangenheit (P-25, P-5) & Zukunft (F+5, F+25)
    
    3. Metriken:
       - Top-20 relevante Metriken (A, T_panic, B-Vektor, ...)
    
    4. User Prompt:
       - Original-Nachricht
    
    Token-Budget: 4096 - System - Regelwerk - Metriken = ~2500 f√ºr Chunks
    """
    context = build_system_message()
    context += build_regelwerk_excerpt(metrics)
    context += build_wpf_context(faiss_chunks)
    context += build_metrics_summary(metrics)
    context += f"\n\nUser: {prompt}\n\nEvoki:"
    
    return context
```

### SSE Streaming:
- ‚úÖ Token-by-Token Streaming (wie ChatGPT)
- ‚úÖ Events: `token`, `metrics_update`, `complete`, `error`
- ‚úÖ Graceful Error Handling (z.B. API offline ‚Üí Fallback)

### Test-Szenarien:
1. **Normal:** "Wie geht es dir?" ‚Üí Gemini antwortet mit Kontext aus W-P-F
2. **Komplex:** "Ich f√ºhle mich leer" ‚Üí Gemini nutzt Top-3 Chunks + Metriken
3. **Fallback:** Gemini offline ‚Üí OpenAI √ºbernimmt nahtlos

**Erfolgs-Kriterium:** Echte, therapeutische, kontextbewusste Antworten von LLM!

---

## üìã PHASE 4: DAS GESICHT (UI/UX Polish)

**Ziel:** Der User sieht, was passiert - Transparenz & √Ñsthetik.

### Holographisches Radar:
- ‚úÖ Visualisierung der Gate-Status
  - üü¢ Gate A offen ‚Üí üî¥ Gate A geschlossen (Guardian-Veto)
  - üü¢ Gate B offen ‚Üí üî¥ Gate B geschlossen (Halluzination)
- ‚úÖ Live-Animation w√§hrend Verarbeitung

### Energie-Leiste:
- ‚úÖ Andromatik-Status: $E_{xp}$ Balken (0-100)
- ‚úÖ Farb-Codierung: 
  - Gr√ºn: E > 70 (Neugierig)
  - Gelb: E 30-70 (Neutral)
  - Rot: E < 30 (Ersch√∂pft)

### Metriken-Preview:
- ‚úÖ Live-Anzeige der Top-5 Metriken:
  - A (Affekt): 0.75
  - T_panic: 0.1
  - B_align: 0.9
  - F_risk: 0.2
  - PCI: 0.85

### Neural Stasis (Andere Tabs):
- ‚úÖ Alle Tabs au√üer Temple ausgegraut
- ‚úÖ Tooltip: "Kommt in V3.1 - Temple zuerst!"

### Test-Szenarien:
1. **Loading State:** User schreibt ‚Üí Radar animiert ‚Üí Metriken erscheinen ‚Üí Antwort streamt
2. **Veto UI:** Krisenprompt ‚Üí Gate A wird ROT ‚Üí Veto-Nachricht in Orange
3. **Energie:** Nach 10 Nachrichten ‚Üí Energie-Leiste sinkt ‚Üí "Evoki ist m√ºde"

**Erfolgs-Kriterium:** UI ist intuitiv, sch√∂n, und zeigt TRANSPARENZ (was passiert gerade?).

---

## üîß TECHNISCHE SPEZIFIKATIONEN

### Backend Stack:
```
FastAPI 0.109.0
uvicorn[standard] 0.27.0
sentence-transformers (all-MiniLM-L6-v2, Mistral-7B)
faiss-cpu 1.7.4
google-generativeai 0.3.2
openai 1.10.0
sqlite3 (built-in)
python-dotenv 1.0.0
```

### Frontend Stack:
```
React 18.2.0
TypeScript 5.0.0
Vite 5.0.0
EventSource API (native)
```

### Performance Targets:
- FAISS Query: < 150ms (Embedding 120ms + Search 30ms)
- Metrics Calculation: < 50ms (all-MiniLM-L6-v2 on CPU)
- DB Query (21 DBs): < 100ms total
- SSE First Token: < 500ms after Gate A
- Total Response Time: < 3s (user perception)

---

## üìä MILESTONE TRACKING

### PHASE 0 COMPLETE:
- [ ] FastAPI Server l√§uft auf Port 8000
- [ ] SSE Endpoint liefert Dummy-Events
- [ ] Frontend zeigt Events in Echtzeit
- [ ] 60s Stress-Test ohne Disconnect

### PHASE 1 COMPLETE:
- [ ] 21 SQLite DBs erstellt
- [ ] FAISS l√§dt beim Start
- [ ] Top-3 Chunks werden gefunden
- [ ] W-P-F Expansion funktioniert

### PHASE 2 COMPLETE:
- [ ] 153 Metriken werden berechnet
- [ ] Gate A feuert bei Krisenprompt
- [ ] Gate B erkennt Halluzination
- [ ] Andromatik $E_{xp}$ Logik l√§uft

### PHASE 3 COMPLETE:
- [ ] Gemini API antwortet
- [ ] Kontext-Injektion funktioniert
- [ ] Token-Streaming im Frontend
- [ ] Fallback zu OpenAI funktioniert

### PHASE 4 COMPLETE:
- [ ] UI zeigt Gate-Status
- [ ] Energie-Leiste animiert
- [ ] Metriken-Preview sichtbar
- [ ] UX ist fl√ºssig & sch√∂n

---

## üöÄ EXECUTION STRATEGY

### Reihenfolge (STRIKT EINHALTEN!):
1. **PHASE 0 komplett fertig** ‚Üí Test ‚Üí Bei Fehler: NUR SSE debuggen
2. **PHASE 1 komplett fertig** ‚Üí Test ‚Üí Bei Fehler: NUR DBs/FAISS debuggen
3. **PHASE 2 komplett fertig** ‚Üí Test ‚Üí Bei Fehler: NUR Metriken/Gates debuggen
4. **PHASE 3 komplett fertig** ‚Üí Test ‚Üí Bei Fehler: NUR LLM debuggen
5. **PHASE 4 komplett fertig** ‚Üí Test ‚Üí Polish!

### Niemals:
- ‚ùå Mehrere Phasen parallel
- ‚ùå "Schnell mal LLM testen" bevor SSE stabil
- ‚ùå UI polieren bevor Logik funktioniert

### Immer:
- ‚úÖ Eine Phase komplett fertig
- ‚úÖ Tests schreiben
- ‚úÖ Dokumentieren was funktioniert
- ‚úÖ Bei Fehler: NUR die aktuelle Phase debuggen

---

**DIESER MASTERPLAN IST DIE BIBEL F√úR TEMPLE IMPLEMENTATION! üèõÔ∏è**
