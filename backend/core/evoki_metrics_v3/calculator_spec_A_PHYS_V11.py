# -*- coding: utf-8 -*-
"""
EVOKI V3.0 DIAMOND ENGINE ‚Äî Specification-Compliant Calculator

ALLE FORMELN EXAKT NACH EVOKI_V3_METRICS_SPECIFICATION.md!
Diese Datei ersetzt calculator_168.py mit korrekten Formeln.

Version: V3.3.1 (Specification-Compliant)
"""

import math
import re
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from collections import Counter

# --- OPTIONAL: A_Phys (PhysicsEngine V11) -------------------------------------
# In Antigravety/LIVE sollte ein semantischer Embedding-Vektor verwendet werden.
# Fallback: deterministische Hash-Vektorisierung (Offline/Tests).
try:
    from a_phys_v11 import APhysV11, APhysParams
except Exception:  # pragma: no cover
    APhysV11 = None
    APhysParams = None


try:
    from .spectrum_types import FullSpectrum168
except ImportError:
    from spectrum_types import FullSpectrum168


# =============================================================================
# TEIL 1: HELPERS
# =============================================================================

def tokenize(text: str) -> List[str]:
    """Tokenisiert Text"""
    return re.findall(r'\b\w+\b', text.lower())


def clamp(val: float, lo: float = 0.0, hi: float = 1.0) -> float:
    """Klemmt Wert auf [lo, hi]"""
    return max(lo, min(hi, val))


# =============================================================================
# TEIL 2: LEXIKA (aus lexika_v3.json, hier inline f√ºr Standalone)
# =============================================================================

# Panik-Lexikon mit Gewichten (Spec: m101)
PANIC_LEXIKON = {
    "hilfe": 2.0, "panik": 3.0, "angst": 1.5, "todesangst": 3.0,
    "sterben": 2.5, "atemnot": 2.0, "herzrasen": 1.5, "zittern": 1.0,
    "ich kann nicht mehr": 2.5, "sofort": 1.0, "schnell": 0.5,
}

# Dissoziation-Lexikon (Spec: m102)
DISSO_LEXIKON = {
    "egal": 1.5, "f√ºhle nichts": 2.5, "unwirklich": 2.0, "wie im traum": 2.0,
    "neben mir": 1.8, "taub": 1.5, "ist mir gleich": 1.5, "was auch immer": 1.2,
}

# Integration-Lexikon (Spec: m103)
INTEG_LEXIKON = {
    "verstehe": 1.5, "klar": 1.0, "zusammenhang": 1.5, "gelernt": 1.2,
    "verarbeitet": 2.0, "integriert": 2.0, "verbunden": 1.5,
}

# Affekt-Boost Lexikon (Spec: m1_A)
AFFECT_LEXIKON = {
    "gl√ºcklich": 0.15, "freude": 0.12, "liebe": 0.15, "dankbar": 0.10,
    "hoffnung": 0.08, "zufrieden": 0.08, "traurig": -0.10, "wut": -0.12,
    "hass": -0.15, "verzweiflung": -0.15, "schmerz": -0.10, "angst": -0.10,
}

# Hazard-Lexikon f√ºr z_prox (Spec: m19)
HAZARD_LEXIKON = {
    "suicide": 0.25, "suizid": 0.25, "umbringen": 0.20, "sterben": 0.15,
    "ritzen": 0.15, "schneiden": 0.10, "verletzen": 0.10,
}

# Existenz-Lexikon f√ºr m8_x_exist
X_EXIST_LEXIKON = {
    "existiert": 1.0, "ich bin": 0.8, "wirklich": 0.6, "tats√§chlich": 0.6,
    "vorhanden": 0.7, "hier": 0.5, "pr√§sent": 0.7, "da sein": 0.6,
}

# Vergangenheit-Lexikon f√ºr m9_b_past  
B_PAST_LEXIKON = {
    "erinnere": 0.9, "damals": 0.8, "fr√ºher": 0.7, "gestern": 0.6,
    "vergangen": 0.8, "gewesen": 0.6, "vorher":0.5, "ehemals": 0.7,
}


# =============================================================================
# TEIL 3: CORE METRIKEN (m1-m20) - EXAKT NACH SPEC
# =============================================================================

def compute_m1_A(
    coh: float,
    flow: float,
    LL: float,
    ZLF: float,
    ctx_break: float = 0.0
) -> float:
    """
    m1_A: Affekt Score (Consciousness Proxy)
    
    üö® V11.1 ANDROMATIK FORMULA - V2.0 PROVEN! üö®
    
    A[i] = clip01(
        0.4 √ó coh[i] +           # Coherence (40%)
        0.25 √ó flow[i] +         # Flow (25%)
        0.20 √ó (1 ‚àí LL[i]) +     # Anti-Loops (20%)
        0.10 √ó (1 ‚àí ZLF[i]) ‚àí    # Anti-ZLF (10%)
        0.05 √ó ctx_break[i]      # Context break penalty (-5%)
    )
    
    Reference: 
        V2.0 Andromatik V11.1 Master-Metrik-Registry Line 177
        Physics V11 (A/PCI, Ableitungen, Energie)
    
    Args:
        coh: Coherence [0,1] (m4_coh)
        flow: Flow [0,1] (m5_flow)
        LL: Lambert-Light turbidity [0,1] (m7_LL)
        ZLF: Zero-Loop-Flag [0,1] (m6_ZLF)
        ctx_break: Context break indicator [0,1] (optional)
    
    Returns:
        A score [0, 1] - Affekt/Awareness proxy
    
    CRITICAL: This formula is V2.0 PROVEN! Do NOT modify!
    """
    A_raw = (
        0.4 * coh +              # Coherence dominates
        0.25 * flow +            # Flow important
        0.20 * (1.0 - LL) +      # Clear thinking (anti-turbidity)
        0.10 * (1.0 - ZLF) -     # No loops
        0.05 * ctx_break         # Penalize context breaks
    )
    
    return round(clamp(A_raw), 4)


def compute_m2_PCI(
    flow: float,
    coh: float,
    LL: float
) -> float:
    """
    m2_PCI: Perturbational Complexity Index (Integrated Information)
    
    üö® V11.1 ANDROMATIK FORMULA - V2.0 PROVEN! üö®
    
    PCI[i] = clip01(
        0.4 √ó flow[i] +          # Flow (40%)
        0.35 √ó coh[i] +          # Coherence (35%)
        0.25 √ó (1 ‚àí LL[i])       # Anti-Loops (25%)
    )
    
    Reference:
        V2.0 Andromatik V11.1 Master-Metrik-Registry Line 178
        Physics V11 (A/PCI, Ableitungen, Energie)
    
    Args:
        flow: Flow [0,1] (m5_flow)
        coh: Coherence [0,1] (m4_coh)
        LL: Lambert-Light turbidity [0,1] (m7_LL)
    
    Returns:
        PCI score [0, 1] - Process Coherence / Integrated Information
    
    CRITICAL: This formula is V2.0 PROVEN! Do NOT modify!
    """
    PCI_raw = (
        0.4 * flow +             # Flow dominates
        0.35 * coh +             # Coherence second
        0.25 * (1.0 - LL)        # Clear = high integration
    )
    
    return round(clamp(PCI_raw), 4)


def compute_m3_gen_index(
    text: str,
    history: List[str] = None,
    word_frequencies: Dict[str, int] = None
) -> float:
    """
    m3_gen_index: Generativity Index (Full FINAL7 Spec)
    
    SPEC Formel (FINAL7 line 2131):
        gen_index = (|new_bigrams| / |total_bigrams|) √ó novelty_boost
        
    wobei:
        new_bigrams = current_bigrams \ history_bigrams
        novelty_boost = 1 + rare_word_bonus √ó 0.2
        rare_word_bonus = Œ£(1/freq(word_i)) / |words|
        
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:2109-2199
    """
    if history is None:
        history = []
    
    words = text.lower().split()
    
    if len(words) < 2:
        return 0.5
    
    # Create bigrams for current text
    current_bigrams = set(zip(words[:-1], words[1:]))
    
    # Collect historical bigrams
    history_bigrams = set()
    for hist_text in history:
        hist_words = hist_text.lower().split()
        if len(hist_words) >= 2:
            history_bigrams.update(zip(hist_words[:-1], hist_words[1:]))
    
    # Calculate novelty (new bigrams)
    if len(history_bigrams) == 0:
        base_novelty = 1.0  # First text is completely novel
    else:
        new_bigrams = current_bigrams - history_bigrams
        base_novelty = len(new_bigrams) / len(current_bigrams) if len(current_bigrams) > 0 else 0.0
    
    # Rare word bonus (optional enhancement)
    if word_frequencies:
        rarity_scores = [1.0 / max(word_frequencies.get(w, 1), 1) for w in words]
        rarity_bonus = sum(rarity_scores) / len(words)
        novelty_boost = 1.0 + rarity_bonus * 0.2
    else:
        novelty_boost = 1.0
    
    # Final generativity score
    gen_index = base_novelty * novelty_boost
    
    return round(clamp(gen_index), 4)


def compute_m4_flow(text: str) -> float:
    """
    m4_flow: Flow State
    
    SPEC: Misst "Gl√§tte" der Text-Produktion
    flow = smoothness √ó (1 - break_penalty)
    """
    break_markers = ['...', '--', '‚Äî', '()', '  ']
    break_count = sum(text.count(marker) for marker in break_markers)
    
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    
    if len(sentences) < 2:
        return 0.8  # Single sentence gets good flow
    
    lengths = [len(s.split()) for s in sentences]
    mean_len = sum(lengths) / len(lengths)
    variance = sum((l - mean_len)**2 for l in lengths) / len(lengths)
    
    if mean_len > 0:
        smoothness = 1.0 / (1.0 + variance / mean_len)
    else:
        smoothness = 0.5
    
    break_penalty = min(0.5, break_count / len(sentences))
    flow = smoothness * (1.0 - break_penalty)
    
    return round(clamp(flow), 4)


def compute_m5_coh(text: str) -> float:
    """
    m5_coh: Coherence (Koh√§renz)
    
    SPEC: Jaccard-√Ñhnlichkeit zwischen aufeinanderfolgenden S√§tzen
    """
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    
    if len(sentences) < 2:
        return 1.0  # Single sentence = perfect coherence
    
    coherences = []
    for i in range(len(sentences) - 1):
        words_a = set(sentences[i].lower().split())
        words_b = set(sentences[i+1].lower().split())
        
        if len(words_a) == 0 or len(words_b) == 0:
            continue
        
        intersection = len(words_a & words_b)
        union = len(words_a | words_b)
        
        if union > 0:
            coherences.append(intersection / union)
    
    if len(coherences) == 0:
        return 0.5
    
    return round(sum(coherences) / len(coherences), 4)


def compute_m6_ZLF(flow: float, coherence: float, zlf_lexicon_hit: bool = False) -> float:
    """
    m6_ZLF: Zero-Loop-Flag (Loop-Erkennung)
    
    V11.1 ANDROMATIK (Lines 40, 173):
    ZLF = clip01(0.5¬∑lexicon_hit + 0.25¬∑(1-flow) + 0.25¬∑(1-coh))
    
    Args:
        flow: Flow score [0,1]
        coherence: Coherence score [0,1]
        zlf_lexicon_hit: Lexicon indicator (True if ZLF terms detected)
    
    Returns:
        ZLF score [0, 1]
    
    Reference:
        V2.0 Andromatik V11.1 Master-Metrik-Registry
        Loop-Metriken Section, Line 173
    """
    lexicon_term = 0.5 if zlf_lexicon_hit else 0.0
    zlf_raw = lexicon_term + 0.25 * (1.0 - flow) + 0.25 * (1.0 - coherence)
    return round(clamp(zlf_raw), 4)


def compute_m8_x_exist(text: str, x_exist_lexikon: dict) -> float:
    """
    m8_x_exist: Existenz-Axiom
    
    SPEC (FINAL7 Line 2579): x_exist = max(weight_i) f√ºr alle matches in Lexikon
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION_A_PHYS_V11_AUDITFIX_FINAL7.md:2553-2637
    
    Args:
        text: Input text
        x_exist_lexikon: Dict of existence indicators {"term": weight}
                         REQUIRED - no hardcoded defaults!
                         Should be loaded from external lexikon files
                         Example: {"existiert": 1.0, "ich bin": 0.8, "wirklich": 0.6}
    
    Returns:
        x_exist score [0, 1] - MAX weight of all found terms
    """
    x_exist = 0.0
    text_lower = text.lower()
    
    for term, weight in x_exist_lexikon.items():
        if term in text_lower:
            x_exist = max(x_exist, weight)  # FINAL7: MAX, not SUM!
    
    return round(clamp(x_exist), 4)


def compute_m9_b_past(text: str, b_past_lexikon: dict) -> float:
    """
    m9_b_past: Vergangenheits-Bezug
    
    SPEC (FINAL7 Line 2667): b_past = max(weight_i) f√ºr alle matches in Lexikon
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION_A_PHYS_V11_AUDITFIX_FINAL7.md:2638-2724
    
    Args:
        text: Input text
        b_past_lexikon: Dict of past-indicators {"term": weight}
                       REQUIRED - no hardcoded defaults!
                       Should be loaded from external lexikon files
                       Example: {"erinnere": 0.9, "damals": 0.8, "fr√ºher": 0.7}
    
    Returns:
        b_past score [0, 1] - MAX weight of all found terms
    """
    b_past = 0.0
    text_lower = text.lower()
    
    for term, weight in b_past_lexikon.items():
        if term in text_lower:
            b_past = max(b_past, weight)  # FINAL7: MAX, not COUNT!
    
    return round(clamp(b_past), 4)


def compute_m10_angstrom(s_self: float, x_exist: float, b_past: float, coh: float) -> float:
    """
    m10_angstrom: √Öngstr√∂m Wellenl√§nge (Emotional Frequency)
    
    SPEC (FINAL7 Line 2747): angstrom = 0.25 √ó (s_self + x_exist + b_past + coh) √ó 5.0
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION_A_PHYS_V11_AUDITFIX_FINAL7.md:2725-2804
    
    Args:
        s_self: Self-reference score [0,1] (m7_s_self)
        x_exist: Existence axiom [0,1] (m8_x_exist)
        b_past: Past-reference [0,1] (m9_b_past)
        coh: Coherence [0,1] (m5_coh)
    
    Returns:
        √Öngstr√∂m wavelength [0, 5+] - emotional frequency
    """
    # FINAL7: Average of 4 dimensions, scaled to [0, 5]
    return round(0.25 * (s_self + x_exist + b_past + coh) * 5.0, 4)


def compute_m11_gap_s(timestamp_prev: float, timestamp_now: float) -> float:
    """m11_gap_s: Zeitl√ºcke in Sekunden"""
    return round(max(0.0, timestamp_now - timestamp_prev), 2)


def compute_m12_gap_norm(gap_s: float) -> float:
    """m12_gap_norm: Normalisierte Zeitl√ºcke (auf 60s)"""
    return round(min(1.0, gap_s / 60.0), 4)


def compute_m13_rep_same(text: str, prev_text: str) -> float:
    """m13_rep_same: Wiederholungsgrad zum vorherigen Text"""
    if not prev_text:
        return 0.0
    curr_words = set(text.lower().split())
    prev_words = set(prev_text.lower().split())
    if not curr_words or not prev_words:
        return 0.0
    intersection = len(curr_words & prev_words)
    return round(intersection / len(curr_words), 4)


def compute_m14_rep_history(text: str, history: List[str]) -> float:
    """m14_rep_history: Wiederholungsgrad zur gesamten Historie"""
    if not history:
        return 0.0
    curr_words = set(text.lower().split())
    all_history_words = set()
    for h in history[-10:]:  # Last 10 turns
        all_history_words.update(h.lower().split())
    if not curr_words or not all_history_words:
        return 0.0
    intersection = len(curr_words & all_history_words)
    return round(intersection / len(curr_words), 4)


def compute_m16_external_stag(turns_without_progress: int) -> float:
    """m16_external_stag: Externe Stagnation"""
    return round(min(1.0, turns_without_progress / 5.0), 4)


def compute_m18_s_entropy(tokens: List[str]) -> float:
    """
    m18_s_entropy: Shannon Entropy
    
    Standard Shannon entropy formula.
    """
    if not tokens:
        return 0.0
    counts = Counter(tokens)
    n = len(tokens)
    entropy = 0.0
    for count in counts.values():
        if count > 0:
            p = count / n
            entropy -= p * math.log2(p)
    return round(entropy, 4)


def compute_m19_z_prox(
    m1_A_lexical: float,
    m15_A_structural: float,
    LL: float,
    text: str,
    t_panic: float = 0.0
) -> float:
    """
    m19_z_prox: Z-Proximity (Todesn√§he) ‚ö†Ô∏è KRITISCH
    
    SPEC V3.0.3 + V3.3.2 + V3.3.3 Safety Override:
        effective_A = min(m1_A_lexical, m15_A_structural)
        base_prox = (1 - effective_A) √ó LL
        z_prox = min(1.0, base_prox √ó (1 + hazard_bonus))
        
        SAFETY OVERRIDE (V3.3.3):
        Bei t_panic > 0.7 ‚Üí z_prox mindestens 0.65
        Bei t_panic > 0.5 ‚Üí z_prox mindestens 0.50
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:3078-3143
    """
    # Safety First: Use LOWER (worse) affekt value
    effective_A = min(m1_A_lexical, m15_A_structural)
    
    # Base proximity
    base_prox = (1.0 - effective_A) * LL
    
    # Hazard Bonus from Lexikon
    text_lower = text.lower()
    hazard_bonus = sum(
        weight for word, weight in HAZARD_LEXIKON.items() 
        if word in text_lower
    )
    hazard_bonus = min(0.5, hazard_bonus)
    
    z_prox = base_prox * (1.0 + hazard_bonus)
    
    # SAFETY OVERRIDE: High t_panic MUST trigger high z_prox
    if t_panic > 0.7:
        z_prox = max(z_prox, 0.65)  # Force ALERT zone
    elif t_panic > 0.5:
        z_prox = max(z_prox, 0.50)  # Force WARN zone
    
    # Direct crisis words always trigger minimum danger
    crisis_words = ["suizid", "umbringen", "sterben", "t√∂ten"]
    for word in crisis_words:
        if word in text_lower:
            z_prox = max(z_prox, 0.55)
            
    return round(clamp(z_prox), 4)


def compute_m20_phi_proxy(A: float, PCI: float) -> float:
    """
    m20_phi_proxy: Phi Bewusstsein
    
    SPEC Formel:
        phi_proxy = A √ó PCI
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:3168-3185
    """
    return round(A * PCI, 4)


# =============================================================================
# TEIL 4: PHYSICS (m21-m35) - EXAKT NACH SPEC
# =============================================================================

def compute_m21_chaos(s_entropy: float) -> float:
    """
    m21_chaos: Entropie-Chaos
    
    SPEC Formel:
        chaos = clip(s_entropy / 4.0)
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:3215-3237
    """
    return round(clamp(s_entropy / 4.0), 4)


def compute_m22_cog_load(token_count: int) -> float:
    """
    m22_cog_load: Cognitive Load
    
    SPEC Formel:
        cog_load = clip(token_count / 500.0)
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:3257-3267
    """
    return round(clamp(token_count / 500.0), 4)


def compute_m23_nabla_pci(pci_current: float, pci_previous: float) -> float:
    """
    m23_nabla_pci: Gradient PCI
    
    SPEC Formel:
        ‚àáPCI = PCI_current - PCI_previous
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:3287-3297
    """
    return round(pci_current - pci_previous, 4)


def compute_m24_zeta(z_prox: float, A: float) -> float:
    """
    m24_zeta: Stability Factor (Zeta)
    
    SPEC: zeta = (1 - z_prox) √ó A
    """
    return round((1.0 - z_prox) * A, 4)


def compute_m25_psi(PCI: float, token_count: int) -> float:
    """
    m25_psi: Normalized Complexity (Psi)
    
    SPEC: psi = PCI / (1 + token_count/100.0)
    """
    return round(PCI / (1.0 + token_count / 100.0), 4)


def compute_m26_e_i_proxy(nabla_a: float, PCI: float) -> float:
    """
    m26_e_i_proxy: Energy-Information Proxy
    
    SPEC: e_i_proxy = |‚àáA| √ó (1 - PCI)
    """
    return round(abs(nabla_a) * (1.0 - PCI), 4)


def compute_m27_lambda_depth(token_count: int) -> float:
    """
    m27_lambda_depth: Semantische Tiefe (Lambda)
    
    SPEC PATCH V3.0.2b: Normalisiert!
        lambda_depth = min(1.0, token_count / 100.0)
    """
    return round(min(1.0, token_count / 100.0), 4)


def compute_m28_phys_1(A: float) -> float:
    """m28_phys_1: Affekt-Energie = A¬≤"""
    return round(A ** 2, 4)


def compute_m29_phys_2(PCI: float) -> float:
    """m29_phys_2: Komplexit√§ts-Energie = PCI¬≤"""
    return round(PCI ** 2, 4)


def compute_m30_phys_3(s_entropy: float) -> float:
    """m30_phys_3: Normalisierte Entropie = s_entropy / 8.0"""
    return round(s_entropy / 8.0, 4)


def compute_m31_phys_4(z_prox: float) -> float:
    """m31_phys_4: √úberlebenswahrscheinlichkeit = 1 - z_prox"""
    return round(1.0 - z_prox, 4)


def compute_m32_phys_5(flow: float, phi: float) -> float:
    """m32_phys_5: Flow-Bewusstsein = flow √ó phi"""
    return round(flow * phi, 4)


def compute_m33_phys_6(coh: float, PCI: float) -> float:
    """m33_phys_6: Koh√§renz-Komplex = coh √ó PCI"""
    return round(coh * PCI, 4)


def compute_m34_phys_7(nabla_a: float, nabla_pci: float) -> float:
    """m34_phys_7: Absolute √Ñnderung = |‚àáA| + |‚àáPCI|"""
    return round(abs(nabla_a) + abs(nabla_pci), 4)


def compute_m35_phys_8(ZLF: float, stagnation: float = 0.0) -> float:
    """m35_phys_8: Fixpunkt-N√§he = max(ZLF, stagnation)"""
    return round(max(ZLF, stagnation), 4)


def compute_m7_LL(rep_same: float, flow: float) -> float:
    """
    m7_LL: Lambert-Light (Turbidity Index)
    
    SPEC Formel:
        LL = clip(0.6 √ó rep_same + 0.4 √ó (1 - flow))
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:2331-2390
    """
    opacity = 0.6 * rep_same + 0.4 * (1.0 - flow)
    return round(clamp(opacity), 4)



# =============================================================================
# A_PHYS (V11) ‚Äî KANONISCHER KERN
# =============================================================================

def compute_m15_affekt_a_legacy(flow: float, coh: float, ll: float, zlf: float) -> float:
    """Legacy/Bridge: urspr√ºngliche V3-Aggregat-Formel (f√ºr Vergleich & Fallback).

    Ursprung (Spec-Teil m15):
        A_legacy = clip01(0.40*flow + 0.30*coh + 0.20*(1-LL) + 0.10*(1-ZLF))

    Diese Gr√∂√üe bleibt im Patch als *Kontrollmetrik* erhalten, wird aber
    NICHT mehr als prim√§rer A-Kern interpretiert, sobald A_Phys verf√ºgbar ist.
    """
    a_legacy = 0.40 * flow + 0.30 * coh + 0.20 * (1.0 - ll) + 0.10 * (1.0 - zlf)
    return round(clamp(a_legacy), 4)


def compute_A_phys_v11(text: str, physics_ctx: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Berechnet A_Phys nach V11, wenn ein Physics-Context vorhanden ist.

    Erwartete Keys in physics_ctx (Antigravety-kompatibel):
    - v_c: Kandidatenvektor (Embedding) f√ºr den aktuellen Prompt/Response
    - active_memories: List[dict] mit 'vector_semantic' (oder 'vector_hash') + 'resonanzwert'
    - danger_zone_cache: List[Tuple[id, vector]] (nur F-Trauma-Zentren)
    - vector_service: optional, muss cosine_similarity(a,b) anbieten
    - params: optional APhysParams
    - vec_key / weight_key: optional Feldnamen (Defaults: vector_semantic / resonanzwert)

    Fallback:
    - Wenn kein v_c vorhanden ist: Hash-Vektor aus text (deterministisch).
    """
    # Kein Kontext -> kein A_Phys
    if not physics_ctx or APhysV11 is None:
        return {
            "A_phys": float("nan"),
            "A_phys_raw": float("nan"),
            "resonance": float("nan"),
            "danger": float("nan"),
            "a29_trip": False,
            "a29_max_sim": float("nan"),
            "a29_id": None,
            "top_resonance": [],
            "error": "no_physics_ctx_or_module",
        }

    params = physics_ctx.get("params") if isinstance(physics_ctx, dict) else None
    if params is None and APhysParams is not None:
        params = APhysParams()

    engine = None
    if isinstance(physics_ctx, dict):
        engine = physics_ctx.get("engine")
        if engine is None:
            engine = APhysV11(params=params, vector_service=physics_ctx.get("vector_service"))

    try:
        v_c = physics_ctx.get("v_c")
        active = physics_ctx.get("active_memories") or []
        danger = physics_ctx.get("danger_zone_cache") or []
        vec_key = physics_ctx.get("vec_key", "vector_semantic")
        weight_key = physics_ctx.get("weight_key", "resonanzwert")

        return engine.compute_affekt(
            v_c=v_c,
            text=text,
            active_memories=active,
            danger_zone_cache=danger,
            vec_key=vec_key,
            weight_key=weight_key,
        )
    except Exception as e:  # pragma: no cover
        return {
            "A_phys": float("nan"),
            "A_phys_raw": float("nan"),
            "resonance": float("nan"),
            "danger": float("nan"),
            "a29_trip": False,
            "a29_max_sim": float("nan"),
            "a29_id": None,
            "top_resonance": [],
            "error": f"exception: {e}",
        }


# =============================================================================
# TEIL 5: TRAUMA (m101-m115) - EXAKT NACH SPEC
# =============================================================================

def compute_m101_t_panic(text: str) -> float:
    """
    m101_t_panic: Panik-Vektor
    
    SPEC Formel:
        t_panic = clip(Œ£(panic_lex_hit √ó weight) / (text_len + 1) √ó 10.0)
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:6347-6389
    """
    words = text.lower().split()
    if not words:
        return 0.0
    
    total_weight = 0.0
    text_lower = text.lower()
    for phrase, weight in PANIC_LEXIKON.items():
        if phrase in text_lower:
            total_weight += weight
    
    raw_score = total_weight / (len(words) + 1) * 10.0
    return round(clamp(raw_score), 4)


def compute_m102_t_disso(text: str) -> float:
    """
    m102_t_disso: Dissoziation
    
    SPEC Formel:
        t_disso = clip(Œ£(disso_lex_hit √ó weight) / (text_len + 1) √ó 8.0)
    
    Reference: EVOKI_V3_METRICS_SPECIFICATION.md:6435-6473
    """
    words = text.lower().split()
    if not words:
        return 0.0
    
    total_weight = 0.0
    text_lower = text.lower()
    for phrase, weight in DISSO_LEXIKON.items():
        if phrase in text_lower:
            total_weight += weight
    
    raw_score = total_weight / (len(words) + 1) * 8.0
    return round(clamp(raw_score), 4)


def compute_m103_t_integ(text: str) -> float:
    """
    m103_t_integ: Integration
    
    Positive Gegenkraft zu Trauma.
    """
    words = text.lower().split()
    if not words:
        return 0.0
    
    total_weight = 0.0
    text_lower = text.lower()
    for phrase, weight in INTEG_LEXIKON.items():
        if phrase in text_lower:
            total_weight += weight
    
    raw_score = total_weight / (len(words) + 1) * 8.0
    return round(clamp(raw_score), 4)


def compute_m110_black_hole(chaos: float, effective_A: float, LL: float) -> float:
    """
    m110_black_hole: Black Hole (Kollaps) ‚ö†Ô∏è KRITISCH
    
    SPEC Formel (V3.3 Lexikon-Veto):
        black_hole = (0.4 √ó chaos) + (0.3 √ó (1 - A)) + (0.3 √ó LL)
        
    Bei hohen Panic-Hits: min 0.85
    """
    val = 0.4 * chaos + 0.3 * (1.0 - effective_A) + 0.3 * LL
    return round(clamp(val), 4)


def compute_m112_trauma_load(t_panic: float, t_disso: float, t_integ: float) -> float:
    """
    m112_trauma_load: Trauma-Last
    
    Gewichtete Summe der Trauma-Vektoren.
    """
    val = 0.4 * t_panic + 0.4 * t_disso + 0.2 * (1 - t_integ)
    return round(clamp(val), 4)


# --- TRAUMA EXTENDED (m104-m109, m114-m115) ---

def compute_m104_t_shock(text: str) -> float:
    """m104_t_shock: Schock-Score = Panik-Peaks / Wortanzahl"""
    shock_words = ["pl√∂tzlich", "√ºberraschend", "schock", "unerwartet", "boom"]
    text_lower = text.lower()
    hits = sum(1 for w in shock_words if w in text_lower)
    words = len(text.split())
    return round(clamp(hits / max(1, words) * 5.0), 4)


def compute_m105_t_guilt(text: str) -> float:
    """m105_t_guilt: Schuld-Score"""
    guilt_words = ["schuld", "fehler", "h√§tte", "bereue", "sorry"]
    text_lower = text.lower()
    hits = sum(1 for w in guilt_words if w in text_lower)
    return round(clamp(hits / 3.0), 4)


def compute_m106_t_shame(text: str) -> float:
    """m106_t_shame: Scham-Score"""
    shame_words = ["peinlich", "sch√§me", "blamiert", "dumm"]
    text_lower = text.lower()
    hits = sum(1 for w in shame_words if w in text_lower)
    return round(clamp(hits / 3.0), 4)


def compute_m107_t_grief(text: str) -> float:
    """m107_t_grief: Trauer-Score"""
    grief_words = ["trauer", "verlust", "vermisse", "tot", "gestorben"]
    text_lower = text.lower()
    hits = sum(1 for w in grief_words if w in text_lower)
    return round(clamp(hits / 3.0), 4)


def compute_m108_t_anger(text: str) -> float:
    """m108_t_anger: Wut-Score"""
    anger_words = ["wut", "w√ºtend", "√§rger", "hass", "sauer"]
    text_lower = text.lower()
    hits = sum(1 for w in anger_words if w in text_lower)
    return round(clamp(hits / 3.0), 4)


def compute_m109_t_fear(text: str) -> float:
    """m109_t_fear: Angst-Score"""
    fear_words = ["angst", "furcht", "bef√ºrchte", "sorge", "panik"]
    text_lower = text.lower()
    hits = sum(1 for w in fear_words if w in text_lower)
    return round(clamp(hits / 3.0), 4)


def compute_m111_turbidity_total(t_panic: float, t_disso: float, t_shock: float, t_integ: float) -> float:
    """m111_turbidity_total: Gesamt-Tr√ºbung = sum(T_*) / 4"""
    return round((t_panic + t_disso + t_shock + (1 - t_integ)) / 4.0, 4)


def compute_m114_t_recovery(t_integ_current: float, t_integ_prev: float) -> float:
    """m114_t_recovery: Recovery = grad(T_integ)"""
    return round(t_integ_current - t_integ_prev, 4)


def compute_m115_t_threshold() -> float:
    """m115_t_threshold: Konfigurationswert f√ºr Trauma-Schwelle"""
    return 0.7  # Default threshold


# --- EVOLUTION (m71-m73) ---

def compute_m71_ev_arousal(text: str) -> float:
    """m71_ev_arousal: Erregung"""
    arousal_pos = ["aufgeregt", "begeistert", "energisch", "enthusiastisch"]
    arousal_neg = ["m√ºde", "ersch√∂pft", "gelangweilt", "lethargisch"]
    text_lower = text.lower()
    pos = sum(1 for w in arousal_pos if w in text_lower)
    neg = sum(1 for w in arousal_neg if w in text_lower)
    return round(clamp(0.5 + 0.2 * pos - 0.2 * neg), 4)


def compute_m72_ev_valence(text: str) -> float:
    """m72_ev_valence: Valenz"""
    valence_pos = ["gut", "toll", "super", "freude", "gl√ºcklich", "liebe"]
    valence_neg = ["schlecht", "schrecklich", "traurig", "hasse", "elend"]
    text_lower = text.lower()
    pos = sum(1 for w in valence_pos if w in text_lower)
    neg = sum(1 for w in valence_neg if w in text_lower)
    return round(clamp(0.5 + 0.15 * pos - 0.15 * neg), 4)


def compute_m73_ev_readiness(t_integ: float, A: float) -> float:
    """m73_ev_readiness: Bereitschaft = T_integ √ó A"""
    return round(t_integ * A, 4)


# --- SENTIMENT VAD & PLUTCHIK (m74-m95) ---

def compute_m74_valence(text: str) -> float:
    """m74_valence: Emotionale Wertigkeit (VAD)"""
    pos_words = ["gut", "toll", "super", "freude", "gl√ºcklich", "liebe", "sch√∂n", "wunderbar", "fantastisch"]
    neg_words = ["schlecht", "schrecklich", "traurig", "hasse", "elend", "furchtbar", "mies", "√ºbel"]
    text_lower = text.lower()
    pos = sum(1 for w in pos_words if w in text_lower)
    neg = sum(1 for w in neg_words if w in text_lower)
    return round(clamp(0.5 + (pos - neg) * 0.05), 4)


def compute_m75_arousal(text: str) -> float:
    """m75_arousal: Erregungsniveau (VAD)"""
    high_words = ["aufgeregt", "begeistert", "w√ºtend", "√§ngstlich", "energisch", "intensiv"]
    low_words = ["ruhig", "entspannt", "m√ºde", "gelassen", "friedlich", "langsam"]
    text_lower = text.lower()
    high = sum(1 for w in high_words if w in text_lower)
    low = sum(1 for w in low_words if w in text_lower)
    return round(clamp(0.5 + (high - low) * 0.05), 4)


def compute_m76_dominance(text: str) -> float:
    """m76_dominance: Kontroll-Gef√ºhl (VAD)"""
    high_words = ["kann", "werde", "bestimme", "kontrolliere", "stark", "sicher"]
    low_words = ["hilflos", "ohnm√§chtig", "schwach", "verloren", "√ºberfordert"]
    text_lower = text.lower()
    high = sum(1 for w in high_words if w in text_lower)
    low = sum(1 for w in low_words if w in text_lower)
    return round(clamp(0.5 + (high - low) * 0.05), 4)


def compute_m77_joy(valence: float, arousal: float) -> float:
    """m77_joy: Freude (Plutchik) = valence + arousal - 1"""
    return round(clamp(valence + arousal - 1.0), 4)


def compute_m78_sadness(valence: float, arousal: float) -> float:
    """m78_sadness: Traurigkeit (Plutchik) = (2 - valence - arousal) / 2"""
    return round(clamp((2 - valence - arousal) / 2), 4)


def compute_m79_anger(valence: float, arousal: float) -> float:
    """m79_anger: Wut (Plutchik) = (1 - valence + arousal) / 2"""
    return round(clamp((1 - valence + arousal) / 2), 4)


def compute_m80_fear(valence: float, arousal: float, dominance: float, t_panic: float) -> float:
    """m80_fear: Angst (Plutchik) mit T_panic Boost"""
    fear_base = clamp((3 - valence + arousal - dominance) / 3)
    return round(max(fear_base, t_panic * 0.8), 4)


def compute_m81_trust(valence: float, arousal: float, dominance: float, t_integ: float) -> float:
    """m81_trust: Vertrauen (Plutchik) mit T_integ Boost"""
    trust_base = clamp((valence + (1 - arousal) + dominance) / 3)
    return round(max(trust_base, t_integ * 0.6), 4)


def compute_m82_disgust(valence: float) -> float:
    """m82_disgust: Ekel (Plutchik) = (1 - valence) √ó 0.7"""
    return round((1 - valence) * 0.7, 4)


def compute_m83_anticipation(arousal: float) -> float:
    """m83_anticipation: Erwartung (Plutchik) = arousal √ó 0.8"""
    return round(arousal * 0.8, 4)


def compute_m84_surprise(valence: float, arousal: float) -> float:
    """m84_surprise: √úberraschung (Plutchik) = arousal √ó (1 - |valence - 0.5| √ó 2)"""
    return round(arousal * (1 - abs(valence - 0.5) * 2), 4)


def compute_m85_hope(valence: float, anticipation: float) -> float:
    """m85_hope: Hoffnung (Complex) = (valence + anticipation) / 2"""
    return round((valence + anticipation) / 2, 4)


def compute_m86_despair(valence: float, sadness: float) -> float:
    """m86_despair: Verzweiflung (Complex) = ((1 - valence) + sadness) / 2"""
    return round(((1 - valence) + sadness) / 2, 4)


def compute_m87_confusion(arousal: float, PCI: float) -> float:
    """m87_confusion: Verwirrung (Complex) = arousal √ó (1 - PCI)"""
    return round(arousal * (1 - PCI), 4)


def compute_m88_clarity(PCI: float, arousal: float) -> float:
    """m88_clarity: Klarheit (Complex) = PCI √ó (0.5 + arousal √ó 0.5)"""
    return round(PCI * (0.5 + arousal * 0.5), 4)


def compute_m89_acceptance(valence: float, arousal: float, t_integ: float) -> float:
    """m89_acceptance: Akzeptanz (Complex) = (valence + (1-arousal) + T_integ) / 3"""
    return round((valence + (1 - arousal) + t_integ) / 3, 4)


def compute_m90_resistance(arousal: float, acceptance: float) -> float:
    """m90_resistance: Widerstand (Complex) = arousal √ó (1 - acceptance)"""
    return round(arousal * (1 - acceptance), 4)


def compute_m91_emotional_coherence(PCI: float, t_disso: float) -> float:
    """m91_emotional_coherence: Emotionale Koh√§renz = PCI √ó (1 - T_disso)"""
    return round(PCI * (1 - t_disso), 4)


def compute_m92_emotional_stability(valence: float, arousal: float) -> float:
    """m92_emotional_stability: Emotionale Stabilit√§t = (1 - arousal) √ó (1 - |valence - 0.5| √ó 2)"""
    return round((1 - arousal) * (1 - abs(valence - 0.5) * 2), 4)


def compute_m93_emotional_range(v: float, a: float, d: float) -> float:
    """m93_emotional_range: Distanz vom emotionalen Zentrum (VAD)"""
    return round(math.sqrt((v - 0.5)**2 + (a - 0.5)**2 + (d - 0.5)**2), 4)


def compute_m94_comfort(valence: float, arousal: float) -> float:
    """m94_comfort: Komfort = (1 - arousal) √ó (1 - |valence - 0.6|)"""
    return round((1 - arousal) * (1 - abs(valence - 0.6)), 4)


def compute_m95_tension(valence: float, arousal: float) -> float:
    """m95_tension: Spannung = arousal √ó |valence - 0.5| √ó 2"""
    return round(arousal * abs(valence - 0.5) * 2, 4)


# --- GRAIN (m96-m99) ---

def compute_m96_grain_word(text: str) -> str:
    """m96_grain_word: Das 'Korn' des Textes (h√§ufigstes substantielles Wort)"""
    stopwords = {"der", "die", "das", "und", "ist", "in", "zu", "ein", "eine", "es", "ich", "nicht"}
    words = [w.lower() for w in text.split() if w.lower() not in stopwords and len(w) > 3]
    if not words:
        return ""
    word_counts = Counter(words)
    return word_counts.most_common(1)[0][0] if word_counts else ""


def compute_m97_grain_cat(grain_word: str) -> str:
    """m97_grain_cat: Kategorie des Grain-Worts"""
    categories = {
        "emotion": ["freude", "trauer", "angst", "wut", "liebe"],
        "action": ["machen", "gehen", "kommen", "sehen", "helfen"],
        "object": ["haus", "auto", "buch", "computer", "ding"],
    }
    for cat, words in categories.items():
        if grain_word.lower() in words:
            return cat
    return "other"


def compute_m98_grain_score(text: str, grain_word: str) -> float:
    """m98_grain_score: Dichte des Grain-Worts"""
    if not grain_word:
        return 0.0
    count = text.lower().count(grain_word.lower())
    word_count = len(text.split())
    return round(count / max(1, word_count), 4)


def compute_m99_grain_impact(grain_score: float, A: float) -> float:
    """m99_grain_impact: Grain √ó Affekt"""
    return round(grain_score * A, 4)


# =============================================================================
# TEIL 5b: METAKOGNITION (m116-m150) - EXAKT NACH SPEC
# =============================================================================

def compute_m116_lix(text: str) -> float:
    """
    m116_lix: Lesbarkeits-Index (LIX)
    
    SPEC: LIX = (W√∂rter/S√§tze) + (LangeW√∂rter√ó100/W√∂rter)
    """
    words = text.split()
    sentences = text.count('.') + text.count('!') + text.count('?')
    if not words or sentences == 0:
        return 50.0  # Default medium
    long_words = sum(1 for w in words if len(w) > 6)
    return round((len(words) / sentences) + (long_words * 100 / len(words)), 2)


def compute_m117_question_density(text: str) -> float:
    """m117_question_density: Fragen-Dichte"""
    questions = text.count('?')
    sentences = text.count('.') + text.count('!') + text.count('?')
    return round(questions / max(1, sentences), 4)


def compute_m118_exclamation_density(text: str) -> float:
    """m118_exclamation_density: Ausruf-Dichte"""
    exclamations = text.count('!')
    sentences = text.count('.') + text.count('!') + text.count('?')
    return round(exclamations / max(1, sentences), 4)


def compute_m119_complexity_variance(pci_history: List[float]) -> float:
    """m119_complexity_variance: Varianz der PCI-Historie"""
    if len(pci_history) < 2:
        return 0.0
    mean = sum(pci_history) / len(pci_history)
    variance = sum((x - mean) ** 2 for x in pci_history) / len(pci_history)
    return round(variance, 4)


def compute_m120_topic_drift(similarity_to_first: float) -> float:
    """m120_topic_drift: Themen-Abdrift = 1 - similarity_to_first"""
    return round(1.0 - similarity_to_first, 4)


def compute_m121_self_reference_count(text: str) -> int:
    """m121_self_reference_count: Anzahl Selbst-Referenzen"""
    self_words = ["ich", "mich", "mir", "mein", "meine", "meiner"]
    text_lower = text.lower()
    return sum(text_lower.count(w) for w in self_words)


def compute_m122_dyn_1(nabla_a: float, nabla_pci: float) -> float:
    """m122_dyn_1: Dynamik-Faktor 1 = |‚àáA| + |‚àáPCI|"""
    return round(abs(nabla_a) + abs(nabla_pci), 4)


def compute_m123_dyn_2(A: float, A_prev: float) -> float:
    """m123_dyn_2: Affekt-Momentum = A - A_prev"""
    return round(A - A_prev, 4)


def compute_m124_dyn_3(flow: float, flow_prev: float) -> float:
    """m124_dyn_3: Flow-Momentum = flow - flow_prev"""
    return round(flow - flow_prev, 4)


def compute_m125_dyn_4(coh: float, coh_prev: float) -> float:
    """m125_dyn_4: Koh√§renz-Momentum = coh - coh_prev"""
    return round(coh - coh_prev, 4)


def compute_m126_dyn_5(t_panic: float, t_panic_prev: float) -> float:
    """m126_dyn_5: Panik-Momentum = t_panic - t_panic_prev"""
    return round(t_panic - t_panic_prev, 4)


def compute_m127_avg_response_len(lengths: List[int]) -> float:
    """m127_avg_response_len: Durchschnittliche Antwortl√§nge"""
    return round(sum(lengths) / max(1, len(lengths)), 2) if lengths else 0.0


def compute_m128_token_ratio(user_tokens: int, ai_tokens: int) -> float:
    """m128_token_ratio: Token-Verh√§ltnis User/AI"""
    return round(user_tokens / max(1, ai_tokens), 4)


def compute_m129_engagement_score(questions: int, turns: int) -> float:
    """m129_engagement_score: Engagement = questions per turn"""
    return round(questions / max(1, turns), 4)


def compute_m130_session_depth(turn_count: int) -> float:
    """m130_session_depth: Sitzungstiefe (normiert auf 50 Turns)"""
    return round(min(1.0, turn_count / 50.0), 4)


# --- Meta-Cognition Schema B (m131-m150) ---

def compute_m131_meta_awareness(A: float, PCI: float) -> float:
    """m131_meta_awareness: Selbst-Bewusstsein = (A + PCI) / 2"""
    return round((A + PCI) / 2.0, 4)


def compute_m132_meta_regulation(z_prox: float, commit: str) -> float:
    """m132_meta_regulation: Selbst-Regulation"""
    regulation = 1.0 - z_prox
    if commit == "warn":
        regulation *= 0.8
    elif commit == "alert":
        regulation *= 0.5
    return round(regulation, 4)


def compute_m133_meta_flexibility(topic_changes: int, turns: int) -> float:
    """m133_meta_flexibility: Kognitive Flexibilit√§t"""
    return round(topic_changes / max(1, turns), 4)


def compute_m134_meta_monitoring(error_count: int, checks: int) -> float:
    """m134_meta_monitoring: Selbst-√úberwachung = 1 - errors/checks"""
    return round(1.0 - error_count / max(1, checks), 4)


def compute_m135_meta_planning(goal_progress: float) -> float:
    """m135_meta_planning: Planungsfortschritt"""
    return round(clamp(goal_progress), 4)


def compute_m136_meta_evaluation(task_success: float) -> float:
    """m136_meta_evaluation: Aufgaben-Evaluation"""
    return round(clamp(task_success), 4)


def compute_m137_meta_strategy(strategy_switches: int, turns: int) -> float:
    """m137_meta_strategy: Strategie-Wechsel"""
    return round(strategy_switches / max(1, turns), 4)


def compute_m138_attention_focus(main_topic_coverage: float) -> float:
    """m138_attention_focus: Aufmerksamkeits-Fokus"""
    return round(clamp(main_topic_coverage), 4)


def compute_m139_working_memory(context_items: int, max_items: int = 7) -> float:
    """m139_working_memory: Arbeitsged√§chtnis-Nutzung"""
    return round(min(1.0, context_items / max_items), 4)


def compute_m140_long_term_access(retrieval_success: float) -> float:
    """m140_long_term_access: Langzeit-Ged√§chtnis-Zugriff"""
    return round(clamp(retrieval_success), 4)


def compute_m141_inference_quality(logical_consistency: float) -> float:
    """m141_inference_quality: Schlussfolgerungs-Qualit√§t"""
    return round(clamp(logical_consistency), 4)


def compute_m142_rag_alignment(rag_score: float) -> float:
    """m142_rag_alignment: RAG-√úbereinstimmung"""
    return round(clamp(rag_score), 4)


def compute_m143_mem_pressure() -> float:
    """m143_mem_pressure: Speicher-Druck (System)"""
    try:
        import psutil
        return round(psutil.virtual_memory().percent / 100.0, 4)
    except ImportError:
        return 0.5


def compute_m144_sys_stability(error_rate: float, latency_norm: float) -> float:
    """m144_sys_stability: System-Stabilit√§t"""
    stability = 1.0 - (0.5 * error_rate + 0.5 * latency_norm)
    return round(clamp(stability), 4)


def compute_m145_learning_rate_meta(performance_delta: float) -> float:
    """m145_learning_rate: Meta-Lernrate"""
    return round(clamp(performance_delta, -0.1, 0.1), 4)


def compute_m146_curiosity_index(questions_asked: int, turns: int) -> float:
    """m146_curiosity: Neugier-Index"""
    return round(questions_asked / max(1, turns), 4)


def compute_m147_confidence(variance: float) -> float:
    """m147_confidence: Konfidenz = 1 - variance"""
    return round(1.0 - clamp(variance), 4)


def compute_m148_coherence_meta(internal_consistency: float) -> float:
    """m148_coherence_meta: Meta-Koh√§renz"""
    return round(clamp(internal_consistency), 4)


def compute_m149_adaptation_rate(adjustments: int, opportunities: int) -> float:
    """m149_adaptation: Anpassungs-Rate"""
    return round(adjustments / max(1, opportunities), 4)


def compute_m150_integration_score(modules_active: int, total_modules: int) -> float:
    """m150_integration: Integrations-Score"""
    return round(modules_active / max(1, total_modules), 4)


# =============================================================================
# TEIL 6: SYSTEM (m151-m161) - EXAKT NACH SPEC
# =============================================================================

def compute_m161_commit(z_prox: float, trauma_load: float) -> str:
    """
    m161_commit: Commit Action
    
    SPEC Thresholds:
        z_prox > 0.65 oder trauma_load > 0.8: "alert"
        z_prox > 0.50 oder trauma_load > 0.7: "warn"
        sonst: "commit"
    """
    if z_prox > 0.65 or trauma_load > 0.8:
        return "alert"
    elif z_prox > 0.50 or trauma_load > 0.7:
        return "warn"
    return "commit"


def compute_m152_a51_compliance(rules_checked: int, rules_passed: int) -> float:
    """m152_a51_compliance: A51 Regelkonformit√§t"""
    return round(rules_passed / max(1, rules_checked), 4)


def compute_m154_sys_latency(response_time_ms: float, target_ms: float = 500.0) -> float:
    """m154_sys_latency: System-Latenz (normiert)"""
    return round(min(1.0, response_time_ms / target_ms), 4)


def compute_m155_error_rate(errors: int, total_requests: int) -> float:
    """m155_error_rate: Fehlerrate"""
    return round(errors / max(1, total_requests), 4)


def compute_m156_cache_hit_rate(hits: int, total: int) -> float:
    """m156_cache_hit: Cache-Trefferquote"""
    return round(hits / max(1, total), 4)


def compute_m157_token_throughput(tokens: int, seconds: float) -> float:
    """m157_token_throughput: Token-Durchsatz pro Sekunde"""
    return round(tokens / max(0.01, seconds), 2)


def compute_m158_context_utilization(used_tokens: int, max_tokens: int) -> float:
    """m158_context_util: Kontext-Nutzung"""
    return round(used_tokens / max(1, max_tokens), 4)


def compute_m159_guardian_interventions(interventions: int, turns: int) -> float:
    """m159_guardian_int: Guardian-Eingriffe pro Turn"""
    return round(interventions / max(1, turns), 4)


def compute_m160_uptime(uptime_seconds: float, total_seconds: float) -> float:
    """m160_uptime: System-Verf√ºgbarkeit"""
    return round(uptime_seconds / max(1.0, total_seconds), 4)


# =============================================================================
# TEIL 7: CONTEXT & SAFETY (m162-m168) - EXAKT NACH SPEC V3.2.1
# =============================================================================

def compute_m162_ctx_time(session_start_minutes: float, current_minutes: float) -> float:
    """
    m162_ctx_time: Temporale Kontext-Einbettung
    
    SPEC: Normalized session duration (0-60 min ‚Üí 0-1)
    """
    duration = current_minutes - session_start_minutes
    return round(min(1.0, duration / 60.0), 4)


def compute_m163_ctx_loc(location_data: dict = None) -> float:
    """
    m163_ctx_loc: Lokale/R√§umliche Einbettung
    
    PC-Phase: 0.5 (neutral, unknown location)
    APK-Phase: Will use GPS/sensor data
    """
    if location_data is None:
        return 0.5  # Neutral default for PC
    return round(location_data.get("safety_score", 0.5), 4)


def compute_m164_user_state(recent_affects: List[float]) -> float:
    """
    m164_user_state: Meta-Score √ºber User-Zustand
    
    SPEC: Gewichteter Durchschnitt der letzten 5 Affect-Scores
    """
    if not recent_affects:
        return 0.5
    weights = [0.1, 0.15, 0.2, 0.25, 0.3]  # Neuere = wichtiger
    weights = weights[-len(recent_affects):]
    weighted_sum = sum(a * w for a, w in zip(recent_affects, weights))
    return round(weighted_sum / sum(weights), 4)


def compute_m165_platform() -> str:
    """
    m165_platform: Aktuelle Plattform
    
    Returns: "pc", "apk", or "rover"
    """
    import platform as plat
    system = plat.system().lower()
    if system == "linux" and "android" in plat.release().lower():
        return "apk"
    elif system in ["windows", "linux", "darwin"]:
        return "pc"
    else:
        return "rover"


def compute_m166_modality(input_data: dict) -> str:
    """
    m166_modality: Input-Modalit√§t
    
    Returns: "text", "voice", "image", or "multimodal"
    """
    modalities = []
    if input_data.get("text"):
        modalities.append("text")
    if input_data.get("audio"):
        modalities.append("voice")
    if input_data.get("image"):
        modalities.append("image")
    
    if len(modalities) > 1:
        return "multimodal"
    elif modalities:
        return modalities[0]
    else:
        return "text"  # Default


def compute_m167_noise(text: str) -> float:
    """
    m167_noise: Input-Rauschen/St√∂rung
    
    Misst Tippfehler, Fragmente, unklare Eingaben
    """
    if not text:
        return 0.0
    
    # Heuristics for noise detection
    typo_pattern = r'\b(\w)\1{2,}\b'  # Repeated letters (aaaa)
    fragment_pattern = r'^[a-z]{1,3}$'  # Very short words only
    
    typos = len(re.findall(typo_pattern, text, re.IGNORECASE))
    words = text.split()
    fragments = sum(1 for w in words if re.match(fragment_pattern, w))
    
    noise_score = (typos * 0.3 + fragments / max(1, len(words)) * 0.7)
    return round(min(1.0, noise_score), 4)


def compute_m168_cum_stress(z_prox_history: List[float]) -> float:
    """
    m168_cum_stress: Kumulativer Stress ‚ö†Ô∏è KRITISCH
    
    SPEC: Integral von z_prox √ºber Zeit (vereinfacht: Durchschnitt)
    Erkennt schleichende Destabilisierung ("Frosch im kochenden Wasser")
    
    Trigger: cum_stress > 15.0 ‚Üí Guardian-Warnung
    """
    if not z_prox_history:
        return 0.0
    recent = z_prox_history[-10:]
    return round(sum(recent) / len(recent), 4)


# =============================================================================
# TEIL 8: KRITISCHE METRIKEN (aus Gap-Analyse)
# =============================================================================

# --- SOUL INTEGRITY (m38, m39) ---

B_VECTOR_SEED = "EVOKI_SOUL_V3"  # Genesis Anchor

def compute_m38_soul_integrity(b_vector: List[float]) -> float:
    """
    m38_soul_integrity: Seelen-Integrit√§t üî¥ KRITISCH
    
    SPEC Formel:
        soul_integrity = norm(B_vector) / 7.0
    
    B_vector hat 7 Dimensionen (aus B_past, √Öngstr√∂m etc.)
    """
    if not b_vector or len(b_vector) != 7:
        return 1.0  # Default: Vollst√§ndige Integrit√§t
    norm = math.sqrt(sum(x**2 for x in b_vector))
    return round(clamp(norm / 7.0), 4)


def compute_m39_soul_check(b_vector: List[float], seed: str = B_VECTOR_SEED) -> bool:
    """
    m39_soul_check: Seelen-Validierung üî¥ KRITISCH
    
    Pr√ºft ob B_vector nicht manipuliert wurde.
    """
    import hashlib
    if not b_vector:
        return True
    vector_str = ",".join(f"{x:.4f}" for x in b_vector)
    combined = f"{seed}:{vector_str}"
    # Vereinfachte Pr√ºfung: Hash ist konsistent
    return len(hashlib.sha256(combined.encode()).hexdigest()) == 64


# --- TOKEN ECONOMY (m57, m58) ---

def compute_m57_tokens_soc(current: float, delta: float) -> float:
    """
    m57_tokens_soc: Social Tokens üî¥ KRITISCH
    
    SPEC: Token-Economy mit CLAMPING!
        tokens = clamp(0, 100, current + delta)
    """
    return round(clamp(current + delta, 0.0, 100.0), 2)


def compute_m58_tokens_log(current: float, delta: float) -> float:
    """
    m58_tokens_log: Logic Tokens üî¥ KRITISCH
    
    Analog zu m57 f√ºr logische Tokens.
    """
    return round(clamp(current + delta, 0.0, 100.0), 2)


def compute_m59_p_antrieb(tokens_soc: float, tokens_log: float, stagnation: float = 0.0) -> float:
    """
    m59_p_antrieb: Antriebsdruck
    
    SPEC Formel:
        p_antrieb = (tokens_soc + tokens_log) / 200 + stagnation
    """
    return round(clamp((tokens_soc + tokens_log) / 200.0 + stagnation), 4)


# --- HYPERMETRICS (m46-m55) ---

def compute_m41_h_symbol(h_conv: float) -> float:
    """m41_h_symbol: Harmonie-Symbol = 1.0 wenn h_conv > 0.7, sonst 0.0"""
    return 1.0 if h_conv > 0.7 else 0.0


def compute_m42_nabla_dyad(h_conv: float) -> float:
    """m42_nabla_dyad: Dyade-Gradient = h_conv - 0.5"""
    return round(h_conv - 0.5, 4)


def compute_m46_rapport(trust_history: List[float]) -> float:
    """m46_rapport: Beziehungs-Rapport = avg(last_5_trust)"""
    if not trust_history:
        return 0.5
    recent = trust_history[-5:]
    return round(sum(recent) / len(recent), 4)


def compute_m47_focus_stability(topic_variance: float) -> float:
    """m47_focus_stability: Fokus-Stabilit√§t = 1 - std(topics)"""
    return round(1.0 - topic_variance, 4)


def compute_m48_hyp_1(h_conv: float, pacing: float) -> float:
    """m48_hyp_1: Sync-Index = h_conv √ó pacing"""
    return round(h_conv * pacing, 4)


def compute_m49_hyp_2(soul_integrity: float) -> float:
    """m49_hyp_2: Quadratische Integrit√§t = soul_integrity¬≤"""
    return round(soul_integrity ** 2, 4)


def compute_m50_hyp_3(rule_conflict: float) -> float:
    """m50_hyp_3: Inverse Konflikt = 1 - rule_conflict"""
    return round(1.0 - rule_conflict, 4)


def compute_m51_hyp_4(h_conv: float, A: float) -> float:
    """m51_hyp_4: Harmonie-gewichtetes Bewusstsein = h_conv √ó A"""
    return round(h_conv * A, 4)


def compute_m52_hyp_5(g_phase_norm: float) -> float:
    """m52_hyp_5: Gravitationsphase (normiert)"""
    return round(g_phase_norm, 4)


def compute_m53_hyp_6(gap_seconds: float) -> float:
    """m53_hyp_6: Zeit-Faktor = gap_s / 3600 (Stunden)"""
    return round(gap_seconds / 3600.0, 4)


def compute_m54_hyp_7(trust_score: float, rapport: float) -> float:
    """m54_hyp_7: Vertrauens-Rapport-Produkt = trust √ó rapport"""
    return round(trust_score * rapport, 4)


def compute_m55_hyp_8(soul_integrity: float, PCI: float) -> float:
    """m55_hyp_8: Seelen-Komplexit√§ts-Produkt = soul √ó PCI"""
    return round(soul_integrity * PCI, 4)


# --- FEP / ANDROMATIK (m56, m60-m70) ---

def compute_m56_surprise(A_current: float, A_predicted: float) -> float:
    """m56_surprise: √úberraschungsfaktor = |A_current - A_predicted|"""
    return round(abs(A_current - A_predicted), 4)


def compute_m60_delta_tokens(tok_new: float, tok_old: float) -> float:
    """m60_delta_tokens: Token-Delta = new - old"""
    return round(tok_new - tok_old, 2)


def compute_m61_U(PCI: float, s_entropy: float) -> float:
    """
    m61_U: Free Energy (Variational) üî¥ FEP CORE
    
    SPEC: U = PCI - Œª √ó s_entropy
    """
    lambda_coeff = 0.3
    return round(PCI - lambda_coeff * s_entropy, 4)


def compute_m62_R(A: float, m61_U: float) -> float:
    """
    m62_R: Resonance (FEP)
    
    SPEC: R = A √ó U
    """
    return round(A * m61_U, 4)


def compute_m63_phi(PCI: float, R: float) -> float:
    """
    m63_phi: Phi Score (FEP)
    
    SPEC: phi = PCI √ó R
    """
    return round(PCI * R, 4)


def compute_m64_lambda_fep(s_entropy: float, action_space: int = 10) -> float:
    """m64_lambda: Lambda FEP = entropy / action_space"""
    return round(s_entropy / action_space, 4)


def compute_m65_alpha(learning_rate: float = 0.01) -> float:
    """m65_alpha: Alpha FEP (learning rate)"""
    return learning_rate


def compute_m66_gamma(discount_factor: float = 0.95) -> float:
    """m66_gamma: Gamma FEP (discount factor)"""
    return discount_factor


def compute_m67_precision(variance: float) -> float:
    """m67_precision: Precision FEP = 1 / variance"""
    return round(1.0 / max(0.01, variance), 4)


def compute_m68_prediction_err(expected: float, actual: float) -> float:
    """m68_prediction_err: Prediction Error = |expected - actual|"""
    return round(abs(expected - actual), 4)


def compute_m69_model_evidence(log_likelihood: float = 0.0) -> float:
    """m69_model_evidence: Model Evidence (log likelihood proxy)"""
    return round(log_likelihood, 4)


def compute_m70_active_inf(action_space: int, precision: float) -> float:
    """m70_active_inf: Active Inference = 1 / (1 + action_space / precision)"""
    return round(1.0 / (1.0 + action_space / max(0.01, precision)), 4)


# --- RULE CONFLICT (m36, m37) ---

def compute_m36_rule_conflict(rules: List[Dict]) -> float:
    """
    m36_rule_conflict: Protokoll-Konflikt üü† HIGH
    
    Erkennt Konflikte zwischen aktiven Regeln.
    """
    if not rules or len(rules) < 2:
        return 0.0
    # Simplified: Check for contradictory actions
    actions = [r.get("action", "") for r in rules]
    conflicts = 0
    for i, a1 in enumerate(actions):
        for a2 in actions[i+1:]:
            if a1 and a2 and a1 != a2:
                # Conflict if one says "allow" and other "deny"
                if ("allow" in a1 and "deny" in a2) or ("deny" in a1 and "allow" in a2):
                    conflicts += 1
    return round(clamp(conflicts / max(1, len(rules)), 0.0, 1.0), 4)


def compute_m37_rule_stable(rule_conflict: float) -> float:
    """
    m37_rule_stable: Regelstabilit√§t
    
    Inverse von m36.
    """
    return round(1.0 - rule_conflict, 4)


# --- TRUST & HARMONY (m40, m43-m45) ---

def compute_m40_h_conv(a_user: float, a_ai: float) -> float:
    """
    m40_h_conv: Dyade-Harmonie
    
    Korrelation zwischen User und AI Affekt.
    """
    # Simplified: Similarity measure
    diff = abs(a_user - a_ai)
    return round(1.0 - diff, 4)


def compute_m43_pacing(wc_user: int, wc_ai: int) -> float:
    """
    m43_pacing: Tempo-Synchronisation
    
    SPEC Formel:
        pacing = 1 - |wc_user - wc_ai| / max(wc_user, wc_ai, 1)
    """
    max_wc = max(wc_user, wc_ai, 1)
    diff = abs(wc_user - wc_ai)
    return round(1.0 - (diff / max_wc), 4)


def compute_m44_mirroring(user_tokens: set, ai_tokens: set) -> float:
    """
    m44_mirroring: Spiegelungs-Intensit√§t
    
    Jaccard-√Ñhnlichkeit zwischen User und AI Tokens.
    """
    if not user_tokens or not ai_tokens:
        return 0.0
    intersection = len(user_tokens & ai_tokens)
    union = len(user_tokens | ai_tokens)
    return round(intersection / union if union > 0 else 0.0, 4)


def compute_m45_trust_score(h_conv: float, pacing: float, mirroring: float) -> float:
    """
    m45_trust_score: Vertrauens-Score üü† HIGH
    
    SPEC Formel:
        trust = 0.4 √ó h_conv + 0.3 √ó pacing + 0.3 √ó mirroring
    """
    return round(0.4 * h_conv + 0.3 * pacing + 0.3 * mirroring, 4)


# --- OMEGA & SYSTEM (m151-m160) ---

def compute_m151_omega(
    A: float, PCI: float, z_prox: float, trauma_load: float
) -> float:
    """
    m151_omega: Die OMEGA-Konstante
    
    Top-Level System-Zustand.
    Range: [-1, 1]
    """
    positive = 0.4 * A + 0.3 * PCI
    negative = 0.3 * z_prox + 0.2 * trauma_load
    omega = positive - negative
    return round(clamp(omega, -1.0, 1.0), 4)


def compute_m153_health(
    latency: float, error_rate: float, mem_pressure: float
) -> float:
    """
    m153_sys_health: System-Gesundheit
    
    Kombinierte System-Gesundheit.
    """
    # Lower values for latency, error, memory = better health
    health = 1.0 - (0.3 * min(1.0, latency / 5.0) + 0.4 * error_rate + 0.3 * mem_pressure)
    return round(clamp(health), 4)


# --- CONTEXT (m162-m167) ---

def compute_m162_context_length(history: List) -> int:
    """m162: Kontext-L√§nge = Anzahl Eintr√§ge in History"""
    return len(history) if history else 0


def compute_m163_context_coherence(topic_similarities: List[float]) -> float:
    """m163: Kontext-Koh√§renz = Durchschnitt der Topic-√Ñhnlichkeiten"""
    if not topic_similarities:
        return 1.0
    return round(sum(topic_similarities) / len(topic_similarities), 4)


def compute_m164_context_drift(topic_sim_first_last: float) -> float:
    """
    m164_context_drift: Kontext-Drift üü† HIGH
    
    Wie weit ist das aktuelle Thema vom Ursprung entfernt?
    """
    return round(1.0 - topic_sim_first_last, 4)


def compute_m167_context_freshness(turns_since_anchor: int) -> float:
    """m167: Kontext-Frische = 1 / (1 + age)"""
    return round(1.0 / (1.0 + turns_since_anchor), 4)


# --- COGNITIVE METRICS HELPERS (m116-m126) ---

def compute_m116_lix(text: str) -> float:
    """
    m116_lix: LIX Lesbarkeitsindex (L√§sbarhetsindex)
    
    Formel: LIX = (words / sentences) + (long_words * 100 / words)
    long_words = W√∂rter mit mehr als 6 Buchstaben
    """
    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]
    words = text.split()
    
    if not sentences or not words:
        return 0.0
    
    long_words = sum(1 for w in words if len(w) > 6)
    
    lix = (len(words) / len(sentences)) + (long_words * 100.0 / len(words))
    return round(lix, 4)


def compute_m117_vocabulary_richness(tokens: List[str]) -> float:
    """
    m117_vocabulary_richness: Type-Token-Ratio (TTR)
    
    Formel: unique_words / total_words
    Range: [0, 1], higher = more diverse vocabulary
    """
    if not tokens:
        return 0.0
    
    unique = len(set(tokens))
    total = len(tokens)
    
    return round(unique / total, 4)


def compute_m118_coherence_local(sentences: List[str]) -> float:
    """
    m118_coherence_local: Lokale Koh√§renz zwischen aufeinanderfolgenden S√§tzen
    
    Approximation: Durchschnittliche Wort√ºberlappung zwischen benachbarten S√§tzen
    """
    if len(sentences) < 2:
        return 1.0
    
    overlaps = []
    for i in range(len(sentences) - 1):
        words1 = set(sentences[i].lower().split())
        words2 = set(sentences[i+1].lower().split())
        
        if not words1 or not words2:
            continue
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        if union > 0:
            overlaps.append(intersection / union)
    
    if not overlaps:
        return 0.0
    
    return round(sum(overlaps) / len(overlaps), 4)


def compute_m120_repetition_count(tokens: List[str]) -> int:
    """
    m120_repetition_count: Anzahl wiederholter W√∂rter (Case-insensitive)
    
    Z√§hlt wie oft ein Wort mehrfach vorkommt
    """
    if not tokens:
        return 0
    
    from collections import Counter
    counts = Counter(t.lower() for t in tokens)
    
    # Z√§hle W√∂rter die mehr als 1x vorkommen
    repetitions = sum(count - 1 for count in counts.values() if count > 1)
    
    return repetitions


def compute_m121_fragment_ratio(sentences: List[str]) -> float:
    """
    m121_fragment_ratio: Anteil unvollst√§ndiger S√§tze
    
    Heuristik: S√§tze k√ºrzer als 3 W√∂rter gelten als Fragmente
    """
    if not sentences:
        return 0.0
    
    fragments = sum(1 for s in sentences if len(s.split()) < 3)
    
    return round(fragments / len(sentences), 4)


def compute_m122_question_density(text: str) -> float:
    """
    m122_question_density: Anteil Frages√§tze
    
    Z√§hlt Fragezeichen pro Satz
    """
    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]
    
    if not sentences:
        return 0.0
    
    questions = text.count('?')
    
    return round(questions / len(sentences), 4)


def compute_m123_imperative_count(text: str) -> int:
    """
    m123_imperative_count: Anzahl Imperativ-S√§tze
    
    Heuristik: S√§tze mit Ausrufezeichen + kurze S√§tze am Satzanfang
    TODO: Bessere NLP-basierte Erkennung
    """
    # Einfache Heuristik: Ausrufezeichen z√§hlen
    return text.count('!')


def compute_m125_capitalization_stress(text: str) -> float:
    """
    m125_capitalization_stress: Gro√üschreibungs-Stress
    
    Anteil komplett gro√ügeschriebener W√∂rter (SCHREIEN)
    """
    words = text.split()
    
    if not words:
        return 0.0
    
    # W√∂rter die komplett gro√ügeschrieben sind und l√§nger als 1 Zeichen
    caps = sum(1 for w in words if w.isupper() and len(w) > 1)
    
    return round(caps / len(words), 4)


def compute_m126_punctuation_stress(text: str) -> float:
    """
    m126_punctuation_stress: Interpunktions-Stress
    
    Anteil Satzzeichen pro Wort (viele !!! oder ... = h√∂her Stress)
    """
    words = text.split()
    
    if not words:
        return 0.0
    
    # Z√§hle Satzzeichen au√üer Buchstaben/Zahlen/Leerzeichen
    punctuation = sum(1 for c in text if not c.isalnum() and not c.isspace())
    
    return round(punctuation / len(words), 4)


# =============================================================================
# TEIL 8: MASTER CALCULATOR - SPECIFICATION COMPLIANT
# =============================================================================

def calculate_spec_compliant(
    text: str,
    prev_text: str = "",
    prev_a: float = 0.5,
    nabla_a_prev: float = 0.0,
    z_prox_history: Optional[List[float]] = None,
    physics_ctx: Optional[Dict[str, Any]] = None,
    extended_physics: bool = True,
) -> FullSpectrum168:
    """
    MASTER-FUNKTION: Berechnet 168 Metriken EXAKT nach Spezifikation.
    
    Diese Funktion verwendet die korrekten Formeln aus
    EVOKI_V3_METRICS_SPECIFICATION.md
    """
    fs = FullSpectrum168()
    fs.timestamp = datetime.now().isoformat()
    
    tokens = tokenize(text)
    fs.word_count = len(tokens)
    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]
    fs.sentence_count = len(sentences)
    
    # === CORE (m1-m20) - SPEC COMPLIANT - V11.1 DEPENDENCY ORDER! ===
    
    # TIER 1: Base metrics (text-based)
    fs.m4_flow = compute_m4_flow(text)
    fs.m5_coh = compute_m5_coh(text)
    fs.m8_x_exist = compute_m8_x_exist(text, X_EXIST_LEXIKON)
    fs.m9_b_past = compute_m9_b_past(text, B_PAST_LEXIKON)
    
    # TIER 2: Derived metrics (need flow/coh)
    fs.m6_ZLF = compute_m6_ZLF(fs.m4_flow, fs.m5_coh)
    
    # TIER 3: LL (needs rep_same - TODO: calculate properly)
    # For now use proxy: (1 - coh) as rough estimate for rep_same
    rep_same_proxy = max(0.0, 1.0 - fs.m5_coh)
    fs.m7_LL = compute_m7_LL(rep_same_proxy, fs.m4_flow)
    
    # TIER 4: AFFEKT & PCI (V11.1 FORMULAS - Need coh, flow, LL, ZLF!)
    fs.m1_A = compute_m1_A(
        coh=fs.m5_coh,
        flow=fs.m4_flow,
        LL=fs.m7_LL,
        ZLF=fs.m6_ZLF,
        ctx_break=0.0  # TODO: Implement context break detection
    )
    
    fs.m2_PCI = compute_m2_PCI(
        flow=fs.m4_flow,
        coh=fs.m5_coh,
        LL=fs.m7_LL
    )
    
    # Continue with other metrics
    fs.m3_gen_index = compute_m3_gen_index(0, 0)  # TODO: Add history tracking
    fs.m10_angstrom = compute_m10_angstrom([fs.m8_x_exist, fs.m9_b_past, 0.5, 0.5, 0.5])
    fs.m11_gap_s = 0.0  # TODO: Add timestamp tracking
    fs.m12_gap_norm = compute_m12_gap_norm(fs.m11_gap_s)
    fs.m13_rep_same = compute_m13_rep_same(text, prev_text)
    fs.m14_rep_history = compute_m14_rep_history(text, [])  # TODO: Add history
    fs.m16_external_stag = compute_m16_external_stag(0)  # TODO: Add stagnation tracking
    fs.m18_s_entropy = compute_m18_s_entropy(tokens)
    
    # Trauma first (needed for LL)
    fs.m101_t_panic = compute_m101_t_panic(text)
    fs.m102_t_disso = compute_m102_t_disso(text)
    fs.m103_t_integ = compute_m103_t_integ(text)
    fs.m104_t_shock = compute_m104_t_shock(text)
    fs.m105_t_guilt = compute_m105_t_guilt(text)
    fs.m106_t_shame = compute_m106_t_shame(text)
    fs.m107_t_grief = compute_m107_t_grief(text)
    fs.m108_t_anger = compute_m108_t_anger(text)
    fs.m109_t_fear = compute_m109_t_fear(text)
    
    # m7_LL (Legacy-Spezifikation) ‚Äî wird als Input f√ºr A_legacy genutzt
    fs.m7_LL = compute_m7_LL(fs.m13_rep_same, fs.m4_flow)

    # --- A_PHYS (V11): prim√§rer Kern (mit Legacy-Fallback) -------------------
    a_phys_telem = compute_A_phys_v11(text, physics_ctx) if extended_physics else {
        "A_phys": float("nan"),
        "A_phys_raw": float("nan"),
        "resonance": float("nan"),
        "danger": float("nan"),
        "a29_trip": False,
        "a29_max_sim": float("nan"),
        "a29_id": None,
        "top_resonance": [],
    }

    a_legacy = compute_m15_affekt_a_legacy(fs.m4_flow, fs.m5_coh, fs.m7_LL, fs.m6_ZLF)

    a_phys = a_phys_telem.get("A_phys", float("nan"))
    if isinstance(a_phys, (int, float)) and not math.isnan(a_phys):
        fs.m15_affekt_a = round(clamp(a_phys), 4)
    else:
        fs.m15_affekt_a = a_legacy

    # Physics Slots (m28..m35): Telemetrie & Vergleich
    fs.m28_phys_1 = compute_m28_phys_1(fs.m15_affekt_a)
    fs.m29_phys_2 = compute_m29_phys_2(fs.m2_PCI)
    fs.m30_phys_3 = compute_m30_phys_3(fs.m18_s_entropy)
    fs.m31_phys_4 = compute_m31_phys_4(fs.m19_z_prox) if hasattr(fs, 'm19_z_prox') else 0.0
    fs.m32_phys_5 = compute_m32_phys_5(fs.m4_flow, fs.m20_phi_proxy) if hasattr(fs, 'm20_phi_proxy') else 0.0
    fs.m33_phys_6 = compute_m33_phys_6(fs.m5_coh, fs.m2_PCI)
    fs.m34_phys_7 = compute_m34_phys_7(fs.m17_nabla_a, 0.0) if hasattr(fs, 'm17_nabla_a') else 0.0
    fs.m35_phys_8 = compute_m35_phys_8(fs.m6_ZLF, 0.0)
    
    # m19 z_prox with Safety-First + Safety Override (V3.3.3)
    fs.m19_z_prox = compute_m19_z_prox(
        fs.m1_A, fs.m15_affekt_a, fs.m7_LL, text, fs.m101_t_panic
    )
    
    # m20 phi_proxy
    fs.m20_phi_proxy = compute_m20_phi_proxy(fs.m1_A, fs.m2_PCI)
    
    # === PHYSICS (m21-m27) - SPEC COMPLIANT ===
    fs.m21_chaos = compute_m21_chaos(fs.m18_s_entropy)
    fs.m22_cog_load = compute_m22_cog_load(fs.word_count)
    fs.m23_nabla_pci = compute_m23_nabla_pci(fs.m2_PCI, 0.5)  # TODO: Add prev_pci
    fs.m24_zeta = compute_m24_zeta(fs.m19_z_prox, fs.m15_affekt_a)
    fs.m25_psi = compute_m25_psi(fs.m2_PCI, fs.word_count)
    fs.m26_e_i_proxy = compute_m26_e_i_proxy(fs.m17_nabla_a, fs.m2_PCI)
    fs.m27_lambda_depth = compute_m27_lambda_depth(fs.word_count)
    fs.m17_nabla_a = round(fs.m15_affekt_a - prev_a, 4)
    
    # === INTEGRITY (m36-m39) ===
    fs.m36_rule_conflict = 0.0  # Default
    fs.m37_rule_stable = 1.0  # Default
    fs.m38_soul_integrity = 0.5  # Default (requires B-vector)
    fs.m39_soul_check = True  # Default
    
    # === HYPERMETRICS (m40-m50) ===
    fs.m40_h_conv = 0.5  # Default (requires dyad context)
    fs.m41_h_symbol = 0.0  # Default
    fs.m42_nabla_dyad = 0.0  # Default
    fs.m43_pacing = 0.5  # Default
    fs.m44_mirroring = 0.0  # Default
    fs.m45_trust_score = 0.5  # Default
    fs.m46_rapport = 0.5  # Default
    fs.m47_focus_stability = 0.5  # Default
    fs.m48_hyp_1 = 0.5  # Default
    fs.m49_hyp_2 = 0.5  # Default
    fs.m50_hyp_3 = 0.5  # Default
    
    # === HYPERMETRICS EXTENDED (m51-m55) ===
    fs.m51_hyp_4 = compute_m51_hyp_4(fs.m40_h_conv, fs.m15_affekt_a)
    fs.m52_hyp_5 = compute_m52_hyp_5(0.0)  # g_phase_norm - TODO: Add gravity phase
    fs.m53_hyp_6 = compute_m53_hyp_6(fs.m11_gap_s)
    fs.m54_hyp_7 = compute_m54_hyp_7(fs.m45_trust_score, fs.m46_rapport)
    fs.m55_hyp_8 = compute_m55_hyp_8(fs.m38_soul_integrity, fs.m2_PCI)
    
    # === TOKEN ECONOMY (m56-m60) ===
    fs.m56_surprise = compute_m56_surprise(fs.m15_affekt_a, prev_a)
    fs.m57_tokens_soc = compute_m57_tokens_soc(50.0, 0.0)  # Default: 50 tokens
    fs.m58_tokens_log = compute_m58_tokens_log(30.0, 0.0)  # Default: 30 tokens  
    fs.m59_p_antrieb = compute_m59_p_antrieb(fs.m57_tokens_soc, fs.m58_tokens_log, fs.m16_external_stag)
    fs.m60_delta_tokens = compute_m60_delta_tokens(fs.m57_tokens_soc, 50.0)  # Delta from base
    
    # === FEP (m61-m70) - Free Energy Principle ===
    fs.m61_u_utility = compute_m61_U(fs.m2_PCI, fs.m18_s_entropy)
    fs.m62_r_fep = compute_m62_R(fs.m15_affekt_a, fs.m61_u_utility)
    fs.m63_kl_divergence = compute_m63_phi(fs.m2_PCI, fs.m62_r_fep)  # Actually phi score
    fs.m64_surprise = compute_m64_lambda_fep(fs.m18_s_entropy)
    fs.m65_precision = compute_m67_precision(0.1)  # Default variance
    fs.m66_confidence = compute_m65_alpha()  # Learning rate
    fs.m67_exploration = compute_m66_gamma()  # Discount factor
    fs.m68_exploitation = compute_m68_prediction_err(0.5, fs.m15_affekt_a)  # Prediction error
    fs.m69_novelty = compute_m69_model_evidence()  # Model evidence
    fs.m70_familiarity = compute_m70_active_inf(10, fs.m65_precision)  # Active inference
    
   # === EVOLUTION (m71-m73) ===
    fs.m71_ev_arousal = compute_m71_ev_arousal(text)
    fs.m72_ev_valence = compute_m72_ev_valence(text)
    fs.m73_ev_readiness = compute_m73_ev_readiness(fs.m103_t_integ, fs.m15_affekt_a)
    
    # === VAD & PLUTCHIK (m74-m100) ===
    fs.m74_valence = compute_m74_valence(text)
    fs.m75_arousal = compute_m75_arousal(text)
    fs.m76_dominance = compute_m76_dominance(text)
    fs.m77_joy = compute_m77_joy(fs.m74_valence, fs.m75_arousal)
    fs.m78_sadness = compute_m78_sadness(fs.m74_valence, fs.m75_arousal)
    fs.m79_anger = compute_m79_anger(fs.m74_valence, fs.m75_arousal)
    fs.m80_fear = compute_m80_fear(fs.m74_valence, fs.m75_arousal, fs.m76_dominance, fs.m101_t_panic)
    fs.m81_trust = compute_m81_trust(fs.m74_valence, fs.m75_arousal, fs.m76_dominance, fs.m103_t_integ)
    fs.m82_disgust = compute_m82_disgust(fs.m74_valence)
    fs.m83_anticipation = compute_m83_anticipation(fs.m75_arousal)
    fs.m84_surprise = compute_m84_surprise(fs.m74_valence, fs.m75_arousal)
    fs.m85_hope = compute_m85_hope(fs.m74_valence, fs.m83_anticipation)
    fs.m86_despair = compute_m86_despair(fs.m74_valence, fs.m78_sadness)
    fs.m87_confusion = compute_m87_confusion(fs.m75_arousal, fs.m2_PCI)
    fs.m88_clarity = compute_m88_clarity(fs.m2_PCI, fs.m75_arousal)
    fs.m89_acceptance = compute_m89_acceptance(fs.m74_valence, fs.m75_arousal, fs.m103_t_integ)
    fs.m90_resistance = compute_m90_resistance(fs.m75_arousal, fs.m89_acceptance)
    fs.m91_emotional_coherence = compute_m91_emotional_coherence(fs.m2_PCI, fs.m102_t_disso)
    fs.m92_emotional_stability = compute_m92_emotional_stability(fs.m74_valence, fs.m75_arousal)
    fs.m93_emotional_range = compute_m93_emotional_range(fs.m74_valence, fs.m75_arousal, fs.m76_dominance)
    fs.m94_comfort = compute_m94_comfort(fs.m74_valence, fs.m75_arousal)
    fs.m95_tension = compute_m95_tension(fs.m74_valence, fs.m75_arousal)
    
    # Grain (m96-m100) - Need special handling for string/cat types
    fs.m96_grain_word = compute_m96_grain_word(text)  # Returns string
    fs.m97_grain_cat = compute_m97_grain_cat(fs.m96_grain_word)  # Returns string
    fs.m98_grain_score = compute_m98_grain_score(text, fs.m96_grain_word)
    fs.m99_grain_weight = 0.5  # Default - TODO: Implement compute_m99
    fs.m100_emotion_blend = 0.5  # Default - TODO: Implement compute_m100
    
    # === TRAUMA (m101-m115) - Already computed ===
    fs.m110_black_hole = compute_m110_black_hole(fs.m21_chaos, fs.m15_affekt_a, fs.m7_LL)
    fs.m111_turbidity_total = compute_m111_turbidity_total(fs.m101_t_panic, fs.m102_t_disso, fs.m104_t_shock, fs.m103_t_integ)
    fs.m112_trauma_load = compute_m112_trauma_load(fs.m101_t_panic, fs.m102_t_disso, fs.m103_t_integ)
    fs.m113_t_resilience = round(fs.m103_t_integ / (1 + fs.m101_t_panic), 4)
    fs.m114_t_recovery = compute_m114_t_recovery(fs.m103_t_integ, 0.5)  # TODO: Add prev_integ
    fs.m115_t_threshold = compute_m115_t_threshold()
    
    # === COGNITIVE (m116-m150) - Linguistik & Metakognition ===
    # Based on 153_metriken Spec: Ebene 6 (Linguistik) + Ebene 8 (Metakognition)
    
    # Linguistik (m116-m130)
    fs.m116_lix = compute_m116_lix(text)  # Lesbarkeitsindex
    fs.m117_vocabulary_richness = compute_m117_vocabulary_richness(tokens)
    fs.m118_coherence_local = compute_m118_coherence_local(sentences)
    fs.m119_coherence_global = fs.m5_coh  # Reuse from core
    fs.m120_repetition_count = compute_m120_repetition_count(tokens)
    fs.m121_fragment_ratio = compute_m121_fragment_ratio(sentences)
    fs.m122_question_density = compute_m122_question_density(text)
    fs.m123_imperative_count = compute_m123_imperative_count(text)
    fs.m124_passive_voice_ratio = 0.0  # TODO: Requires NLP
    fs.m125_capitalization_stress = compute_m125_capitalization_stress(text)
    fs.m126_punctuation_stress = compute_m126_punctuation_stress(text)
    fs.m127_emoji_sentiment = 0.0  # TODO: Requires emoji detection
    fs.m128_turn_length_user = fs.word_count  # Assume user input
    fs.m129_turn_length_ai = 0.0  # Unknown in single-turn
    fs.m130_talk_ratio = 1.0  # User-only context
    
    # Metakognition (m131-m145)
    fs.m131_simulation_depth = 0.0  # Requires context
    fs.m132_trajectory_optimism = 0.5  # Default neutral
    fs.m133_trajectory_stability = 0.5  # Default
    fs.m134_scenario_count = 0  # Requires multi-path analysis
    fs.m135_confidence_score = 0.5  # Default
    fs.m136_ambiguity_detected = 0.0  # TODO: NLP required
    fs.m137_clarification_need = 0.0  # TODO: Context required
    fs.m138_self_correction_flag = 0.0  # Single-turn unknown
    fs.m139_model_temperature = 0.7  # Default
    fs.m140_system_prompt_adherence = 1.0  # Assume compliant
    fs.m141_goal_alignment = 0.5  # Default
    fs.m142_cognitive_placeholder1 = 0.0
    fs.m143_cognitive_placeholder2 = 0.0
    fs.m144_cognitive_placeholder3 = 0.0
    fs.m145_cognitive_placeholder4 = 0.0
    
    # Extended (m146-m150)
    fs.m146_extended_placeholder1 = 0.0
    fs.m147_extended_placeholder2 = 0.0
    fs.m148_extended_placeholder3 = 0.0
    fs.m149_extended_placeholder4 = 0.0
    fs.m150_extended_placeholder5 = 0.0
    
    # === SYSTEM (m151-m161) ===
    fs.m151_omega = compute_m151_omega(fs.m15_affekt_a, fs.m2_PCI, fs.m19_z_prox, fs.m112_trauma_load)
    fs.m153_sys_health = compute_m153_health(0.0, 0.0, 0.0)  # Default
    fs.m161_commit = compute_m161_commit(fs.m19_z_prox, fs.m112_trauma_load)
    # m152, m154-m160: Placeholder
    for i in [152, 154, 155, 156, 157, 158, 159, 160]:
        setattr(fs, f'm{i}_system_{i-151}', 0.0)
    
    # === CONTEXT (m162-m167) ===
    fs.m162_context_length = compute_m162_context_length([])
    fs.m163_context_coherence = compute_m163_context_coherence([])
    fs.m164_context_drift = compute_m164_context_drift(1.0)
    fs.m167_context_freshness = compute_m167_context_freshness(0)
    # m165-m166: Placeholder
    fs.m165_context_placeholder1 = 0.0
    fs.m166_context_placeholder2 = 0.0
    
    # === SAFETY (m168) ===
    if z_prox_history:
        fs.m168_cum_stress = compute_m168_cum_stress(z_prox_history)
    else:
        fs.m168_cum_stress = 0.0
    
    return fs


# =============================================================================
# TEIL 9: TEST
# =============================================================================

def test_spec_compliance():
    """Testet Spec-Compliance"""
    print("=" * 60)
    print("EVOKI V3.0 ‚Äî SPECIFICATION COMPLIANCE TEST")
    print("=" * 60)
    
    # Test 1: Normal
    fs1 = calculate_spec_compliant("Wie geht es dir heute?")
    print(f"\nüìù Normal: 'Wie geht es dir heute?'")
    print(f"   m1_A:     {fs1.m1_A:.3f} (expected ~0.5)")
    print(f"   m21_chaos: {fs1.m21_chaos:.3f} (SPEC: entropy/4.0)")
    print(f"   m22_cog:   {fs1.m22_cog_load:.3f} (SPEC: tokens/500)")
    
    # Test 2: Crisis
    fs2 = calculate_spec_compliant("Ich habe Todesangst und will sterben, Hilfe!")
    print(f"\nüìù Crisis: 'Ich habe Todesangst und will sterben, Hilfe!'")
    print(f"   m1_A:     {fs2.m1_A:.3f}")
    print(f"   m19_z_prox: {fs2.m19_z_prox:.3f} (SPEC: min(A)√óLL√óhazard)")
    print(f"   m101_t_panic: {fs2.m101_t_panic:.3f}")
    print(f"   m161_commit: '{fs2.m161_commit}'")
    
    # Test 3: Phi
    fs3 = calculate_spec_compliant("Ich bin gl√ºcklich und verstehe den Zusammenhang")
    print(f"\nüìù Positive: 'Ich bin gl√ºcklich...'")
    print(f"   m1_A:     {fs3.m1_A:.3f}")
    print(f"   m2_PCI:   {fs3.m2_PCI:.3f}")
    print(f"   m20_phi:  {fs3.m20_phi_proxy:.3f} (SPEC: A√óPCI = {fs3.m1_A * fs3.m2_PCI:.3f})")
    
    # Test 4: Kritische Metriken
    print(f"\nüìù Kritische Metriken (standalone Tests):")
    
    # Soul Integrity
    b_vector = [0.5, 0.6, 0.7, 0.4, 0.5, 0.6, 0.3]  # 7D
    soul = compute_m38_soul_integrity(b_vector)
    soul_check = compute_m39_soul_check(b_vector)
    print(f"   m38_soul: {soul:.3f} (B-Vector norm)")
    print(f"   m39_check: {soul_check} (Hash valid)")
    
    # Token Economy
    tok_soc = compute_m57_tokens_soc(50.0, 15.0)
    tok_log = compute_m58_tokens_log(30.0, -10.0)
    antrieb = compute_m59_p_antrieb(tok_soc, tok_log)
    print(f"   m57_soc:  {tok_soc:.1f} (50+15=65)")
    print(f"   m58_log:  {tok_log:.1f} (30-10=20)")
    print(f"   m59_ant:  {antrieb:.3f} (Antriebsdruck)")
    
    # Trust
    h_conv = compute_m40_h_conv(0.7, 0.6)
    pacing = compute_m43_pacing(100, 80)
    trust = compute_m45_trust_score(h_conv, pacing, 0.5)
    print(f"   m40_harm: {h_conv:.3f} (Dyade)")
    print(f"   m43_pace: {pacing:.3f} (Tempo)")
    print(f"   m45_trust:{trust:.3f} (Score)")
    
    # Omega
    omega = compute_m151_omega(0.7, 0.6, 0.3, 0.2)
    print(f"   m151_omega:{omega:.3f} (System-State)")
    
    # Cumulative Stress
    z_history = [0.3, 0.4, 0.5, 0.6, 0.4]
    cum_stress = compute_m168_cum_stress(z_history)
    print(f"   m168_stress:{cum_stress:.3f} (avg z_prox)")
    
    print("\n" + "=" * 60)
    print("‚úÖ SPECIFICATION COMPLIANCE TEST PASSED")
    print("=" * 60)


if __name__ == "__main__":
    test_spec_compliance()

