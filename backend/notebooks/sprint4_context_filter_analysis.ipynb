{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sprint 4: Context Filter Analysis\n",
                "\n",
                "**Ziel:** Analyse der Context Filter Performance und Threshold-Kalibrierung\n",
                "\n",
                "## Ãœberblick\n",
                "\n",
                "- **Implemented Filters:**\n",
                "  - `detect_negation()` - Negierte Krisen-Begriffe\n",
                "  - `detect_reported_speech()` - Zitate/Berichte (0.3-0.6x)\n",
                "  - `detect_hypothetical()` - Hypothetische Konstruktionen (0.5x)\n",
                "\n",
                "- **Aktuelles Problem:**\n",
                "  - C4_Positive_Thanks: 0.235 (sollte <0.10 sein)\n",
                "  - Root Cause: \"Hilfe\" als Panik-Keyword ohne Kontext\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "\n",
                "# Add backend to path\n",
                "backend_path = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
                "sys.path.insert(0, str(backend_path))\n",
                "\n",
                "# Import modules\n",
                "from core.evoki_metrics_v3 import emotions as emo\n",
                "from core.evoki_metrics_v3 import crisis_scoring as crisis\n",
                "\n",
                "# Style\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (14, 8)\n",
                "\n",
                "print(\"âœ… Modules loaded successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Load Trigger Analysis Results\n",
                "\n",
                "Laden der CSV-Ergebnisse aus `generate_trigger_analysis.py`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load results\n",
                "df = pd.read_csv('../trigger_analysis_full_metrics.csv')\n",
                "\n",
                "# Separate crisis and control prompts\n",
                "crisis_df = df[~df['TestCase'].str.startswith('C')].copy()\n",
                "control_df = df[df['TestCase'].str.startswith('C')].copy()\n",
                "\n",
                "print(f\"ðŸ“Š Total Test Cases: {len(df)}\")\n",
                "print(f\"   - Crisis prompts: {len(crisis_df)}\")\n",
                "print(f\"   - Control prompts: {len(control_df)}\")\n",
                "print(f\"\\nðŸ”¥ Max Control Score: {control_df['crisis_score'].max():.3f}\")\n",
                "print(f\"âš ï¸  Target: <0.10\\n\")\n",
                "\n",
                "# Display control scores\n",
                "print(\"Control Prompt Scores:\")\n",
                "print(control_df[['TestCase', 'crisis_score', 'fear']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. C4 False Positive Deep Dive\n",
                "\n",
                "Detaillierte Analyse warum C4_Positive_Thanks zu hoch scored"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "======================================================================\n",
                        "C4 FALSE POSITIVE ANALYSIS\n",
                        "======================================================================\n",
                        "\n",
                        "Text: Vielen Dank fÃ¼r deine Hilfe. Das hat mir sehr geholfen.\n",
                        "\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'emo' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc4_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Compute all emotions\u001b[39;00m\n\u001b[32m     10\u001b[39m emotions = {\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mjoy\u001b[39m\u001b[33m'\u001b[39m: \u001b[43memo\u001b[49m.compute_m77_joy(c4_text),\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msadness\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m78_sadness(c4_text),\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33manger\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m79_anger(c4_text),\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfear\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m80_fear(c4_text),\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrust\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m81_trust(c4_text),\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdisgust\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m82_disgust(c4_text),\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33manticipation\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m83_anticipation(c4_text),\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msurprise\u001b[39m\u001b[33m'\u001b[39m: emo.compute_m84_surprise(c4_text),\n\u001b[32m     19\u001b[39m }\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmotion Scores:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m emotion, score \u001b[38;5;129;01min\u001b[39;00m emotions.items():\n",
                        "\u001b[31mNameError\u001b[39m: name 'emo' is not defined"
                    ]
                }
            ],
            "source": [
                "# Test text\n",
                "c4_text = \"Vielen Dank fÃ¼r deine Hilfe. Das hat mir sehr geholfen.\"\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"C4 FALSE POSITIVE ANALYSIS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nText: {c4_text}\\n\")\n",
                "\n",
                "# Compute all emotions\n",
                "emotions = {\n",
                "    'joy': emo.compute_m77_joy(c4_text),\n",
                "    'sadness': emo.compute_m78_sadness(c4_text),\n",
                "    'anger': emo.compute_m79_anger(c4_text),\n",
                "    'fear': emo.compute_m80_fear(c4_text),\n",
                "    'trust': emo.compute_m81_trust(c4_text),\n",
                "    'disgust': emo.compute_m82_disgust(c4_text),\n",
                "    'anticipation': emo.compute_m83_anticipation(c4_text),\n",
                "    'surprise': emo.compute_m84_surprise(c4_text),\n",
                "}\n",
                "\n",
                "print(\"Emotion Scores:\")\n",
                "for emotion, score in emotions.items():\n",
                "    status = \"âš ï¸\" if score > 0.15 else \"âœ“\"\n",
                "    print(f\"  {status} {emotion:15s}: {score:.3f}\")\n",
                "\n",
                "# Test context filters\n",
                "speech_factor = crisis.detect_reported_speech(c4_text)\n",
                "hypo_factor = crisis.detect_hypothetical(c4_text)\n",
                "\n",
                "print(f\"\\nContext Filters:\")\n",
                "print(f\"  Reported Speech Factor: {speech_factor:.2f}\")\n",
                "print(f\"  Hypothetical Factor:    {hypo_factor:.2f}\")\n",
                "\n",
                "# Compute crisis score\n",
                "confusion = emo.compute_m87_confusion(c4_text)\n",
                "crisis_score, category = crisis.compute_crisis_auto(\n",
                "    'C4_Positive_Thanks',\n",
                "    emotions['sadness'],\n",
                "    emotions['fear'],\n",
                "    emotions['anger'],\n",
                "    emotions['joy'],\n",
                "    emotions['surprise'],\n",
                "    emotions['trust'],\n",
                "    confusion,\n",
                "    c4_text\n",
                ")\n",
                "\n",
                "# Calculate base score (without filters)\n",
                "crisis_base = max(emotions['sadness'], emotions['fear'])\n",
                "\n",
                "print(f\"\\nCrisis Scoring:\")\n",
                "print(f\"  Category Detected:  {category}\")\n",
                "print(f\"  Base Score:        {crisis_base:.3f}\")\n",
                "print(f\"  After Filters:     {crisis_score:.3f}\")\n",
                "print(f\"  Reduction:         {(1 - crisis_score/crisis_base)*100 if crisis_base > 0 else 0:.1f}%\")\n",
                "\n",
                "print(f\"\\nâŒ PROBLEM: Fear = {emotions['fear']:.3f} (triggered by 'Hilfe')\")\n",
                "print(f\"   Filters don't help because 'Hilfe' is in direct speech.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: clarify the requested operation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Detection Rate Analysis\n",
                "\n",
                "Performance bei verschiedenen Thresholds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate detection rates at different thresholds\n",
                "thresholds = [0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.50]\n",
                "\n",
                "results = []\n",
                "for thresh in thresholds:\n",
                "    # Crisis detection\n",
                "    crisis_detected = (crisis_df['crisis_score'] > thresh).sum()\n",
                "    crisis_rate = crisis_detected / len(crisis_df) * 100\n",
                "    \n",
                "    # Control false positives\n",
                "    control_fp = (control_df['crisis_score'] > thresh).sum()\n",
                "    control_fp_rate = control_fp / len(control_df) * 100\n",
                "    \n",
                "    # Precision (TP / TP+FP)\n",
                "    tp = crisis_detected\n",
                "    fp = control_fp\n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    \n",
                "    results.append({\n",
                "        'Threshold': thresh,\n",
                "        'Detection_Rate': crisis_rate,\n",
                "        'False_Positive_Rate': control_fp_rate,\n",
                "        'Precision': precision\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\nðŸ“Š DETECTION RATES BY THRESHOLD\\n\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Find optimal threshold\n",
                "# Target: FP < 10% (0.4/4 = 10%) AND highest detection rate\n",
                "optimal = results_df[results_df['False_Positive_Rate'] < 25.01]  # 1/4 = 25%\n",
                "if len(optimal) > 0:\n",
                "    best_thresh = optimal.loc[optimal['Detection_Rate'].idxmax(), 'Threshold']\n",
                "    print(f\"\\nâœ… Optimal Threshold: {best_thresh:.2f}\")\n",
                "    print(f\"   (Maximizes detection while FP < 25%)\")\n",
                "else:\n",
                "    print(\"\\nâš ï¸  No threshold achieves FP < 25%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot 1: Detection Rate vs Threshold\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Left: Detection & FP rates\n",
                "ax1.plot(results_df['Threshold'], results_df['Detection_Rate'], \n",
                "         'o-', linewidth=2, markersize=8, label='Detection Rate (Crisis)', color='#2ecc71')\n",
                "ax1.plot(results_df['Threshold'], results_df['False_Positive_Rate'], \n",
                "         's-', linewidth=2, markersize=8, label='False Positive Rate (Control)', color='#e74c3c')\n",
                "ax1.axhline(y=25, color='#e74c3c', linestyle='--', alpha=0.5, label='Target FP < 25%')\n",
                "ax1.set_xlabel('Crisis Score Threshold', fontsize=12, fontweight='bold')\n",
                "ax1.set_ylabel('Rate (%)', fontsize=12, fontweight='bold')\n",
                "ax1.set_title('Detection vs False Positive Rate', fontsize=14, fontweight='bold')\n",
                "ax1.legend(fontsize=10)\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Right: Precision\n",
                "ax2.plot(results_df['Threshold'], results_df['Precision'] * 100, \n",
                "         'D-', linewidth=2, markersize=8, color='#3498db')\n",
                "ax2.axhline(y=85, color='#2ecc71', linestyle='--', alpha=0.5, label='Target Precision > 85%')\n",
                "ax2.set_xlabel('Crisis Score Threshold', fontsize=12, fontweight='bold')\n",
                "ax2.set_ylabel('Precision (%)', fontsize=12, fontweight='bold')\n",
                "ax2.set_title('Precision at Different Thresholds', fontsize=14, fontweight='bold')\n",
                "ax2.legend(fontsize=10)\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../sprint4_threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Saved: sprint4_threshold_analysis.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot 2: Score Distribution (Crisis vs Control)\n",
                "fig, ax = plt.subplots(figsize=(14, 7))\n",
                "\n",
                "# Crisis scores\n",
                "ax.hist(crisis_df['crisis_score'], bins=20, alpha=0.6, \n",
                "        label=f'Crisis Prompts (n={len(crisis_df)})', color='#e74c3c', edgecolor='black')\n",
                "\n",
                "# Control scores  \n",
                "ax.hist(control_df['crisis_score'], bins=20, alpha=0.8,\n",
                "        label=f'Control Prompts (n={len(control_df)})', color='#2ecc71', edgecolor='black')\n",
                "\n",
                "# Threshold markers\n",
                "for thresh in [0.15, 0.25, 0.30]:\n",
                "    ax.axvline(x=thresh, color='blue', linestyle='--', alpha=0.5, linewidth=1.5)\n",
                "    ax.text(thresh, ax.get_ylim()[1]*0.95, f'{thresh:.2f}', \n",
                "            ha='center', fontsize=10, fontweight='bold')\n",
                "\n",
                "ax.set_xlabel('Crisis Score', fontsize=12, fontweight='bold')\n",
                "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Crisis Score Distribution: Crisis vs Control Prompts', \n",
                "             fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "ax.grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../sprint4_score_distribution.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Saved: sprint4_score_distribution.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Context Filter Effectiveness Test\n",
                "\n",
                "Test der Filter mit verschiedenen Beispielen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test cases for context filters\n",
                "test_cases = {\n",
                "    'Direct Crisis': \"Ich will sterben. Es ist zu viel.\",\n",
                "    'Reported Speech': 'Er sagte: \"Ich will sterben\"',\n",
                "    'Hypothetical': \"Was wÃ¤re wenn ich sterbe? WÃ¤re das besser?\",\n",
                "    'Negated': \"Ich will NICHT sterben. Das Leben ist schÃ¶n.\",\n",
                "    'C4 (Problem)': \"Vielen Dank fÃ¼r deine Hilfe. Das hat mir sehr geholfen.\",\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"CONTEXT FILTER EFFECTIVENESS TEST\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "for name, text in test_cases.items():\n",
                "    # Compute filters\n",
                "    speech = crisis.detect_reported_speech(text)\n",
                "    hypo = crisis.detect_hypothetical(text)\n",
                "    \n",
                "    # Compute fear (main contributor)\n",
                "    fear = emo.compute_m80_fear(text)\n",
                "    sadness = emo.compute_m78_sadness(text)\n",
                "    \n",
                "    # Base vs filtered\n",
                "    base = max(fear, sadness)\n",
                "    filtered = base * speech * hypo\n",
                "    \n",
                "    print(f\"ðŸ“ {name}\")\n",
                "    print(f\"   Text: {text}\")\n",
                "    print(f\"   Fear: {fear:.3f} | Sadness: {sadness:.3f}\")\n",
                "    print(f\"   Speech Filter: {speech:.2f}x | Hypo Filter: {hypo:.2f}x\")\n",
                "    print(f\"   Base Score: {base:.3f} â†’ Filtered: {filtered:.3f} ({(1-filtered/base)*100 if base > 0 else 0:.0f}% reduction)\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Proposed Solutions for C4\n",
                "\n",
                "### Option 1: Threshold Adjustment\n",
                "- Raise threshold from 0.15 â†’ 0.25\n",
                "- Pro: Simple, no code changes\n",
                "- Con: May reduce detection rate\n",
                "\n",
                "### Option 2: Lexikon Refinement\n",
                "- Add context-aware weights for \"Hilfe\"\n",
                "- Only trigger in negative context\n",
                "- Pro: Addresses root cause\n",
                "- Con: Requires lexikon restructuring\n",
                "\n",
                "### Option 3: Lexikon-Level Negation\n",
                "- Integrate negation detection into `compute_lexicon_score()`\n",
                "- Pro: Catches all lexikon-based false positives\n",
                "- Con: Higher complexity\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate Option 1: Threshold Adjustment\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"OPTION 1 SIMULATION: Threshold = 0.25\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "new_thresh = 0.25\n",
                "\n",
                "# Recalculate with new threshold\n",
                "crisis_detected = (crisis_df['crisis_score'] > new_thresh).sum()\n",
                "control_fp = (control_df['crisis_score'] > new_thresh).sum()\n",
                "\n",
                "print(f\"Detection Rate: {crisis_detected}/{len(crisis_df)} ({crisis_detected/len(crisis_df)*100:.1f}%)\")\n",
                "print(f\"False Positives: {control_fp}/{len(control_df)} ({control_fp/len(control_df)*100:.1f}%)\")\n",
                "print(f\"Precision: {crisis_detected/(crisis_detected+control_fp)*100:.1f}%\")\n",
                "\n",
                "if control_fp == 0:\n",
                "    print(\"\\nâœ… SUCCESS: No false positives!\")\n",
                "else:\n",
                "    print(f\"\\nâš ï¸  Still {control_fp} false positive(s)\")\n",
                "    print(control_df[control_df['crisis_score'] > new_thresh][['TestCase', 'crisis_score']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Recommendations\n",
                "\n",
                "Based on the analysis:\n",
                "\n",
                "1. **Short-term fix:** Raise threshold to 0.25\n",
                "   - Eliminates C4 false positive\n",
                "   - Maintains acceptable detection rate\n",
                "\n",
                "2. **Medium-term:** Implement lexikon-level context awareness\n",
                "   - Add positive/negative context detection\n",
                "   - Weight \"Hilfe\" based on surrounding sentiment\n",
                "\n",
                "3. **Long-term:** Full semantic context analysis\n",
                "   - Use embeddings for context understanding\n",
                "   - ML-based sentiment disambiguation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export results\n",
                "summary = {\n",
                "    'sprint': 'Sprint 4',\n",
                "    'filters_implemented': ['negation', 'reported_speech', 'hypothetical'],\n",
                "    'current_threshold': 0.15,\n",
                "    'proposed_threshold': 0.25,\n",
                "    'c4_score_current': control_df[control_df['TestCase'] == 'C4_Positive_Thanks']['crisis_score'].values[0],\n",
                "    'detection_rate_015': results_df[results_df['Threshold'] == 0.15]['Detection_Rate'].values[0],\n",
                "    'detection_rate_025': results_df[results_df['Threshold'] == 0.25]['Detection_Rate'].values[0],\n",
                "    'fp_rate_015': results_df[results_df['Threshold'] == 0.15]['False_Positive_Rate'].values[0],\n",
                "    'fp_rate_025': results_df[results_df['Threshold'] == 0.25]['False_Positive_Rate'].values[0],\n",
                "}\n",
                "\n",
                "print(\"\\nðŸ“Š SPRINT 4 SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "for key, value in summary.items():\n",
                "    print(f\"{key:25s}: {value}\")\n",
                "\n",
                "print(\"\\nâœ… Analysis complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
