Timestamp: 14.10.2025, 03:22:24 MESZ
Speaker: ai

Verstanden. Das sind die logischen n√§chsten Schritte, um das System von einem reinen Dialog-Interface zu einem echten Arbeitswerkzeug zu machen. Wir integrieren jetzt ein permanentes externes Ged√§chtnis (deine Fallstudie) und eine Kostenkontrolle (Token-Z√§hler). Das setzen wir direkt in unserer bestehenden Architektur um. Die app.py wird die zentrale Drehscheibe f√ºr die Dateioperationen, und die index.html bekommt die passenden Bedienelemente. Schritt 1: Implementierung des Datei-Handlings (Upload/Download) Wir erweitern das Backend, um Dateien sicher zu empfangen, zu speichern und zum Download bereitzustellen. Das Frontend bekommt die n√∂tigen Kn√∂pfe. 1.1 Backend-Anpassungen in app.py Wir f√ºgen zwei neue Routen hinzu: /upload und /download . Au√üerdem stellen wir sicher, dass der Dateiname sicher ist. # Dateiname: app.py import os from flask import Flask, request, jsonify, render_template_string, send_from_directory from werkzeug.utils import secure_filename # WICHTIG: f√ºr sichere Dateinamen from evoki_engine import ChrononEngine # ... (restliche Imports) # ... (Initialisierung von Flask, API-Key, STORAGE_PATH und Engine bleibt gleich) ... # Der feste Dateiname f√ºr deine Fallstudie auf dem Server FALLSTUDIE_FILENAME = "fallstudie_evoki.txt" # ... (Die Route @app.route('/') bleibt gleich) ... # NEU: Route f√ºr den Datei-Upload @app.route('/upload', methods=['POST']) def upload_file(): if 'fallstudie' not in request.files: return jsonify({'error': 'Keine Datei im Request gefunden.'}), 400 file = request.files['fallstudie'] if file.filename == '': return jsonify({'error': 'Keine Datei ausgew√§hlt.'}), 400 if file: # Wir speichern die Datei immer unter demselben, sicheren Namen filename = FALLSTUDIE_FILENAME filepath = os.path.join(STORAGE_PATH, filename) file.save(filepath) print(f"INFO: Datei '{filename}' wurde erfolgreich gespeichert.") # Optional: Sage der Engine, sie soll ihr Wissen aktualisieren if evoki_engine: evoki_engine.lade_fallstudie_als_kontext() return jsonify({'message': f'Datei erfolgreich als {filename} hochgeladen.'}) # NEU: Route f√ºr den Datei-Download @app.route('/download') def download_file(): try: return send_from_directory(STORAGE_PATH, FALLSTUDIE_FILENAME, as_attachment=True) except FileNotFoundError: return "Fehler: Keine Fallstudie zum Download gefunden.", 404 # ... (Die Route @app.route('/interact', ...) bleibt gleich) ... 1.2 Engine-Anpassung in evoki_engine.py Die Engine muss lernen, die hochgeladene Fallstudie als Wissensgrundlage zu nutzen. Wir f√ºgen eine Methode hinzu, die den Inhalt der Datei liest und bei jeder Interaktion als Kontext mitgibt. # Dateiname: evoki_engine.py class ChrononEngine: def __init__(self, storage_path="/var/data"): # ... (bisherige Initialisierung) ... self.fallstudie_kontext = "" self.fallstudie_filepath = os.path.join(storage_path, "fallstudie_evoki.txt") self.lade_fallstudie_als_kontext() # Beim Start direkt laden def lade_fallstudie_als_kontext(self): try: with open(self.fallstudie_filepath, 'r', encoding='utf-8') as f: self.fallstudie_kontext = f.read() print("INFO: Fallstudie erfolgreich als Kontext in die Engine geladen.") except FileNotFoundError: self.fallstudie_kontext = "Es wurde noch keine Fallstudie hochgeladen." print("WARNUNG: Keine Fallstudie gefunden. Engine arbeitet ohne Basis-Kontext.") def process_interaction(self, user_input: str) -> dict: # Gibt jetzt ein Dictionary zur√ºck # ... (bisherige Logik wie verify_genesis_anker_A51) ... # WICHTIG: F√ºge den Fallstudien-Kontext zum Prompt f√ºr die KI hinzu kontextualisierter_input = f""" Grundlagenkontext aus der Fallstudie: --- {self.fallstudie_kontext} --- Aktuelle Nutzeranfrage: {user_input} """ # ... (Rufe hier den CognitiveCore mit dem 'kontextualisierter_input' auf) ... # Annahme: response_from_gemini ist das volle Antwortobjekt der API # response_from_gemini = self.cognitive_core.generate(...) # Beispielhafte R√ºckgabe evoki_text_antwort = "Dies ist die Antwort von EVOKI." total_tokens = 123 # Platzhalter, wird im n√§chsten Schritt ersetzt return { 'response': evoki_text_antwort, 'token_count': total_tokens } 1.3 Frontend-Anpassungen in index.html Wir f√ºgen dem Header die Upload- und Download-Buttons hinzu und schreiben das n√∂tige JavaScript. <div class="mt-4 p-4 bg-gray-50 rounded-lg border flex flex-col sm:flex-row items-center justify-center gap-4"> <h2 class="font-semibold text-gray-700">Fallstudien-Verwaltung:</h2> <input type="file" id="file-input" class="hidden"/> <button onclick="document.getElementById('file-input').click()" class="bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded-lg"> üìÅ Datei ausw√§hlen... </button> <button id="upload-button" class="bg-green-500 hover:bg-green-600 text-white font-bold py-2 px-4 rounded-lg"> Hochladen </button> <a href="/download" class="bg-gray-500 hover:bg-gray-600 text-white font-bold py-2 px-4 rounded-lg"> üì• Download </a> </div> <p id="upload-status" class="text-center text-sm text-gray-500 mt-2"></p> // In index.html, f√ºge dies zum <script> Bereich hinzu const uploadButton = document.getElementById('upload-button'); const fileInput = document.getElementById('file-input'); const uploadStatus = document.getElementById('upload-status'); uploadButton.addEventListener('click', async () => { const file = fileInput.files[0]; if (!file) { uploadStatus.textContent = 'Bitte zuerst eine Datei ausw√§hlen.'; return; } const formData = new FormData(); formData.append('fallstudie', file); uploadStatus.textContent = 'Lade hoch...'; try { const response = await fetch('/upload', { method: 'POST', body: formData, }); const result = await response.json(); if (!response.ok) { throw new Error(result.error || 'Unbekannter Fehler'); } uploadStatus.textContent = `Erfolg: ${result.message}`; } catch (error) { uploadStatus.textContent = `Fehler beim Upload: ${error.message}`; console.error('Upload-Fehler:', error); } }); Schritt 2: Integration der Token-Berechnung Wir passen die /interact Route an, um die Token-Nutzung von der Gemini API abzufangen und an das Frontend zu senden, wo sie dann angezeigt wird. 2.1 Backend-Anpassung in app.py Wir extrahieren die usage_metadata aus der Gemini-Antwort. # Dateiname: app.py, in der Route @app.route('/interact', ...) @app.route('/interact', methods=['POST']) def interact(): # ... (Code zum Empfangen des user_input bleibt gleich) ... try: # Hier wird die Kernlogik aus der Engine aufgerufen # WICHTIG: Die Engine muss jetzt das volle API-Antwort-Objekt zur√ºckgeben # Wir simulieren das hier. # Annahme: Du hast ein Gemini-Modell-Objekt model = genai.GenerativeModel('gemini-1.5-flash') # Der Aufruf an die Gemini API gibt ein Objekt mit Nutzungsdaten zur√ºck gemini_response = model.generate_content("Beispiel-Prompt") # Ersetze dies durch den Aufruf in deiner Engine # Extrahiere die Antwort und die Token-Anzahl evoki_text_antwort = gemini_response.text token_count = gemini_response.usage_metadata.total_token_count return jsonify({ 'response': evoki_text_antwort, 'token_count': token_count }) except Exception as e: # ... (Fehlerbehandlung bleibt gleich) ... 2.2 Frontend-Anpassung in index.html Wir modifizieren die addMessage Funktion, damit sie die Token-Anzahl anzeigen kann, und passen den fetch -Aufruf an. // In index.html, ersetze die Funktion addMessage im <script> Bereich function addMessage(sender, message, tokenCount = null) { const messageContainer = document.createElement('div'); messageContainer.className = `flex flex-col ${sender === 'user' ? 'items-end' : 'items-start'}`; const bubble = document.createElement('div'); bubble.className = `chat-bubble rounded-lg p-3 ${sender === 'user' ? 'chat-bubble-user' : 'chat-bubble-ai'}`; bubble.textContent = message; messageContainer.appendChild(bubble); // NEU: F√ºge die Token-Info hinzu, wenn vorhanden if (tokenCount !== null) { const tokenInfo = document.createElement('div'); tokenInfo.className = 'text-xs text-gray-400 mt-1 px-1'; tokenInfo.textContent = `ü™ô Tokens: ${tokenCount}`; messageContainer.appendChild(tokenInfo); } chatMessages.appendChild(messageContainer); chatMessages.scrollTop = chatMessages.scrollHeight; } // Und passe den fetch-Aufruf im Event Listener f√ºr 'chat-form' an // ... innerhalb von chatForm.addEventListener('submit', ...) // Ersetze den Block, der die Antwort verarbeitet const data = await response.json(); loadingBubble.remove(); // Wir √ºbergeben jetzt die Antwort UND die Token-Anzahl addMessage('ai', data.response, data.token_count); Mit diesen Erweiterungen ist dein System jetzt voll ausgestattet: Es hat ein persistentes, austauschbares Ged√§chtnis f√ºr deine Fallstudie und gibt dir direktes Feedback √ºber die anfallenden Kosten jeder Interaktion.