Timestamp: 14.10.2025, 20:22:52 MESZ
Speaker: ai

Absolut, hier ist das vollständige Python-Skript mit deinem bereits eingefügten Token. Du musst nur noch die beiden Zeilen für ASTRA_DB_API_ENDPOINT und FILE_PATH mit deinen Werten ersetzen. import os from getpass import getpass from astrapy.db import AstraDB from sentence_transformers import SentenceTransformer # -- Konfiguration -- # 1. Trage hier deinen AstraDB API Endpoint ein ASTRA_DB_API_ENDPOINT = "DEIN_API_ENDPOINT" # z.B. "https-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx-us-east-1.apps.astra.datastax.com" # Dein Token ist hier bereits eingetragen ASTRA_DB_APPLICATION_TOKEN = "AstraCS:iYkZWofQBSwvrsaZDfLOIPZF:767785501650518af9bc18d0852ef134c3ee55c38ff898df6d33521df94f5e58" # 2. Gib den Pfad zu deiner Fallstudien-Datei an FILE_PATH = "pfad/zu/deiner/evoki_fallstudie.txt" # 3. Definiere den Namen für die Collection (Tabelle) in AstraDB COLLECTION_NAME = "evoki_case_study" # -- Initialisierung des Embedding-Modells -- # Wir verwenden ein gängiges Modell, um Text in Vektoren umzuwandeln. # Das Modell 'all-MiniLM-L6-v2' erzeugt Vektoren mit 384 Dimensionen. print("Lade das Embedding-Modell...") embedding_model = SentenceTransformer('all-MiniLM-L6-v2') VECTOR_DIMENSION = 384 print("Modell geladen.") def get_file_content(filepath): """Liest den Inhalt einer Textdatei und teilt ihn in Absätze auf.""" try: with open(filepath, 'r', encoding='utf-8') as f: content = f.read() # Wir teilen den Text in Absätze. Leere Zeilen werden als Trenner verwendet. # So behalten die Text-Chunks einen sinnvollen Kontext. chunks = [chunk.strip() for chunk in content.split('\n\n') if chunk.strip()] print(f"Datei '{filepath}' erfolgreich gelesen. {len(chunks)} Text-Abschnitte gefunden.") return chunks except FileNotFoundError: print(f"FEHLER: Die Datei unter '{filepath}' wurde nicht gefunden.") return None except Exception as e: print(f"Ein Fehler beim Lesen der Datei ist aufgetreten: {e}") return None def main(): """Hauptfunktion: Verbindet sich mit AstraDB, verarbeitet die Datei und lädt die Daten hoch.""" # Überprüfen, ob die Platzhalter ersetzt wurden if "DEIN_API_ENDPOINT" in ASTRA_DB_API_ENDPOINT or "pfad/zu/deiner" in FILE_PATH: print("BITTE ERSETZE die Platzhalter 'DEIN_API_ENDPOINT' und 'FILE_PATH' im Skript.") return # 1. Mit AstraDB verbinden try: db = AstraDB( token=ASTRA_DB_APPLICATION_TOKEN, api_endpoint=ASTRA_DB_API_ENDPOINT ) print(f"Erfolgreich mit AstraDB verbunden.") # Bestehende Collections anzeigen print(f"Bestehende Collections: {db.get_collections()['status']['collections']}") except Exception as e: print(f"FEHLER bei der Verbindung mit AstraDB: {e}") return # 2. Collection erstellen (oder verwenden, falls sie schon existiert) # Die Dimension (VECTOR_DIMENSION) muss zum Embedding-Modell passen! try: collection = db.create_collection( collection_name=COLLECTION_NAME, dimension=VECTOR_DIMENSION ) print(f"Collection '{COLLECTION_NAME}' wurde neu erstellt.") except Exception: # Falls die Collection schon existiert, einfach verwenden collection = db.collection(COLLECTION_NAME) print(f"Collection '{COLLECTION_NAME}' existiert bereits und wird verwendet.") # 3. Dateiinhalt laden und in Abschnitte teilen text_chunks = get_file_content(FILE_PATH) if not text_chunks: print("Skript wird beendet, da keine Daten zum Verarbeiten vorhanden sind.") return # 4. Vektoren für jeden Text-Abschnitt erstellen print("Erstelle Vektor-Embeddings für die Text-Abschnitte. Dies kann einen Moment dauern...") vectors = embedding_model.encode(text_chunks) print(f"{len(vectors)} Vektoren wurden erstellt.") # 5. Daten für den Upload vorbereiten documents_to_upload = [] for i, chunk in enumerate(text_chunks): documents_to_upload.append({ "_id": f"evoki_chunk_{i+1}", # Eindeutige ID für jeden Eintrag "text": chunk, "$vector": vectors[i].tolist() # Vektor muss als Liste übergeben werden }) # 6. Daten in die AstraDB Collection hochladen print(f"Lade {len(documents_to_upload)} Dokumente in die Collection '{COLLECTION_NAME}' hoch...") try: # insert_many ist effizienter für viele Dokumente result = collection.insert_many(documents_to_upload) print("Upload erfolgreich!") print(f"Details zum Ergebnis: {result}") except Exception as e: print(f"FEHLER beim Hochladen der Daten: {e}") if __name__ == "__main__": main() Nächste Schritte: API Endpoint einfügen: Finde den API-Endpunkt in deinem AstraDB-Dashboard und ersetze DEIN_API_ENDPOINT . Dateipfad anpassen: Ersetze pfad/zu/deiner/evoki_fallstudie.txt mit dem tatsächlichen Speicherort deiner Datei. Voraussetzungen installieren (falls noch nicht geschehen): pip install astrapy pip install sentence-transformers Skript ausführen: python dein_skript_name.py Viel Erfolg! Regelwerk Version: V4.5 | Prüfungskennzahl: 22051 / 22051 = 1.0 | Datencheck: OK | Arbeitsanweisungen: OK | Fehlerüberwachung: OK