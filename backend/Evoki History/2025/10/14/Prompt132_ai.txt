Timestamp: 14.10.2025, 20:29:21 MESZ
Speaker: ai

Ah, danke für die Klarstellung! Das ist ein sehr wichtiger Punkt. Du hast also die entscheidenden Informationen (die "Vektoren" oder Text-Abschnitte) bereits gedanklich aus deinen Originaldateien – der Fallstudie Evoki...json und der MeineAktivitätentxt.txt – herausgefiltert und mir als Beispiel in der JSON-Struktur gezeigt. Das ist perfekt, denn genau diesen Prozess des "Heraussuchens" automatisiert das Skript für dich! Das Skript, das ich dir in meiner vorletzten Antwort ( upload_evoki_substrates.py ) gegeben habe, ist genau das richtige Werkzeug dafür. Es schaut sich deine beiden Originaldateien an, extrahiert automatisch die relevanten Textstellen (wie den Chatverlauf, die persönlichen Daten, die Projekt-Historie usw.) und lädt genau diese Informationen dann in die Vektor-Datenbank hoch. Du musst die Vektoren also nicht von Hand heraussuchen. Das Skript erledigt das für dich. Dein finales Skript zum Vektorisieren der Originaldateien Hier ist noch einmal das korrekte Skript, das deine beiden Quelldateien verarbeitet. Wir nennen es upload_evoki_substrates.py , um klarzustellen, dass es deine Original-Substrate hochlädt. import os import json from astrapy.db import AstraDB from sentence_transformers import SentenceTransformer # -- Konfiguration -- # 1. Trage hier deinen AstraDB API Endpoint ein ASTRA_DB_API_ENDPOINT = "DEIN_API_ENDPOINT" # z.B. "https-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx-us-east-1.apps.astra.datastax.com" # Dein Token ist hier bereits eingetragen ASTRA_DB_APPLICATION_TOKEN = "AstraCS:iYkZWofQBSwvrsaZDfLOIPZF:767785501650518af9bc18d0852ef134c3ee55c38ff898df6d33521df94f5e58" # 2. Pfade zu deinen Originaldateien (sollten im selben Ordner liegen) JSON_SUBSTRATE_PATH = "Fallstudie Evoki mit Verlauf_INTEGRATED_20251009_124322.json" TEXT_SUBSTRATE_PATH = "MeineAktivitätentxt.txt" # 3. Definiere den Namen für die Collection (Tabelle) in AstraDB COLLECTION_NAME = "evoki_wissensbasis" # -- Initialisierung des Embedding-Modells -- print("Lade das Embedding-Modell...") embedding_model = SentenceTransformer('all-MiniLM-L6-v2') VECTOR_DIMENSION = 384 print("Modell geladen.") def process_json_substrate(filepath: str) -> list[str]: """ Parst die komplexe 'Fallstudie Evoki' JSON-Datei und extrahiert sinnvolle Text-Chunks. """ print(f"Verarbeite strukturiertes JSON-Substrat: {filepath}...") chunks = [] try: with open(filepath, 'r', encoding='utf-8') as f: data = json.load(f) # Extrahiere Chatverlauf chat_data = data.get("gesamter_chatverlauf", {}) for year, months in chat_data.items(): for month, days in months.items(): for day, interactions in days.items(): if isinstance(interactions, list): for i, current in enumerate(interactions): if isinstance(current, dict) and current.get("message"): speaker = current.get("speaker", "unknown") message = current.get("message") chunks.append(f"Chat am {day}.{month}.{year} ({speaker}): {message}") # Extrahiere persönliche Daten aus der tief verschachtelten Struktur pdt_entries = data.get("regelwerke_meta", {}).get("aktuelle_version", {}).get("aktuelle_version", {}).get("regelwerk_version_dokumentation", {}).get("meta_regeln_details", {}).get("persoenliche_daten_temporaer_eintraege", []) for entry in pdt_entries: if isinstance(entry, dict) and entry.get("inhalt"): chunks.append(f"Persönlicher Datensatz (Kategorie: {entry.get('kategorie', 'unbekannt')}): {entry.get('inhalt')}") print(f"Erfolgreich {len(chunks)} Text-Abschnitte aus der JSON-Datei extrahiert.") return chunks except FileNotFoundError: print(f"FEHLER: Die Datei unter '{filepath}' wurde nicht gefunden.") return [] except Exception as e: print(f"Ein Fehler beim Verarbeiten der JSON-Datei ist aufgetreten: {e}") return [] def process_text_substrate(filepath: str) -> list[str]: """ Liest die 'MeineAktivitätentxt.txt' und extrahiert die Chat-Nachrichten. """ print(f"Verarbeite generisches Text-Substrat: {filepath}...") chunks = [] try: # Dies ist eine einfache Methode, um die Prompts und Antworten zu extrahieren. # Für komplexeres HTML wäre eine Bibliothek wie BeautifulSoup4 besser. with open(filepath, 'r', encoding='utf-8') as f: content = f.read() # Finde alle Prompts und Antworten entries = re.findall(r'