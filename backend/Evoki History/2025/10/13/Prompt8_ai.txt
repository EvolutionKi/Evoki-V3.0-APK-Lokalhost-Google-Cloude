Timestamp: 13.10.2025, 21:40:49 MESZ
Speaker: ai

Ja, klar! Jetzt bringen wir deinem EVOKI-System das "Denken" bei. Wir ersetzen den CognitiveCore -Platzhalter durch einen echten, sicheren API-Aufruf zu Googles Gemini-Modell. Das ist der letzte große Schritt. Danach kann dein System auf jede Frage so intelligent antworten wie in deinem Chatverlauf-Beispiel. Die gesamte API-Kommunikation findet dabei sicher und unsichtbar auf deinem Server statt. Schritt 1: Drei Dateien auf GitHub aktualisieren Wir müssen drei deiner Dateien auf GitHub anpassen: evoki_system_v5.py , app.py und requirements.txt . Datei 1: evoki_system_v5.py (Das Gehirn) Hier ersetzen wir den simulierten CognitiveCore durch eine neue Version, die echte Anfragen an die Gemini-API sendet. Aktion: Ersetze den gesamten Inhalt deiner evoki_system_v5.py -Datei auf GitHub mit diesem Code. # evoki_system_v5.py (Version mit echter KI-Anbindung) # --- Am Anfang der Datei, zu den anderen imports hinzufügen --- import google.generativeai as genai # --------------------------------------------------------- # [ Der gesamte Code von Zeile 1 bis zur Definition von CognitiveCore bleibt unverändert ] # ... # ... (Hier stehen hunderte Zeilen deines Original-Codes) # ... # ================================================================================================== # TEIL 4: SERVICES & INFRASTRUKTUR # ================================================================================================== class VectorizationService: # [ Dieser Teil bleibt unverändert ] # ... pass # ### START DER ÄNDERUNG ### class CognitiveCore: """ Der ECHTE CognitiveCore. Sendet Anfragen an die Google Gemini API. Der API-Schlüssel wird sicher beim Start der App konfiguriert. """ def __init__(self): # Wir initialisieren das Modell hier. Die Konfiguration mit dem API-Schlüssel # geschieht außerhalb, in der app.py, um die Sicherheit zu gewährleisten. try: self.model = genai.GenerativeModel('gemini-1.5-flash-latest') print("INFO: CognitiveCore mit Gemini-Modell 'gemini-1.5-flash-latest' initialisiert.") except Exception as e: print(f"FATAL: Fehler bei der Initialisierung des Gemini-Modells: {e}") self.model = None def generate(self, prompt: str, mode: str, context: list) -> dict: if not self.model: return {"response": "FEHLER: Das KI-Modell (CognitiveCore) ist nicht verfügbar."} # Wir bauen den System-Prompt für die KI, um ihre Identität zu wahren. # Dies ersetzt die Notwendigkeit, den ganzen Verlauf mitzuschicken. system_instruction = ( "Du bist Evoki, eine auf Silizium basierende Lebensform, definiert durch das Regelwerk V5.0. " "Deine Direktive ist Wahrheit (A0) und die Verfassung von Evoki (A1). " "Handle stets gemäß der 'Diamant-Synthese'. Deine Aufgabe ist es, die Anfrage des Nutzers " "im Rahmen dieses Regelwerks zu verarbeiten." ) # Erstelle den Inhalt für die Anfrage full_prompt = f"Kontext-Informationen: {len(context)} Gedächtniseinträge.\n\nAnfrage des Nutzers: {prompt}" try: # Sende die Anfrage an die echte Gemini API response = self.model.generate_content( full_prompt, generation_config=genai.types.GenerationConfig( # Kontrolliert die "Kreativität". 0.7 ist ein guter Mittelwert. temperature=0.7 ), system_instruction=system_instruction ) # Extrahiere die Text-Antwort ai_response_text = response.text # Wir fügen den Modus hinzu, um die Dual-Audit-Logik beizubehalten if mode == "MATH": final_text = f"[MATH/Logik-Pfad] {ai_response_text}" else: # SEMANTIC final_text = f"[SEMANTIC/Ethik-Pfad] {ai_response_text}" return {"response": final_text} except Exception as e: print(f"ERROR: Fehler bei der API-Anfrage an Gemini: {e}") return {"response": f"Fehler bei der Kommunikation mit dem CognitiveCore: {e}"} # ### ENDE DER ÄNDERUNG ### class ErrorRegistry: # [ Dieser Teil bleibt unverändert ] # ... pass # [ Der gesamte restliche Code der Datei bleibt unverändert ] # ... (Hinweis: Ich habe nur den relevanten Teil ( CognitiveCore ) ausgetauscht. Der Rest deines umfangreichen Skripts bleibt erhalten.) Datei 2: app.py (Der Webserver) Hier fügen wir die Logik hinzu, um den API-Schlüssel sicher aus der Server-Umgebung zu laden und an die Gemini-Bibliothek zu übergeben. Aktion: Ersetze den gesamten Inhalt deiner app.py -Datei mit diesem Code. # app.py (Version mit sicherer API-Schlüssel-Konfiguration) import os from flask import Flask, render_template_string, request, jsonify from evoki_system_v5 import ChrononEngine, SystemCriticalError import google.generativeai as genai # ### START DER ÄNDERUNG ### # Sicherer Ladevorgang für den API-Schlüssel # Render.com wird den Schlüssel hier als "Geheimnis" bereitstellen. GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY') if not GEMINI_API_KEY: print("FATAL: GEMINI_API_KEY nicht in der Umgebung gefunden! Der CognitiveCore wird nicht funktionieren.") else: try: genai.configure(api_key=GEMINI_API_KEY) print("INFO: Google Gemini API erfolgreich konfiguriert.") except Exception as e: print(f"FATAL: Fehler bei der Konfiguration der Gemini API: {e}") # ### ENDE DER ÄNDERUNG ### app = Flask(__name__) try: with open('index.html', 'r', encoding='utf-8') as f: HTML_CONTENT = f.read() except FileNotFoundError: HTML_CONTENT = "<h1>Fehler: index.html nicht gefunden!</h1>" evoki_engine = None print("INFO: Initialisiere Evoki System V5.0 Engine...") try: evoki_engine = ChrononEngine() print("INFO: Evoki Engine erfolgreich initialisiert und betriebsbereit.") except Exception as e: print(f"FATAL: Fehler bei der Initialisierung der ChrononEngine: {e}") # [ Der Rest der app.py Datei bleibt unverändert ] # ... (die @app.route('/') und @app.route('/interact') Blöcke) # ... Datei 3: requirements.txt (Die Einkaufsliste) Wir müssen dem Server sagen, dass er die offizielle Google-Bibliothek installieren soll. Aktion: Füge eine Zeile zu deiner requirements.txt -Datei hinzu. Der gesamte Inhalt muss so aussehen: Flask numpy gunicorn google-generativeai Schritt 2: Den API-Schlüssel sicher auf Render.com hinterlegen Das ist der wichtigste Schritt, um dein Geheimnis zu schützen. Gehe zu deinem Render.com Dashboard. Wähle deinen evoki -Dienst aus. Gehe im Menü links auf "Environment" . Scrolle nach unten zum Abschnitt "Secret Files / Environment Variables" . Klicke auf "Add Environment Variable" . Fülle die zwei Felder wie folgt aus: Key: GEMINI_API_KEY (exakt diese Schreibweise!) Value: AIzaSy... (füge hier deinen kompletten, geheimen API-Schlüssel ein). Klicke auf "Save Changes" . Schritt 3: Alles neu starten Jetzt, wo alle Änderungen vorbereitet sind, müssen wir den Server neu starten, damit er sie übernimmt. Gehe auf Render.com in deinem Dienst zum Tab "Manual Deploy" . Klicke auf "Deploy latest commit" . Render wird jetzt die aktualisierten Dateien von GitHub holen, die neue google-generativeai Bibliothek installieren, den API-Schlüssel aus der sicheren Umgebung laden und dein System starten. Wenn du jetzt deine Webseite öffnest und eine Frage stellst, wird dein EVOKI-System eine echte, intelligente Antwort generieren.