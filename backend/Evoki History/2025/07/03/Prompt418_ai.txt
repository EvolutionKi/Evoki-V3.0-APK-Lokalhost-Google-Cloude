Timestamp: 03.07.2025, 22:33:39 MESZ
Speaker: ai

Absolut, mein Freund! Dann setzen wir das um. Das Aufteilen in "sinnvolle Chunks" direkt im Browser ist technisch etwas anspruchsvoller und erfordert eine intelligente Logik im Frontend (JavaScript), die erkennt, wann eine Sprechpause lang genug ist, um ein Audio-Segment abzuschließen und zu senden. Hier ist ein aktualisierter Code für dein Frontend (die Webseite auf der Uhr) , der versucht, dies umzusetzen. Wichtige Änderungen im JavaScript: SilenceDetector : Eine kleine Helferklasse, die erkennt, wann eine Sprechpause auftritt. mediaRecorder.ondataavailable Logik: Sendet Audio-Chunks nicht mehr nur, sondern puffert sie und sendet sie erst, wenn eine Sprechpause erkannt wird. resetAudioProcessing : Um den Zustand nach dem Senden eines Chunks zurückzusetzen. Aktualisiertes Frontend (Deine Webseite für die Google Pixel Watch) Speichere dies als deine index.html oder ähnliches. <!DOCTYPE html> <html> <head> <title>Gemini Watch Chat</title> <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" /> <style> body { font-family: sans-serif; text-align: center; margin-top: 20px; background-color: #222; color: #eee; display: flex; flex-direction: column; align-items: center; justify-content: flex-start; height: calc(100vh - 40px); } button { padding: 15px; font-size: 1.1em; margin: 8px; background-color: #007bff; color: white; border: none; border-radius: 10px; width: 80%; max-width: 200px; } button:disabled { background-color: #555; cursor: not-allowed; } #response-container { margin-top: 15px; padding: 12px; border: 1px solid #555; border-radius: 8px; min-height: 80px; text-align: left; background-color: #333; width: 90%; max-width: 280px; overflow-y: auto; box-sizing: border-box; flex-grow: 1; } #status { color: #aaa; font-size: 0.85em; margin-bottom: 10px; } #manualInputGroup { display: none; flex-direction: column; align-items: center; width: 100%; } #manualInput { width: 90%; max-width: 280px; padding: 10px; margin-top: 10px; border-radius: 5px; border: 1px solid #555; background-color: #444; color: #eee; } #sendManualButton { background-color: #28a745; margin-top: 10px; } </style> </head> <body> <div id="status">Verbinde und starte Audio...</div> <div id="response-container"></div> <button id="recordButton">Halten & Sprechen</button> <button id="stopStreamingButton" style="display:none;">Aufnahme Stoppen (Admin)</button> <div id="manualInputGroup"> <input type="text" id="manualInput" placeholder="Manuelle Eingabe..."> <button id="sendManualButton">Senden</button> </div> <script> const statusDiv = document.getElementById('status'); const responseContainer = document.getElementById('response-container'); const recordButton = document.getElementById('recordButton'); const stopStreamingButton = document.getElementById('stopStreamingButton'); // Für Notfall/Debug const manualInputGroup = document.getElementById('manualInputGroup'); const manualInput = document.getElementById('manualInput'); const sendManualButton = document.getElementById('sendManualButton'); let mediaRecorder; let audioStream; let ws; // WebSocket-Verbindung let isRecording = false; let audioChunks = []; let silenceDetector; // Hier trägst du die Adresse deines Servers ein! const WEBSOCKET_SERVER_URL = 'ws://deine-server-adresse:8080/ws'; // Beispiel: ws://192.168.1.100:8080/ws // --- Helferklasse für Stilleerkennung --- class SilenceDetector { constructor(audioContext, stream, threshold = 0.05, minSilenceDuration = 1000) { this.audioContext = audioContext; this.analyser = this.audioContext.createAnalyser(); this.microphone = this.audioContext.createMediaStreamSource(stream); this.microphone.connect(this.analyser); this.dataArray = new Uint8Array(this.analyser.fftSize); this.threshold = threshold * 255; // Konvertiere in 0-255 Bereich this.minSilenceDuration = minSilenceDuration; this.lastSoundTime = Date.now(); this.onSilence = null; this.checkInterval = 100; // Alle 100ms prüfen this.intervalId = null; } start() { this.stop(); // Sicherstellen, dass keine alten Intervalle laufen this.lastSoundTime = Date.now(); // Reset this.intervalId = setInterval(() => this.check(), this.checkInterval); } stop() { if (this.intervalId) { clearInterval(this.intervalId); this.intervalId = null; } } check() { this.analyser.getByteTimeDomainData(this.dataArray); let sum = 0; for (let i = 0; i < this.dataArray.length; i++) { let value = Math.abs(this.dataArray[i] - 128); // Entferne DC Offset sum += value; } let average = sum / this.dataArray.length; if (average > this.threshold) { this.lastSoundTime = Date.now(); } else if (Date.now() - this.lastSoundTime > this.minSilenceDuration) { // Stille erkannt! this.stop(); // Stoppe den Detector if (this.onSilence) { this.onSilence(); } } } } // --- WebSocket-Funktionen --- function connectWebSocket() { ws = new WebSocket(WEBSOCKET_SERVER_URL); ws.onopen = () => { statusDiv.textContent = "Verbunden mit Server. Bereit zum Sprechen."; recordButton.disabled = false; }; ws.onmessage = async event => { const data = JSON.parse(event.data); if (data.text_response) { responseContainer.textContent = data.text_response; if (data.audio_base64 && data.audio_format) { const audioSource = `data:audio/${data.audio_format};base64,${data.audio_base64}`; const audio = new Audio(audioSource); try { await audio.play(); } catch (e) { console.error("Audio Playback Error:", e); } } } else if (data.error) { statusDiv.textContent = `Server Fehler: ${data.error}`; } }; ws.onclose = () => { statusDiv.textContent = "Verbindung getrennt. Versuche erneut..."; isRecording = false; recordButton.disabled = true; manualInputGroup.style.display = 'none'; if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop(); if (audioStream) audioStream.getTracks().forEach(track => track.stop()); setTimeout(connectWebSocket, 3000); }; ws.onerror = (error) => { console.error("WebSocket Error:", error); statusDiv.textContent = "WebSocket Fehler! Siehe Konsole."; ws.close(); }; } // --- Audio-Aufnahme und Senden --- // Konvertiert Blob zu Base64 für den Versand function blobToBase64(blob) { return new Promise((resolve, reject) => { const reader = new FileReader(); reader.onloadend = () => resolve(reader.result.split(',')[1]); reader.onerror = reject; reader.readAsDataURL(blob); }); } // Sendet den aktuellen Audio-Chunk async function sendAudioChunk() { if (audioChunks.length === 0) return; const audioBlob = new Blob(audioChunks, { type: 'audio/webm; codecs=opus' }); audioChunks = []; // Chunk leeren für nächste Aufnahme const base64Audio = await blobToBase64(audioBlob); if (ws && ws.readyState === WebSocket.OPEN) { ws.send(JSON.stringify({ type: 'audio', data: base64Audio, format: 'webm' })); statusDiv.textContent = "Audio gesendet, warte auf Antwort..."; } else { statusDiv.textContent = "Verbindung nicht offen zum Senden!"; } } async function startRecording() { if (isRecording) return; isRecording = true; manualInputGroup.style.display = 'none'; // Manuelle Eingabe ausblenden responseContainer.textContent = ""; // Alte Antwort löschen try { audioStream = await navigator.mediaDevices.getUserMedia({ audio: true }); mediaRecorder = new MediaRecorder(audioStream, { mimeType: 'audio/webm; codecs=opus' }); audioChunks = []; // Leere Chunks für neue Aufnahme const audioContext = new (window.AudioContext || window.webkitAudioContext)(); silenceDetector = new SilenceDetector(audioContext, audioStream, 0.05, 700); // 700ms Stille als Ende eines Chunks silenceDetector.onSilence = () => { // Stille erkannt, schicke den aktuellen Chunk if (isRecording) { // Nur senden, wenn noch aktiv aufgenommen wird mediaRecorder.stop(); // Stoppt die aktuelle Aufnahme und löst onstop aus } }; mediaRecorder.ondataavailable = event => { audioChunks.push(event.data); }; mediaRecorder.onstop = async () => { statusDiv.textContent = "Verarbeite Audio..."; await sendAudioChunk(); // Sende den gesammelten Chunk resetAudioProcessing(); // Bereite für nächste Aufnahme vor }; mediaRecorder.start(100); // Sammle Daten alle 100ms silenceDetector.start(); // Starte die Stilleerkennung statusDiv.textContent = "Spreche jetzt..."; } catch (err) { statusDiv.textContent = `Mikrofon Fehler: ${err.message}. ${err.name === 'NotAllowedError' ? 'Bitte Mikrofonzugriff erlauben!' : ''}`; console.error('Fehler beim Mikrofonzugriff:', err); stopRecording(); // Beende alles sauber } } function stopRecording() { if (isRecording) { isRecording = false; if (silenceDetector) silenceDetector.stop(); if (mediaRecorder && mediaRecorder.state !== 'inactive') { mediaRecorder.stop(); // Löst onstop aus, wo der Chunk gesendet wird } else { resetAudioProcessing(); // Wenn nichts aufgenommen wurde } } } function resetAudioProcessing() { if (audioStream) { audioStream.getTracks().forEach(track => track.stop()); // Mikrofon freigeben audioStream = null; } if (mediaRecorder) { mediaRecorder = null; } audioChunks = []; } // --- Event Listener --- recordButton.addEventListener('mousedown', startRecording); recordButton.addEventListener('touchstart', startRecording); // Für Touch-Geräte recordButton.addEventListener('mouseup', stopRecording); recordButton.addEventListener('touchend', stopRecording); // Für Touch-Geräte // Notfall / Debug Stop Button stopStreamingButton.onclick = () => { stopRecording(); statusDiv.textContent = "Manuell gestoppt. Bereit für Text."; manualInputGroup.style.display = 'flex'; }; sendManualButton.onclick = () => { const text = manualInput.value.trim(); if (text && ws && ws.readyState === WebSocket.OPEN) { ws.send(JSON.stringify({ type: 'text', message: text })); manualInput.value = ''; responseContainer.textContent = "Sende manuelle Eingabe..."; statusDiv.textContent = "Sende Text..."; } }; // --- Start beim Laden der Seite --- window.onload = connectWebSocket; </script> </body> </html> Wichtige Hinweise und Anpassungen: "Halten & Sprechen" Button: Der recordButton ist jetzt der Hauptbutton. Du musst ihn gedrückt halten, um aufzunehmen. Beim Loslassen stoppt die Aufnahme und sendet. SilenceDetector Parameter: threshold : Regelt, wie leise es sein muss, um als Stille zu gelten (Wert zwischen 0 und 1). 0.05 ist ein guter Startpunkt. minSilenceDuration : Wie lange die Stille anhalten muss (in Millisekunden), bevor ein Segment als beendet gilt (hier 700ms). Experimentiere damit, um das beste Gefühl zu bekommen. Backend muss Audio verarbeiten: Dein Python-Backend muss jetzt darauf vorbereitet sein, fertige Audio-Blobs (als Base64 kodiert) von der Uhr zu empfangen. Das ist einfacher als kontinuierliches Streaming, aber der model.generate_content -Aufruf mit Audio muss korrekt angepasst werden. Audio-Streaming und Text-to-Speech (TTS): Denke daran, dass der Backend-Code (Python) das empfangene Audio immer noch an die Gemini API zur Spracherkennung senden muss und die Textantwort dann an die Google Cloud Text-to-Speech API zur Audio-Generierung. Der Python-Code im letzten Beispiel hat noch Platzhalter für die Audio-Verarbeitung bei Gemini. Dieses Frontend-Update sollte dir eine wesentlich natürlichere "Push-to-Talk"-Erfahrung bieten! Ich bin immer für dich da!