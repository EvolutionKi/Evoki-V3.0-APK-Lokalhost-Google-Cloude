Timestamp: 03.07.2025, 22:35:06 MESZ
Speaker: ai

Absolut, mein Freund! Dann aktualisiere ich das Backend so, dass alles ineinandergreift und das Audio von der Uhr verarbeitet wird, um dir eine schnelle und hochwertige Antwort zu liefern. Aktualisiertes Backend (Python Flask Server) Dieses Update integriert die Audio-Verarbeitung und die Google Cloud Text-to-Speech API. Wichtige Schritte vor dem Start: GCP Authentifizierung: Stelle sicher, dass du ein Google Cloud Projekt hast, die Google Cloud Text-to-Speech API aktiviert ist und dein Dienstkonto-JSON-Schlüssel auf deinem Server liegt (dessen Pfad dann in der Umgebungsvariable GOOGLE_APPLICATION_CREDENTIALS gesetzt ist). Das ist entscheidend ! Installation: pip install Flask Flask-Sockets google-generativeai google-cloud-texttospeech API-Schlüssel: export GEMINI_API_KEY="DEIN_GEMINI_API_SCHLÜSSEL" Audio-Typ: Der Frontend-Code sendet audio/webm; codecs=opus . Dein Backend wird dies nun empfangen und an Gemini weiterleiten. import os from flask import Flask, request, jsonify from flask_sockets import Sockets import google.generativeai as genai import json import base64 from google.cloud import texttospeech import io # Für die Verarbeitung von Audio-Bytes app = Flask(__name__) sockets = Sockets(app) # --- Konfiguration --- # Lade Gemini API-Schlüssel genai.configure(api_key=os.getenv("GEMINI_API_KEY")) model = genai.GenerativeModel('gemini-pro') # Lade Google Cloud Text-to-Speech Client (authentifiziert sich über GOOGLE_APPLICATION_CREDENTIALS) tts_client = texttospeech.TextToSpeechClient() # Globale Variable für den Gesprächsverlauf (für jede WebSocket-Verbindung) # In einer echten Anwendung solltest du dies persistenter speichern (z.B. Datenbank) # und nicht nur im RAM des Servers, da sonst der Kontext bei Serverneustart verloren geht. conversation_history = {} # Speichert den Verlauf pro WebSocket-SID (Session ID) print("Backend-Server startet...") # --- Funktion zum Generieren von Audio aus Text mit Google Cloud Text-to-Speech --- def synthesize_speech(text, voice_name="de-DE-Wavenet-F"): """ Generiert Audio (MP3) aus Text unter Verwendung der Google Cloud Text-to-Speech API. """ input_text = texttospeech.SynthesisInput(text=text) voice = texttospeech.VoiceSelectionParams( language_code="de-DE", name=voice_name, ssml_gender=texttospeech.SsmlVoiceGender.FEMALE ) audio_config = texttospeech.AudioConfig( audio_encoding=texttospeech.AudioEncoding.MP3 ) response = tts_client.synthesize_speech( input=input_text, voice=voice, audio_config=audio_config ) return response.audio_content # Gibt die MP3-Bytes zurück # --- WebSocket-Endpunkt für Audio/Text-Streaming --- @sockets.route('/ws') def chat_socket(ws): print("Neue WebSocket-Verbindung hergestellt.") session_id = str(id(ws)) # Einfache Session ID basierend auf Objekt-ID conversation_history[session_id] = [] # Neuen Verlauf für diese Session starten while not ws.closed: message = ws.receive() if message is None: break try: data = json.loads(message) message_type = data.get('type') if message_type == 'text': user_text = data.get('message') print(f"[{session_id}] Empfangen (Text): {user_text}") # Füge die Benutzernachricht zum Verlauf hinzu conversation_history[session_id].append({"role": "user", "parts": [user_text]}) # Gemini Anfrage mit Text und Kontext # Das 'generate_content' erwartet bei Kontextführung die Nachrichten als Liste gemini_response = model.generate_content(conversation_history[session_id]) response_text = gemini_response.text # Füge Geminis Antwort zum Verlauf hinzu conversation_history[session_id].append({"role": "model", "parts": [response_text]}) # Text-zu-Sprache Umwandlung audio_content = synthesize_speech(response_text) encoded_audio = base64.b64encode(audio_content).decode('utf-8') ws.send(json.dumps({ "text_response": response_text, "audio_base64": encoded_audio, "audio_format": "mp3" })) print(f"[{session_id}] Gesendet (Text & Audio): {response_text[:50]}...") elif message_type == 'audio': base64_audio = data.get('data') audio_format = data.get('format') # z.B. 'webm' if not base64_audio: ws.send(json.dumps({"error": "Keine Audio-Daten empfangen."})) continue audio_bytes = base64.b64decode(base64_audio) print(f"[{session_id}] Empfangen (Audio Blob, Format: {audio_format}, Größe: {len(audio_bytes)} Bytes)") # --- Audio an Gemini senden (Speech-to-Text und KI-Verarbeitung) --- # Gemini API kann direkt Audio-Bytes verarbeiten audio_input = { "mime_type": f"audio/{audio_format}", # Sollte audio/webm sein "data": audio_bytes } try: # Gemini Anfrage mit Audio und Kontext # Hier muss die Audio-Nachricht zum Verlauf hinzugefügt werden # (Gemini API transkribiert das Audio selbst in Text für den Kontext) # Temporäre Liste für diese Anfrage: bisheriger Verlauf + aktuelles Audio current_prompt_with_history = conversation_history[session_id] + [{"role": "user", "parts": [audio_input]}] gemini_response = model.generate_content(current_prompt_with_history) response_text = gemini_response.text # Füge die transkribierte User-Nachricht (falls Gemini sie zurückgibt, # sonst musst du sie separat transkribieren) und Geminis Antwort zum Verlauf hinzu # Hier wird davon ausgegangen, dass Gemini den Audio-Input in seinem Output als Text zusammenfasst # oder dass wir einfach nur die Textantwort speichern. Für ein genaues Gedächtnis # sollte die User-Audio-Transkription auch hierhin. # Simplere Kontext-Ergänzung für Audio: Wir speichern nur die Textantwort des Modells. # Für eine perfekte Historie bräuchte man auch die Transkription des User-Audios. conversation_history[session_id].append({"role": "user", "parts": [gemini_response.candidates[0].content.parts[0].text if gemini_response.candidates and gemini_response.candidates[0].content.parts and gemini_response.candidates[0].content.parts[0].text else "Audio input"]}) # Not ideal, but ensures a user part for context conversation_history[session_id].append({"role": "model", "parts": [response_text]}) # Text-zu-Sprache Umwandlung audio_content = synthesize_speech(response_text) encoded_audio = base64.b64encode(audio_content).decode('utf-8') ws.send(json.dumps({ "text_response": response_text, "audio_base64": encoded_audio, "audio_format": "mp3" })) print(f"[{session_id}] Gesendet (Text & Audio): {response_text[:50]}...") except Exception as e: print(f"[{session_id}] Fehler bei Audio-Verarbeitung oder Gemini-Anfrage: {e}") ws.send(json.dumps({"error": f"Backend Fehler bei Audio: {e}"})) else: ws.send(json.dumps({"error": "Unbekannter Nachrichtentyp."})) except json.JSONDecodeError: print(f"[{session_id}] Ungültiges JSON empfangen: {message}") ws.send(json.dumps({"error": "Ungültiges JSON-Format."})) # Wenn die Verbindung geschlossen wird, den Verlauf für diese Session löschen if session_id in conversation_history: del conversation_history[session_id] print(f"[{session_id}] WebSocket-Verbindung geschlossen und Verlauf gelöscht.") if __name__ == '__main__': from gevent.pywsgi import WSGIServer from geventwebsocket.handler import WebSocketHandler http_server = WSGIServer(('', 8080), app, handler_class=WebSocketHandler) print("Server läuft auf ws://0.0.0.0:8080/ws") http_server.serve_forever() Was dieses aktualisierte Backend macht: Google Cloud TTS-Client: Ein tts_client wurde initialisiert, um die Sprachsynthese durchzuführen. synthesize_speech Funktion: Diese Hilfsfunktion nimmt einen Text entgegen und gibt die MP3-Audiodaten zurück, die von der Google Cloud Text-to-Speech API generiert wurden. Du kannst hier die voice_name anpassen, um verschiedene Stimmen auszuprobieren (z.B. de-DE-Wavenet-C , de-DE-Wavenet-D etc. – schau in die Google TTS Doku für Optionen). Audio-Verarbeitung ( message_type == 'audio' ): Es dekodiert das Base64-Audio, das vom Frontend kommt. Es erstellt ein audio_input -Objekt im Format, das die Gemini API erwartet (MIME-Typ und die rohen Bytes). Kontext (Gesprächsverlauf): Es versucht, den Gesprächsverlauf ( conversation_history ) in die generate_content -Anfrage an Gemini einzubeziehen. Dies ist der Teil, der unser "Gedächtnis" simuliert. Beachte: Bei Audio-Eingaben fügt Gemini die Transkription des Audios oft selbst in seinen internen Kontext ein. Für die conversation_history habe ich einen Platzhalter hinzugefügt, der versucht, die vom Modell transkribierte User-Nachricht zu speichern oder einen generischen "Audio input"-Platzhalter. Antwort als Audio: Nachdem Gemini die Textantwort generiert hat, wird diese Antwort an synthesize_speech gesendet, um das hochwertige Audio zu erzeugen. Dieses Audio wird dann Base64-kodiert und zusammen mit dem Text an die Uhr zurückgeschickt. conversation_history : Ein simples Dictionary, das den Gesprächsverlauf für jede WebSocket-Verbindung speichert. Das ist wichtig, damit Gemini den Kontext deiner vorherigen Fragen und Antworten beibehält. Hinweis: In einer echten, produktiven Anwendung sollte dieser Verlauf nicht nur im RAM des Servers gespeichert werden, sondern in einer Datenbank, damit er persistent ist und auch funktioniert, wenn mehrere Nutzer gleichzeitig deine Seite verwenden. Dieses Backend-Gerüst sollte jetzt alle Teile zusammenführen, damit deine Uhr Audio senden, Gemini antworten und die Antwort in hochwertiger Sprache auf deiner Uhr ausgegeben werden kann! Ich bin immer für dich da, mein Freund!