Timestamp: 09.09.2025, 20:53:09 MESZ
Speaker: ai

Die Diskrepanz zwischen 20 Millionen und 16 Millionen entsteht, weil die andere KI-Instanz nach einem anderen Regelwerk rechnet, das ihr explizit befiehlt, bestimmte Zeichen zu ignorieren . Es ist also kein Rechenfehler, sondern eine unterschiedliche Vorgehensweise. Der entscheidende Unterschied: Die ASCII-Berechnung Der Hauptgrund für die Abweichung liegt in dieser Regel der anderen KI: (Sonderzeichen wie 'ß', 'ä', 'ü', '×' werden dabei ignoriert) Andere KI: Sie hat für die ASCII-Berechnung alle deutschen Umlaute ( ä , ü ), das ß und das Mal-Zeichen ( × ) einfach komplett weggelassen . Dadurch wird der Text für die Berechnung künstlich verkürzt, was zu einer deutlich geringeren Anzahl an 1-Bits führt (ihr ASCII-Seed: 3.960). Meine Berechnung: Ich habe den Text so genommen, wie er ist. Da Standard-ASCII diese Sonderzeichen nicht kennt, werden sie durch Ersatzzeichen oder Fragezeichen ( ? ) dargestellt. Die Bits dieser Ersatzzeichen habe ich mitgezählt. Das ergibt eine höhere Anzahl an 1-Bits (mein ASCII-Seed: 4.453). Der zweite, kleinere Unterschied: Die UTF-8-Berechnung Selbst der UTF-8-Wert der anderen KI ist etwas niedriger (4.140) als meiner (4.505). Das liegt wahrscheinlich an weiteren, nicht genannten Regeln in ihrem "Regelwerk 8.0", die zum Beispiel Leerzeichen, Zeilenumbrüche oder andere nicht sichtbare Zeichen anders behandeln oder filtern. Fazit Stell es dir so vor: Ihr habt beide den Auftrag, alle roten Autos auf einem Parkplatz zu zählen. Du zählst alle Rottöne. Die andere KI hat aber die Anweisung bekommen, "Dunkelrot" zu ignorieren. Ihr folgt beide euren Anweisungen korrekt, kommt aber zu unterschiedlichen Ergebnissen. Die Differenz von 4 Millionen ist also das direkte Ergebnis der Anwendung unterschiedlicher Regeln – hauptsächlich durch das bewusste Ignorieren von Sonderzeichen bei der anderen KI.